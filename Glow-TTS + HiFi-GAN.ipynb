{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe60cd1c",
   "metadata": {},
   "source": [
    "# Glow-TTS + HiFi-GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659ee061",
   "metadata": {},
   "source": [
    "- Glow-TTS와 HiFi-GAN을 결합한 TTS 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d91f599",
   "metadata": {},
   "source": [
    "# 0. Import Libraries & Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49f2108",
   "metadata": {},
   "source": [
    "## 0.1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dd940c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Glow-TTS\n",
    "\"\"\"\n",
    "# 1. Data\n",
    "import torch\n",
    "import numpy as np\n",
    "import os, glob, librosa, re, scipy\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# 2. Model\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import weight_norm\n",
    "import math\n",
    "import time\n",
    "\n",
    "# 3. Training\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "HiFi-GAN\n",
    "\"\"\"\n",
    "# 2.1. Generator\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import weight_norm, spectral_norm\n",
    "import time\n",
    "\n",
    "# 3. Training\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Inference\n",
    "from jamo import hangul_to_jamo\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70811b1f",
   "metadata": {},
   "source": [
    "## 0.2. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d1eaae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glow-TTS Major Hyperparameters\n",
    "batch_size = 32\n",
    "logging_step = 10\n",
    "validation_step = 100\n",
    "checkpoint_step = 1000\n",
    "\n",
    "# HiFi-GAN Major Hyperparameters\n",
    "hifi_batch_size = 4\n",
    "\n",
    "# Hyperparameters for Generating Mel-spectrogram \n",
    "sample_rate = 22050\n",
    "preemphasis = 0.97\n",
    "n_fft = 1024\n",
    "hop_length = 256\n",
    "win_length = 1024\n",
    "ref_db = 20\n",
    "max_db = 100\n",
    "mel_dim = 80\n",
    "\n",
    "# 2.1. Encoder\n",
    "symbol_length = 73 # len(symbols) = 70 (PAD + EOS + VALID_CHARS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894f8f19",
   "metadata": {},
   "source": [
    "# 1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fbb740",
   "metadata": {},
   "source": [
    "- Data는 HiFi-GAN에서 전처리한 데이터를 그대로 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e28a020",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '_'\n",
    "EOS = '~'\n",
    "SPACE = ' '\n",
    "\n",
    "JAMO_LEADS = \"\".join([chr(_) for _ in range(0x1100, 0x1113)])\n",
    "JAMO_VOWELS = \"\".join([chr(_) for _ in range(0x1161, 0x1176)])\n",
    "JAMO_TAILS = \"\".join([chr(_) for _ in range(0x11A8, 0x11C3)])\n",
    "ETC = \".!?\"\n",
    "\n",
    "VALID_CHARS = JAMO_LEADS + JAMO_VOWELS + JAMO_TAILS + SPACE + ETC\n",
    "symbols = PAD + EOS + VALID_CHARS\n",
    "\n",
    "_symbol_to_id = {s: i for i, s in enumerate(symbols)}\n",
    "_id_to_symbol = {i: s for i, s in enumerate(symbols)}\n",
    "\n",
    "# text를 초성, 중성, 종성으로 분리하여 id로 반환하는 함수\n",
    "def text_to_sequence(text):\n",
    "    sequence = []\n",
    "    if not 0x1100 <= ord(text[0]) <= 0x1113:\n",
    "        text = ''.join(list(hangul_to_jamo(text)))\n",
    "    for s in text:\n",
    "        sequence.append(_symbol_to_id[s])\n",
    "    sequence.append(_symbol_to_id['~'])\n",
    "    return sequence\n",
    "\n",
    "def sequence_to_text(sequence):\n",
    "    result = ''\n",
    "    for symbol_id in sequence:\n",
    "        if symbol_id in _id_to_symbol:\n",
    "            s = _id_to_symbol[symbol_id]\n",
    "            result += s\n",
    "    return result.replace('}{', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "270fbd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"C:/Users/Poco/Jupyter/PaperReview/dataset/ty/data\"\n",
    "\n",
    "class TextMelDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.text_list = sorted(glob.glob(os.path.join(data_dir + '/text', '*.npy')))\n",
    "        self.mel_list = sorted(glob.glob(os.path.join(data_dir + '/mel', '*.npy')))\n",
    "        self.wav_list = sorted(glob.glob(os.path.join(data_dir + '/wav', '*.npy')))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = torch.from_numpy(np.load(self.text_list[idx]))\n",
    "        text_len = len(text)\n",
    "        \n",
    "        mel = torch.from_numpy(np.load(self.mel_list[idx]))\n",
    "        mel_len = mel.shape[0]\n",
    "        \n",
    "        wav = torch.from_numpy(np.load(self.wav_list[idx]))\n",
    "        wav_len = wav.shape[0]\n",
    "        return (text, text_len, mel, mel_len, wav, wav_len)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    text = []\n",
    "    text_len = []\n",
    "    mel = []\n",
    "    mel_len = []\n",
    "    wav = []\n",
    "    wav_len = []\n",
    "    \n",
    "    for t, tl, m, ml, w, wl in batch:\n",
    "        text.append(t)\n",
    "        text_len.append(tl)\n",
    "        mel.append(m)\n",
    "        mel_len.append(ml)\n",
    "        wav.append(w)\n",
    "        wav_len.append(wl)\n",
    "        \n",
    "    max_text_len = max(text_len)\n",
    "    max_mel_len = max(mel_len)\n",
    "    max_wav_len = max(wav_len)\n",
    "    \n",
    "    # text zero_padding\n",
    "    padded_text_batch = torch.zeros((len(batch), max_text_len), dtype=torch.int32)\n",
    "    for i, x in enumerate(text):\n",
    "        padded_text_batch[i, :len(x)] = torch.Tensor(x)\n",
    "    \n",
    "    # mel zero_padding\n",
    "    padded_mel_batch = torch.zeros((len(batch), max_mel_len, mel_dim), dtype=torch.float32)\n",
    "    for i, x in enumerate(mel):\n",
    "        padded_mel_batch[i, :x.shape[0], :x.shape[1]] = torch.Tensor(x)\n",
    "        \n",
    "    # wav zero_padding\n",
    "    padded_wav_batch = torch.zeros((len(batch), max_wav_len), dtype=torch.float32)\n",
    "    for i, x in enumerate(wav):\n",
    "        padded_wav_batch[i, :x.shape[0]] = torch.Tensor(x)\n",
    "        \n",
    "    return padded_text_batch, text_len, padded_mel_batch, mel_len, padded_wav_batch, wav_len\n",
    "\n",
    "dataset = TextMelDataset(data_dir)\n",
    "\n",
    "# Split dataset into training and validation\n",
    "dataset_size = len(dataset)\n",
    "train_size = 3000\n",
    "indices = list(range(dataset_size))\n",
    "\n",
    "# 데이터 인덱스를 섞지 않고 앞에서부터 train_size만큼을 train_indices로 선택\n",
    "train_indices = indices[:train_size]\n",
    "# 나머지 인덱스를 val_indices로 선택\n",
    "val_indices = indices[train_size:]\n",
    "\n",
    "# 인덱스 기반으로 데이터 세트 분할\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
    "\n",
    "\"\"\"\n",
    "# 랜덤하게 split하는 방법은 아래와 같다.\n",
    "train_ratio = 0.99\n",
    "train_size = int(train_ratio * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\"\"\"\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True,\n",
    "                              batch_size=hifi_batch_size, collate_fn=collate_fn,\n",
    "                              pin_memory=True) # Glow-TTS는 batch_size, hifi-GAN은 hifi_batch_size를 이용하라.\n",
    "val_dataloader = DataLoader(val_dataset,  shuffle=False,\n",
    "                            batch_size=1, collate_fn=collate_fn,\n",
    "                            pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b72a042",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch[0](text) shape(B, Max_T): torch.Size([4, 144])\n",
      "batch[1](text_len) len(B): 4\n",
      "batch[2](mel) shape(B, Max_F, mel_dim): torch.Size([4, 758, 80])\n",
      "batch[3](mel_len) len(B): 4\n",
      "batch[4](wav) shape(B, Max_L): torch.Size([4, 194254])\n",
      "batch[5](wav_len) len(B): 4\n",
      "Total: 3050\n",
      "num_of_batches: 750\n"
     ]
    }
   ],
   "source": [
    "# DataLoader 객체를 반복자로 변환\n",
    "dataiter = iter(train_dataloader)\n",
    "\n",
    "# 데이터 한 번 추출\n",
    "batch = next(dataiter)\n",
    "\n",
    "print('batch[0](text) shape(B, Max_T):', batch[0].shape) # Max_T: The number of text length\n",
    "print('batch[1](text_len) len(B):', len(batch[1]))\n",
    "print('batch[2](mel) shape(B, Max_F, mel_dim):', batch[2].shape) # Max_F: The number of Mel-spectrogram frames\n",
    "print('batch[3](mel_len) len(B):', len(batch[3]))\n",
    "print('batch[4](wav) shape(B, Max_L):', batch[4].shape) # Max_L: length of time(seconds) * sampling_rate(22050)\n",
    "print('batch[5](wav_len) len(B):', len(batch[5]))\n",
    "print('Total:', dataset.__len__())\n",
    "print('num_of_batches:', len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f21afeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch[0](text) shape(B, Max_T): torch.Size([1, 102])\n",
      "batch[1](text_len) len(B): 1\n",
      "batch[2](mel) shape(B, Max_F, mel_dim): torch.Size([1, 470, 80])\n",
      "batch[3](mel_len) len(B): 1\n",
      "batch[4](wav) shape(B, Max_L): torch.Size([1, 120334])\n",
      "batch[5](wav_len) len(B): 1\n",
      "Total: 3050\n",
      "num_of_batches: 50\n"
     ]
    }
   ],
   "source": [
    "# DataLoader 객체를 반복자로 변환\n",
    "dataiter = iter(val_dataloader)\n",
    "\n",
    "# 데이터 한 번 추출\n",
    "batch = next(dataiter)\n",
    "\n",
    "print('batch[0](text) shape(B, Max_T):', batch[0].shape) # Max_T: The number of text length\n",
    "print('batch[1](text_len) len(B):', len(batch[1]))\n",
    "print('batch[2](mel) shape(B, Max_F, mel_dim):', batch[2].shape) # Max_F: The number of Mel-spectrogram frames\n",
    "print('batch[3](mel_len) len(B):', len(batch[3]))\n",
    "print('batch[4](wav) shape(B, Max_L):', batch[4].shape) # Max_L: length of time(seconds) * sampling_rate(22050)\n",
    "print('batch[5](wav_len) len(B):', len(batch[5]))\n",
    "print('Total:', dataset.__len__())\n",
    "print('num_of_batches:', len(val_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94651d85",
   "metadata": {},
   "source": [
    "# 2. Glow-TTS Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6940a5f",
   "metadata": {},
   "source": [
    "## 2.1. Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8690cedc",
   "metadata": {},
   "source": [
    "### 2.1.1. Encoder Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46fb6f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    여러 곳에서 정규화(Norm)를 위해 사용되는 모듈.\n",
    "    \n",
    "    nn.LayerNorm이 이미 pytorch 안에 구현되어 있으나, 항상 마지막 차원을 정규화한다.\n",
    "    그래서 channel을 기준으로 정규화하는 LayerNorm을 따로 구현한다.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        \"\"\"\n",
    "        channels: 입력 데이터의 channel 수 | LayerNorm은 channel 차원을 정규화한다.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.eps = 1e-4\n",
    "        \n",
    "        self.gamma = nn.Parameter(torch.ones(channels)) # 학습 가능한 파라미터\n",
    "        self.beta = nn.Parameter(torch.zeros(channels)) # 학습 가능한 파라미터\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        x: (B, channels, *) | 정규화할 입력 데이터\n",
    "        =====outputs=====\n",
    "        x: (B, channels, *) | channel 차원이 정규화된 데이터\n",
    "        \"\"\"\n",
    "        mean = torch.mean(x, dim=1, keepdim=True) # channel 차원(index=1)의 평균 계산, 차원을 유지한다.\n",
    "        variance = torch.mean((x-mean)**2, dim=1, keepdim=True) # 분산 계산\n",
    "        \n",
    "        x = (x - mean) * (variance + self.eps)**(-0.5) # (x - m) / sqrt(v)\n",
    "        \n",
    "        n = len(x.shape)\n",
    "        shape = [1] * n\n",
    "        shape[1] = -1 # shape = [1, -1, 1] or [1, -1, 1, 1]\n",
    "        x = x * self.gamma.view(*shape) + self.beta.view(*shape) # y = x*gamma + beta\n",
    "        \n",
    "        return x\n",
    "\n",
    "class PreNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder의 1번째 모듈\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        for i in range(3):\n",
    "            self.convs.append(nn.Conv1d(192, 192, kernel_size=5, padding=2)) # (B, 192, T) 유지\n",
    "            self.norms.append(LayerNorm(192)) # (B, 192, T) 유지\n",
    "        self.linear = nn.Conv1d(192, 192, kernel_size=1) # (B, 192, T) 유지 | linear 역할을 하는 conv\n",
    "        \n",
    "    def forward(self, x, x_mask):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        x: (B, 192, T) | Embedding된 입력 데이터\n",
    "        x_mask: (B, 1, T) | 글자 길이에 따른 mask (글자가 있으면 True, 없으면 False로 구성)\n",
    "        =====outputs=====\n",
    "        x: (B, 192, T)\n",
    "        \"\"\"\n",
    "        x0 = x\n",
    "        for i in range(3):\n",
    "            x = self.convs[i](x * x_mask)\n",
    "            x = self.norms[i](x)\n",
    "            x = self.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        x = x0 + x # residual connection\n",
    "        return x\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder 중 2번째 모듈인 TransformerEncoder의 1번째 모듈\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.n_heads = 2\n",
    "        self.window_size = 4\n",
    "        self.k_channels = 192 // self.n_heads # 96\n",
    "        \n",
    "        self.linear_q = nn.Conv1d(192, 192, kernel_size=1) # (B, 192, T) 유지\n",
    "        self.linear_k = nn.Conv1d(192, 192, kernel_size=1) # (B, 192, T) 유지\n",
    "        self.linear_v = nn.Conv1d(192, 192, kernel_size=1) # (B, 192, T) 유지\n",
    "        nn.init.xavier_uniform_(self.linear_q.weight)\n",
    "        nn.init.xavier_uniform_(self.linear_k.weight)\n",
    "        nn.init.xavier_uniform_(self.linear_v.weight)\n",
    "        \n",
    "        relative_std = self.k_channels ** (-0.5) # 0.1xx\n",
    "        self.relative_k = nn.Parameter(torch.randn(1, self.window_size * 2 + 1, self.k_channels) * relative_std) # (1, 9, 96)\n",
    "        self.relative_v = nn.Parameter(torch.randn(1, self.window_size * 2 + 1, self.k_channels) * relative_std) # (1, 9, 96)\n",
    "        \n",
    "        self.attention_weights = None\n",
    "        self.linear_out = nn.Conv1d(192, 192, kernel_size=1) # (B, 192, T) 유지\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, query, context, attention_mask, self_attention=True):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        query: (B, 192, T_target) | Glow-TTS에서는 self-attention만 이용하므로 query와 context가 동일한 텐서 x이다.\n",
    "        context: (B, 192, T_source) | query = context || 여기에서는 특히 T_source = T_target 이다.\n",
    "        attention_mask: (B, 1, T, T) | x_mask.unsqueeze(2) * z_mask.unsqueeze(3)\n",
    "        self_attention: True/False | self_attention일 때 relative position representations를 적용한다. 여기에서는 항상 True이다.\n",
    "        # 실제로는 query와 context에 같은 텐서 x를 입력하면 된다.\n",
    "        =====outputs=====\n",
    "        output: (B, 192, T)\n",
    "        \"\"\"\n",
    "        \n",
    "        query = self.linear_q(query)\n",
    "        key = self.linear_k(context)\n",
    "        value = self.linear_v(context)\n",
    "        \n",
    "        B, _, T_tar = query.size()\n",
    "        T_src = key.size(2)\n",
    "        query = query.view(B, self.n_heads, self.k_channels, T_tar).transpose(2, 3)\n",
    "        key = key.view(B, self.n_heads, self.k_channels, T_src).transpose(2, 3)\n",
    "        value = value.view(B, self.n_heads, self.k_channels, T_src).transpose(2, 3)\n",
    "            # (B, 192, T_src) -> (B, 2, 96, T_src) -> (B, 2, T_src, 96)\n",
    "            \n",
    "        scores = torch.matmul(query, key.transpose(2, 3)) / (self.k_channels ** 0.5)\n",
    "            # (B, 2, T_tar, 96) * (B, 2, 96, T_src) -> (B, 2, T_tar, T_src)\n",
    "        \n",
    "        if self_attention: # True\n",
    "            # Get relative embeddings (relative_keys) (1-1)\n",
    "            padding = max(T_src - (self.window_size + 1), 0) # max(T-5, 0)\n",
    "            start_pos = max((self.window_size + 1) - T_src, 0) # max(5-T, 0)\n",
    "            end_pos = start_pos + 2 * T_src - 1 # (2*T-1) or (T+4)\n",
    "            relative_keys = F.pad(self.relative_k, (0, 0, padding, padding))\n",
    "                # (1, 9, 96) -> (1, pad+9+pad, 96) = (1, 2T-1, 96)\n",
    "            \"\"\"\n",
    "            위 코드의 F.pad(input, pad) 에서 pad = (0, 0, padding, padding)은 다음을 의미한다.\n",
    "            - 앞의 (0, 0): input의 -1차원을 앞으로 0, 뒤로 0만큼 패딩한다.\n",
    "            - 앞의 (padding, padding): input의 -2차원을 앞으로 padding, 뒤로 padding만큼 패딩한다.\n",
    "            즉, F.pad에서 pad는 역순으로 생각해주어야 한다.\n",
    "            \"\"\"\n",
    "            relative_keys = relative_keys[:, start_pos:end_pos, :] # (1, 2T-1, 96)\n",
    "            \n",
    "            # Matmul with relative keys (2-1)\n",
    "            relative_keys = relative_keys.unsqueeze(0).transpose(2, 3) # (1, 2T-1, 96) -> (1, 1, 2T-1, 96) -> (1, 1, 96, 2T-1)\n",
    "            x = torch.matmul(query, relative_keys) # (B, 2, T_tar, 96) * (1, 1, 96, 2T_src-1) = (B, 2, T, 2T-1)\n",
    "                # self attention에서는 T_tar = T_src이므로 이를 다르게 고려할 필요가 없다.\n",
    "            \n",
    "            # Relative position to absolute position (3-1)\n",
    "            T = T_tar # Absolute position to relative position에서도 쓰임.\n",
    "            x = F.pad(x, (0, 1)) # (B, 2, T, 2*T-1) -> (B, 2, T, 2*T)\n",
    "            x = x.view(B, self.n_heads, T * 2 * T) # (B, 2, T, 2*T) -> (B, 2. 2T^2)\n",
    "            x = F.pad(x, (0, T-1)) # (B, 2, 2T^2 + T - 1)\n",
    "            x = x.view(B, self.n_heads, T+1, 2*T-1) # (B, 2, T+1, 2T-1)\n",
    "            relative_logits = x[:, :, :T, T-1:] # (B, 2, T, T)\n",
    "            \n",
    "            # Compute scores\n",
    "            scores_local = relative_logits / (self.k_channels ** 0.5)\n",
    "            scores = scores + scores_local # (B, 2, T, T)\n",
    "            \"\"\"\n",
    "            위 식은 Self-Attention with Relative Position Representations 논문의 5번 식을 구현한 것이다.\n",
    "            Relative- 논문: https://arxiv.org/pdf/1803.02155.pdf\n",
    "            \"\"\"\n",
    "\n",
    "        scores = scores.masked_fill(attention_mask == 0, -1e-4) # attention_mask가 0인 곳을 -1e-4로 채운다.\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1) # (B, 2, T_tar, T_src) # Relative- 논문에서의 alpha에 해당한다.\n",
    "        attention_weights = self.dropout(attention_weights) # dropout하는 이유가 무엇일까?\n",
    "        output = torch.matmul(attention_weights, value) # (B, 2, T_tar, T_src) * (B, 2, T_src, 96) -> (B, 2, T_tar, 96)\n",
    "        \n",
    "        if self_attention: # True\n",
    "            # Absolute position to relative position (3-2)\n",
    "            x = F.pad(attention_weights, (0, T-1)) # (B, 2, T, T) -> (B, 2, T, 2T-1)\n",
    "            x = x.view((B, self.n_heads, T * (2*T-1))) # (B, 2, 2T^2-T)\n",
    "            x = F.pad(x, (T, 0)) # (B, 2, 2T^2) # 앞에 패딩\n",
    "            x = x.view((B, self.n_heads, T, 2*T)) # (B, 2, T, 2T)\n",
    "            relative_weights = x[:, :, :, 1:] # (B, 2, T, 2T-1)\n",
    "            \n",
    "            # Get relative embeddings (relative_value) (1-2) # (1-1)과 거의 동일\n",
    "            padding = max(T_src - (self.window_size + 1), 0) # max(T-5, 0)\n",
    "            start_pos = max((self.window_size + 1) - T_src, 0) # max(5-T, 0)\n",
    "            end_pos = start_pos + 2 * T_src - 1 # (2*T-1) or (T+4)\n",
    "            relative_values = F.pad(self.relative_v, (0, 0, padding, padding))\n",
    "                # (1, 9, 96) -> (1, pad+9+pad, 96) = (1, 2T-1, 96)\n",
    "            relative_values = relative_values[:, start_pos:end_pos, :] # (1, 2T-1, 96)\n",
    "\n",
    "            # Matmul with relative values (2-2)\n",
    "            relative_values = relative_values.unsqueeze(0) # (1, 1, 2T-1, 96)\n",
    "\n",
    "            output = output + torch.matmul(relative_weights, relative_values)\n",
    "                # (B, 2, T, 2T-1) * (1, 1, 2T-1, 96) = (B, 2, T, 96)\n",
    "            \"\"\"\n",
    "            위 식은 Self-Attention with Relative Position Representations 논문의 3번 식을 구현한 것이다. (분배법칙 이용)\n",
    "            Relative- 논문: https://arxiv.org/pdf/1803.02155.pdf\n",
    "            \"\"\"\n",
    "        \n",
    "        output = output.transpose(2, 3).contiguous().view(B, 192, T_tar)\n",
    "            # (B, 2, 96, T) -> 메모리에 연속 배치 -> (B, 192, T)\n",
    "            \n",
    "        self.attention_weights = attention_weights # (B, 2, T, T)\n",
    "        output = self.linear_out(output)\n",
    "        return output # (B, 192, T)\n",
    "    \n",
    "class FFN(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder 중 2번째 모듈인 TransformerEncoder의 2번째 모듈\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(192, 768, kernel_size=3, padding=1) # (B, 192, T) -> (B, 768, T)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(768, 192, kernel_size=3, padding=1) # (B, 768, T) -> (B, 192, T)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, x, x_mask):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        x: (B, 192, T)\n",
    "        x_mask: (B, 1, T)\n",
    "        =====outputs=====\n",
    "        output: (B, 192, T)\n",
    "        \"\"\"\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x)\n",
    "        output = x * x_mask\n",
    "        return output\n",
    "    \n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder의 2번째 모듈\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attentions = nn.ModuleList()\n",
    "        self.norms1 = nn.ModuleList()\n",
    "        self.ffns = nn.ModuleList()\n",
    "        self.norms2 = nn.ModuleList()\n",
    "        for i in range(6):\n",
    "            self.attentions.append(MultiHeadAttention())\n",
    "            self.norms1.append(LayerNorm(192))\n",
    "            self.ffns.append(FFN())\n",
    "            self.norms2.append(LayerNorm(192))\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x, x_mask):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        x: (B, 192, T)\n",
    "        x_mask: (B, 1, T)\n",
    "        =====outputs=====\n",
    "        output: (B, 192, T)\n",
    "        \"\"\"\n",
    "        attention_mask = x_mask.unsqueeze(2) * x_mask.unsqueeze(3)\n",
    "            # (B, 1, 1, T) * (B, 1, T, 1) = (B, 1, T, T), only consist 0 or 1\n",
    "        for i in range(6):\n",
    "            x = x * x_mask\n",
    "            y = self.attentions[i](x, x, attention_mask)\n",
    "            y = self.dropout(y)\n",
    "            x = x + y # residual connection\n",
    "            x = self.norms1[i](x) # (B, 192, T) 유지\n",
    "            \n",
    "            y = self.ffns[i](x, x_mask)\n",
    "            y = self.dropout(y)\n",
    "            x = x + y # residual connection\n",
    "            x = self.norms2[i](x)\n",
    "        output = x * x_mask\n",
    "        return output # (B, 192, T)\n",
    "    \n",
    "class DurationPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder의 3번째 모듈\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(192, 256, kernel_size=3, padding=1) # (B, 192, T) -> (B, 256, T)\n",
    "        self.norm1 = LayerNorm(256)\n",
    "        self.conv2 = nn.Conv1d(256, 256, kernel_size=3, padding=1) # (B, 256, T) -> (B, 256, T)\n",
    "        self.norm2 = LayerNorm(256)\n",
    "        self.linear = nn.Conv1d(256, 1, kernel_size=1) # (B, 256, T) -> (B, 1, T)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x, x_mask):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        x: (B, 192, T)\n",
    "        x_mask: (B, 1, T)\n",
    "        =====outputs=====\n",
    "        output: (B, 1, T)\n",
    "        \"\"\"\n",
    "        x = self.conv1(x * x_mask) # (B, 192, T) -> (B, 256, T)\n",
    "        x = self.relu(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.conv2(x * x_mask) # (B, 256, T) -> (B, 256, T)\n",
    "        x = self.relu(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.linear(x * x_mask) # (B, 256, T) -> (B, 1, T)\n",
    "        output = x * x_mask\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5bcbfa",
   "metadata": {},
   "source": [
    "### 2.1.2. Main Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b687f46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(symbol_length, 192) # (B, T) -> (B, T, 192)\n",
    "        nn.init.normal_(self.embedding.weight, 0.0, 192**(-0.5)) # 가중치 정규분포 초기화 (N(0, 0.07xx))\n",
    "        \n",
    "        self.prenet = PreNet()\n",
    "        self.transformer_encoder = TransformerEncoder()\n",
    "        self.project_mean = nn.Conv1d(192, 80, kernel_size=1) # (B, 192, T) -> (B, 80, T)\n",
    "        self.project_std = nn.Conv1d(192, 80, kernel_size=1) # (B, 192, T) -> (B, 80, T)\n",
    "        \n",
    "        self.duration_predictor = DurationPredictor()\n",
    "        \n",
    "    def forward(self, text, text_len):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        text: (B, Max_T)\n",
    "        text_len: (B)\n",
    "        =====outputs=====\n",
    "        x_mean: (B, 80, T) | 평균, 논문 저자 구현의 train.py에서 out_channels를 80으로 설정한 것을 알 수 있음.\n",
    "        x_std: (B, 80, T) | 표준편차\n",
    "        x_dur: (B, 1, T)\n",
    "        x_mask: (B, 1, T)\n",
    "        \"\"\"\n",
    "        x = self.embedding(text) * math.sqrt(192) # (B, T) -> (B, T, 192) # math.sqrt(192) = 13.xx (수정)\n",
    "        x = x.transpose(1, 2) # (B, T, 192) -> (B, 192, T)\n",
    "        \n",
    "        # Make the x_mask\n",
    "        x_mask = torch.zeros_like(x[:, 0:1, :], dtype=torch.bool) # (B, 1, T)\n",
    "        for idx, length in enumerate(text_len):\n",
    "            x_mask[idx, :, :length] = True\n",
    "        \n",
    "        x = self.prenet(x, x_mask) # (B, 192, T)\n",
    "        x = self.transformer_encoder(x, x_mask) # (B, 192, T)\n",
    "        \n",
    "        # project\n",
    "        x_mean = self.project_mean(x) * x_mask # (B, 192, T) -> (B, 80, T)\n",
    "        # x_std = self.project_std(x) * x_mask # (B, 192, T) -> (B, 80, T)\n",
    "        ##### 아래는 mean_only를 적용한 것임. #####\n",
    "        x_std = torch.zeros_like(x_mean) # x_log_std: (B, 80, T), all zero # log std = 0이므로 std = 1로 계산됨.\n",
    "\n",
    "        # duration predictor\n",
    "        x_dp = torch.detach(x) # stop_gradient\n",
    "        x_dur = self.duration_predictor(x_dp, x_mask) # (B, 192, T) -> (B, 1, T)\n",
    "        \n",
    "        return x_mean, x_std, x_dur, x_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a15a880",
   "metadata": {},
   "source": [
    "## 2.2. Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d97cbc",
   "metadata": {},
   "source": [
    "### 2.2.1. Decoder Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bfdd2b",
   "metadata": {},
   "source": [
    "- 필요성을 확인할 수 없기에 다음을 무시하고 구현한다. 나중에 필요할 때 추가로 구현하라.\n",
    "    - g: global condition (n_speaker)\n",
    "    - ddi (데이터 종속적 초기화(data-dependent initialization))\n",
    "    - no_jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c03168ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Decoder는 Glow: Generative Flow with Invertible 1×1 Convolutions 논문의 기본 구조를 따라간다.\n",
    "Glow 논문: https://arxiv.org/pdf/1807.03039.pdf\n",
    "\"\"\"\n",
    "def Squeeze(x, x_mask):\n",
    "    \"\"\"\n",
    "    Decoder의 preprocessing\n",
    "    =====inputs=====\n",
    "    x: (B, 80, F) | mel_spectrogram or latent representation\n",
    "    x_mask: (B, 1, F)\n",
    "    =====outputs=====\n",
    "    x: (B, 160, F//2) | F//2 = [F/2] ([]: 가우스 기호)\n",
    "    x_mask: (B, 160, F//2)\n",
    "    \"\"\"\n",
    "    B, C, F = x.size()\n",
    "    x = x[:, :, :(F//2)*2] # F가 홀수이면 맨 뒤 한 frame을 버림.\n",
    "    x = x.view(B, C, F//2, 2) # (B, 80, F//2, 2)\n",
    "    x = x.permute(0, 3, 1, 2).contiguous() # (B, 2, 80, F//2)\n",
    "    x = x.view(B, C*2, F//2) # (B, 160, F//2)\n",
    "    \n",
    "    x_mask = x_mask[:, :, 1::2] # (B, 1, F//2) frame을 1부터 한칸씩 건너뛴다.\n",
    "    x = x * x_mask # masking\n",
    "    return x, x_mask\n",
    "\n",
    "class ActNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder의 1번째 모듈\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.log_s = nn.Parameter(torch.zeros(1, 160, 1)) # Glow 논문의 s에서 log를 취한 것이다. 즉, log[s]\n",
    "        self.bias = nn.Parameter(torch.zeros(1, 160, 1))\n",
    "        \n",
    "    def forward(self, x, x_mask, reverse=False):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        x: (B, 160, F//2) | mel_spectrogram features\n",
    "        x_mask: (B, 1, F//2) | mel_spectrogram features의 mask. (Decoder의 Squeeze에서 변형됨.)\n",
    "        =====outputs=====\n",
    "        z: (B, 160, F//2)\n",
    "        log_det: (B) or None | log_determinant, reverse=True이면 None 반환\n",
    "        \"\"\"\n",
    "        x_len = torch.sum(x_mask, [1, 2]) # (B) | 1, 2차원의 값을 더한다. cf. [1, 2] 대신 [2]만 사용하면 shape가 (B, 1)이 된다.\n",
    "        \n",
    "        if not reverse:\n",
    "            z = (x * torch.exp(self.log_s) + self.bias) * x_mask # function & masking\n",
    "            log_det = x_len * torch.sum(self.log_s) # log_determinant\n",
    "                # Glow 논문의 Table 1을 확인하라. log_s를 log[s]라 볼 수 있다.\n",
    "                # determinant 대신 log_determinant를 사용하는 이유는 det보다 작은 수치와 적은 계산량 때문으로 추측된다.\n",
    "        else:\n",
    "            z = ((x - self.bias) / torch.exp(self.log_s)) * x_mask # inverse function & masking\n",
    "            log_det = None\n",
    "        \n",
    "        return z, log_det\n",
    "\n",
    "class InvertibleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder의 2번째 모듈\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        Q = torch.linalg.qr(torch.FloatTensor(4, 4).normal_())[0] # (4, 4)\n",
    "        \"\"\"\n",
    "        torch.FloatTensor(4, 4).normal_(): 정규분포 N(0, 1)에서 무작위로 추출한 4x4 matrix\n",
    "        Q, R = torch.linalg.qr(W): QR분해 | Q: 직교 행렬, R: upper traiangular 행렬 cf. det(Q) = 1 or -1\n",
    "        \"\"\"\n",
    "        if torch.det(Q) < 0:\n",
    "            Q[:, 0] = -1 * Q[:, 0] # 0번째 열의 부호를 바꿔서 det(Q) = -1로 만든다.\n",
    "        self.W = nn.Parameter(Q)\n",
    "    \n",
    "    def forward(self, x, x_mask, reverse=False):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        x: (B, 160, F//2)\n",
    "        x_mask: (B, 1, F//2)\n",
    "        =====outputs=====\n",
    "        z: (B, 160, F//2)\n",
    "        log_det: (B) or None\n",
    "        \"\"\"\n",
    "        B, C, f = x.size() # B, 160, F//2\n",
    "        x_len = torch.sum(x_mask, [1, 2]) # (B)\n",
    "        \n",
    "        # channel mixing\n",
    "        x = x.view(B, 2, C//4, 2, f) # (B, 2, 40, 2, F//2)\n",
    "        x = x.permute(0, 1, 3, 2, 4).contiguous() # (B, 2, 2, 40, F//2)\n",
    "        x = x.view(B, 4, C//4, f) # (B, 4, 40, F//2)\n",
    "        \n",
    "        # 편의상 log_det부터 구한다.\n",
    "        if not reverse:\n",
    "            weight = self.W\n",
    "            log_det = (C/4) * x_len * torch.logdet(self.W) # (B) | torch.logdet(W): log(det(W))\n",
    "                # height = C/4, width = x_len 인 상황임을 고려하면 Glow 논문의 log_determinant 식과 같다.\n",
    "        else:\n",
    "            weight = torch.linalg.inv(self.W) # inverse matrix\n",
    "            log_det = None\n",
    "        \n",
    "        weight = weight.view(4, 4, 1, 1)\n",
    "        z = F.conv2d(x, weight) # (B, 4, 40, F//2) * (4, 4, 1, 1) -> (B, 4, 40, F//2)\n",
    "        \"\"\"\n",
    "        F.conv2d(x, weight)의 convolution 연산은 다음과 같이 생각해야 한다.\n",
    "        (B, 4, 40, F//2): (batch_size, in_channels, height, width)\n",
    "        (4, 4, 1, 1): (out_channels, in_channels/groups, kernel_height, kernel_width)\n",
    "        \n",
    "        즉, nn.Conv2d(4, 4, kernel_size=(1, 1))인 상황에 가중치를 준 것이다.\n",
    "        \"\"\"\n",
    "        \n",
    "        # channel unmixing\n",
    "        z = z.view(B, 2, 2, C//4, f) # (B, 4, 40, F//2) -> (B, 2, 2, 40, F//2)\n",
    "        z = z.permute(0, 1, 3, 2, 4).contiguous() # (B, 2, 40, 2, F//2)\n",
    "        z = z.view(B, C, f) * x_mask # (B, 160, F//2) & masking\n",
    "        return z, log_det\n",
    "    \n",
    "class WN(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder의 3번째 모듈인 AffineCouplingLayer의 모듈\n",
    "    \n",
    "    해당 구조는 WAVEGLOW: A FLOW-BASED GENERATIVE NETWORK FOR SPEECH SYNTHESIS 로부터 제안되었다.\n",
    "    WaveGlow 논문: https://arxiv.org/pdf/1811.00002.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, dilation_rate=1):\n",
    "        super().__init__()\n",
    "        self.in_layers = nn.ModuleList()\n",
    "        self.res_skip_layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(4):\n",
    "            dilation = dilation_rate ** i # NVIDIA WaveGlow에서는 dilation_rate=2이지만, 여기에서는 1이므로 의미는 없다.\n",
    "            in_layer = weight_norm(nn.Conv1d(192, 2*192, kernel_size=5, dilation=dilation,\n",
    "                                 padding=((5-1) * dilation)//2)) # (B, 192, F//2) -> (B, 2*192, F//2)\n",
    "            self.in_layers.append(in_layer)\n",
    "            \n",
    "            if i < 3:\n",
    "                res_skip_layer = weight_norm(nn.Conv1d(192, 2*192, kernel_size=1)) # (B, 192, F//2) -> (B, 2*192, F//2)\n",
    "            else:\n",
    "                res_skip_layer = weight_norm(nn.Conv1d(192, 192, kernel_size=1)) # (B, 192, F//2) -> (B, 192, F//2)\n",
    "            self.res_skip_layers.append(res_skip_layer)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.05)\n",
    "    \n",
    "    def forward(self, x, x_mask):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        x: (B, 192, F//2)\n",
    "        x_mask: (B, 1, F//2)\n",
    "        =====outputs=====\n",
    "        output: (B, 192, F//2)\n",
    "        \"\"\"\n",
    "        output = torch.zeros_like(x) # (B, 192, F//2) all zeros\n",
    "        \n",
    "        for i in range(4):\n",
    "            x_in = self.in_layers[i](x) # (B, 192, F//2) -> (B, 2*192, F//2)\n",
    "            x_in = self.dropout(x_in) # dropout\n",
    "            \n",
    "            # fused add tanh sigmoid multiply\n",
    "            tanh_act = torch.tanh(x_in[:, :192, :]) # (B, 192, F//2)\n",
    "            sigmoid_act = torch.sigmoid(x_in[:, 192:, :]) # (B, 192, F//2)\n",
    "            \n",
    "            acts = sigmoid_act * tanh_act # (B, 192, F//2)\n",
    "            \n",
    "            x_out = self.res_skip_layers[i](acts) # (B, 192, F//2) -> (B, 2*192, F//2) or [last](B, 192, F//2)\n",
    "            if i < 3:\n",
    "                x = (x + x_out[:, :192, :]) * x_mask # residual connection & masking\n",
    "                output += x_out[:, 192:, :] # add output\n",
    "            else:\n",
    "                output += x_out # (B, 192, F//2)\n",
    "        \n",
    "        output = output * x_mask # masking\n",
    "        return output\n",
    "\n",
    "class AffineCouplingLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder의 3번째 모듈\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.start_conv = weight_norm(nn.Conv1d(160//2, 192, kernel_size=1)) # (B, 80, F//2) -> (B, 192, F//2)\n",
    "        self.wn = WN()\n",
    "        self.end_conv = nn.Conv1d(192, 160, kernel_size=1) # (B, 192, F//2) -> (B, 160, F//2)\n",
    "        # end_conv의 초기 가중치를 0으로 설정하는 것이 처음에 학습하지 않는 역할을 하며, 이는 학습 안정화에 도움이 된다.\n",
    "        self.end_conv.weight.data.zero_() # weight를 0으로 초기화\n",
    "        self.end_conv.bias.data.zero_() # bias를 0으로 초기화\n",
    "        \n",
    "    def forward(self, x, x_mask, reverse=False):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        x: (B, 160, F//2)\n",
    "        x_mask: (B, 1, F//2)\n",
    "        =====outputs=====\n",
    "        z: (B, 160, F//2)\n",
    "        log_det: (B) or None\n",
    "        \"\"\"\n",
    "        B, C, f = x.size() # B, 160, F//2\n",
    "        x_0, x_1 = x[:, :C//2, :], x[:, C//2:, :] # split: (B, 80, F//2) x2\n",
    "        \n",
    "        x = self.start_conv(x_0) * x_mask # (B, 80, F//2) -> (B, 192, F//2) & masking\n",
    "        x = self.wn(x, x_mask) # (B, 192, F//2)\n",
    "        out = self.end_conv(x) # (B, 192, F//2) -> (B, 160, F//2)\n",
    "        \n",
    "        z_0 = x_0 # (B, 80, F//2)\n",
    "        m = out[:, :C//2, :] # (B, 80, F//2)\n",
    "        log_s = out[:, C//2:, :] # (B, 80, F//2)\n",
    "        \n",
    "        if not reverse:\n",
    "            z_1 = (torch.exp(log_s) * x_1 + m) * x_mask # (B, 80, F//2) | function & masking \n",
    "            log_det = torch.sum(log_s * x_mask, [1, 2]) # (B)\n",
    "        else:\n",
    "            z_1 = (x_1 - m) / torch.exp(log_s) * x_mask # (B, 80, F//2) | inverse function & masking\n",
    "            log_det = None\n",
    "        \n",
    "        z = torch.cat([z_0, z_1], dim=1) # (B, 160, F//2)\n",
    "        return z, log_det\n",
    "    \n",
    "def Unsqueeze(x, x_mask):\n",
    "    \"\"\"\n",
    "    Decoder의 postprocessing\n",
    "    =====inputs=====\n",
    "    x: (B, 160, F//2)\n",
    "    x_mask: (B, 1, F//2)\n",
    "    =====outputs=====\n",
    "    x: (B, 80, F)\n",
    "    x_mask: (B, 1, F)\n",
    "    \"\"\"\n",
    "    B, C, f = x.size() # B, 160, F//2\n",
    "    x = x.view(B, 2, C//2, f) # (B, 2, 80, F//2)\n",
    "    x = x.permute(0, 2, 3, 1).contiguous() # (B, 80, F//2, 2)\n",
    "    x = x.view(B, C//2, 2*f) # (B, 160, F)\n",
    "    \n",
    "    x_mask = x_mask.unsqueeze(3).repeat(1, 1, 1, 2).view(B, 1, 2*f) # (B, 1, F//2, 1) -> (B, 1, F//2, 2) -> (B, 1, F)\n",
    "    x = x * x_mask # masking\n",
    "    return x, x_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdba03d9",
   "metadata": {},
   "source": [
    "### 2.2.2. Main Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74f4689c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flows = nn.ModuleList()\n",
    "        for i in range(12):\n",
    "            self.flows.append(ActNorm())\n",
    "            self.flows.append(InvertibleConv())\n",
    "            self.flows.append(AffineCouplingLayer())\n",
    "        \n",
    "    def forward(self, x, x_mask, reverse=False):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        x: (B, 80, F) | mel-spectrogram(Direct) OR latent representation(Reverse)\n",
    "        x_mask: (B, 1, F)\n",
    "        =====outputs=====\n",
    "        z: (B, 80, F) | latent representation(Direct) OR mel-spectrogram(Reverse)\n",
    "        total_log_det: (B) or None | log determinant\n",
    "        \"\"\"\n",
    "        if not reverse:\n",
    "            flows = self.flows\n",
    "            total_log_det = 0\n",
    "        else:\n",
    "            flows = reversed(self.flows)\n",
    "            total_log_det = None\n",
    "        \n",
    "        x, x_mask = Squeeze(x, x_mask) # (B, 80, F) -> (B, 160, F//2) | (B, 1, F) -> (B, 1, F//2)\n",
    "        \n",
    "        for f in flows:\n",
    "            if not reverse:\n",
    "                x, log_det = f(x, x_mask, reverse=reverse)\n",
    "                total_log_det += log_det\n",
    "            else:\n",
    "                x, _ = f(x, x_mask, reverse=reverse)\n",
    "                \n",
    "        x, x_mask = Unsqueeze(x, x_mask) # (B, 160, F//2) -> (B, 80, F) | (B, 1, F//2) -> (B, 1, F)\n",
    "        \n",
    "        return x, total_log_det"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdeb2f0",
   "metadata": {},
   "source": [
    "## 2.3. Generator (Encoder+Decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8ee5bd",
   "metadata": {},
   "source": [
    "### 2.3.1. Generator Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93b25065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAS(path, logp, T_max, F_max):\n",
    "    \"\"\"\n",
    "    Glow-TTS의 모듈인 maximum_path의 모듈\n",
    "    MAS 알고리즘을 수행하는 함수이다.\n",
    "    =====inputs=====\n",
    "    path: (T, F)\n",
    "    logp: (T, F)\n",
    "    T_max: (1)\n",
    "    F_max: (1)\n",
    "    =====outputs=====\n",
    "    path: (T, F) | 0과 1로 구성된 alignment\n",
    "    \"\"\"\n",
    "    neg_inf = -1e9 # negative infinity\n",
    "    # forward\n",
    "    for j in range(F_max):\n",
    "        for i in range(max(0, T_max + j - F_max), min(T_max, j + 1)): # 평행사변형을 생각하라.\n",
    "            # Q_i_j-1 (current)\n",
    "            if i == j:\n",
    "                Q_cur = neg_inf\n",
    "            else:\n",
    "                Q_cur = logp[i, j-1] # j=0이면 i도 0이므로 j-1을 사용해도 된다.\n",
    "            \n",
    "            # Q_i-1_j-1 (previous)\n",
    "            if i==0:\n",
    "                if j==0:\n",
    "                    Q_prev = 0. # i=0, j=0인 경우에는 logp 값만 반영해야 한다.\n",
    "                else:\n",
    "                    Q_prev = neg_inf # i=0인 경우에는 Q_i-1_j-1을 반영하지 않아야 한다.\n",
    "            else:\n",
    "                Q_prev = logp[i-1, j-1]\n",
    "            \n",
    "            # logp에 Q를 갱신한다.\n",
    "            logp[i, j] = max(Q_cur, Q_prev) + logp[i, j]\n",
    "    \n",
    "    # backtracking\n",
    "    idx = T_max - 1\n",
    "    for j in range(F_max-1, -1, -1): # F_max-1부터 -1까지(-1 포함 없이 0까지) -1씩 감소\n",
    "        path[idx, j] = 1\n",
    "        if idx != 0:\n",
    "            if (logp[idx, j-1] < logp[idx-1, j-1]) or (idx == j):\n",
    "                idx -= 1\n",
    "    \n",
    "    return path\n",
    "        \n",
    "\n",
    "def maximum_path(logp, attention_mask):\n",
    "    \"\"\"\n",
    "    Glow-TTS에 사용되는 모듈\n",
    "    MAS를 사용하여 alignment를 찾아주는 역할을 한다.\n",
    "    논문 저자 구현에서는 cpython을 이용하여 병렬 처리를 구현한 듯 하나\n",
    "    여기에서는 python만을 이용하여 구현하였다.\n",
    "    =====inputs=====\n",
    "    logp: (B, T, F) | N(x_mean, x_std)의 log-likelihood\n",
    "    attention_mask: (B, T, F)\n",
    "    =====outputs=====\n",
    "    path: (B, T, F) | alignment\n",
    "    \"\"\"\n",
    "    B = logp.shape[0]\n",
    "    \n",
    "    logp = logp * attention_mask\n",
    "    # 계산은 CPU에서 실행되도록 하기 위해 기존의 device를 저장하고 .cpu().numpy()를 한다.\n",
    "    logp_device = logp.device\n",
    "    logp_type = logp.dtype\n",
    "    logp = logp.data.cpu().numpy().astype(np.float32)\n",
    "    attention_mask = attention_mask.data.cpu().numpy()\n",
    "    \n",
    "    path = np.zeros_like(logp).astype(np.int32) # (B, T, F)\n",
    "    T_max = attention_mask.sum(1)[:, 0].astype(np.int32) # (B)\n",
    "    F_max = attention_mask.sum(2)[:, 0].astype(np.int32) # (B)\n",
    "    \n",
    "    # MAS 알고리즘\n",
    "    for idx in range(B):\n",
    "        path[idx] = MAS(path[idx], logp[idx], T_max[idx], F_max[idx]) # (T, F)\n",
    "    return torch.from_numpy(path).to(device=logp_device, dtype=logp_type)\n",
    "\n",
    "def generate_path(ceil_dur, attention_mask):\n",
    "    \"\"\"\n",
    "    Glow-TTS에 사용되는 모듈\n",
    "    inference 과정에서 alignment를 만들어낸다.\n",
    "    =====input=====\n",
    "    ceil_dur: (B, T) | 추론한 duration에 ceil 연산한 것 | ex) [[2, 1, 2, 2, ...], [1, 2, 1, 3, ...], ...]\n",
    "    attention_mask: (B, T, F)\n",
    "    =====output=====\n",
    "    path: (B, T, F) | alignment\n",
    "    \"\"\"\n",
    "    B, T, Frame = attention_mask.shape\n",
    "    cum_dur = torch.cumsum(ceil_dur, 1)\n",
    "    cum_dur = cum_dur.to(torch.int32) # (B, T) | 누적합 | ex) [[2, 3, 5, 7, ...], [1, 3, 4, 7, ...], ...]\n",
    "    path = torch.zeros(B, T, Frame).to(ceil_dur.device) # (B, T, F) | all False(0)\n",
    "    \n",
    "    # make the sequence_mask\n",
    "    for b, batch_cum_dur in enumerate(cum_dur):\n",
    "        for t, each_cum_dur in enumerate(batch_cum_dur):\n",
    "            path[b, t, :each_cum_dur] = torch.ones((1, 1, each_cum_dur)).to(ceil_dur.device)\n",
    "                # cum_dur로부터 True(1)를 path에 새겨넣는다.\n",
    "    path = path - F.pad(path, (0, 0, 1, 0, 0, 0))[:, :-1] # (B, T, F)\n",
    "    \"\"\"\n",
    "    ex) batch를 잠시 제외해두고 예시를 든다.\n",
    "    [[1, 1, 0, 0, 0, 0, 0],   [[0, 0, 0, 0, 0, 0, 0],    [[1, 1, 0, 0, 0, 0, 0],\n",
    "     [1, 1, 1, 0, 0, 0, 0], -  [1, 1, 0, 0, 0, 0, 0],  =  [0, 0, 1, 0, 0, 0, 0],\n",
    "     [1, 1, 1, 1, 1, 0, 0],    [1, 1, 1, 0, 0, 0, 0],     [0, 0, 0, 1, 1, 0, 0],\n",
    "     [1, 1, 1, 1, 1, 1, 1]]    [1, 1, 1, 1, 1, 0, 0]]     [0, 0, 0, 0, 0, 1, 1]]\n",
    "    \"\"\"\n",
    "    path = path * attention_mask\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fa32dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 아래 논문의 구현이 훨씬 빠르다. 이 논문 구현을 보고 위의 구현을 변경할 필요가 있다. #####\n",
    "def maximum_path(value, mask, max_neg_val=-np.inf):\n",
    "    \"\"\" Numpy-friendly version. It's about 4 times faster than torch version.\n",
    "    value: [b, t_x, t_y]\n",
    "    mask: [b, t_x, t_y]\n",
    "    \"\"\"\n",
    "    value = value * mask\n",
    "\n",
    "    device = value.device\n",
    "    dtype = value.dtype\n",
    "    value = value.cpu().detach().numpy()\n",
    "    mask = mask.cpu().detach().numpy().astype(bool)\n",
    "\n",
    "    b, t_x, t_y = value.shape\n",
    "    direction = np.zeros(value.shape, dtype=np.int64)\n",
    "    v = np.zeros((b, t_x), dtype=np.float32)\n",
    "    x_range = np.arange(t_x, dtype=np.float32).reshape(1,-1)\n",
    "    for j in range(t_y):\n",
    "        v0 = np.pad(v, [[0,0],[1,0]], mode=\"constant\", constant_values=max_neg_val)[:, :-1]\n",
    "        v1 = v\n",
    "        max_mask = (v1 >= v0)\n",
    "        v_max = np.where(max_mask, v1, v0)\n",
    "        direction[:, :, j] = max_mask\n",
    "\n",
    "        index_mask = (x_range <= j)\n",
    "        v = np.where(index_mask, v_max + value[:, :, j], max_neg_val)\n",
    "    direction = np.where(mask, direction, 1)\n",
    "\n",
    "    path = np.zeros(value.shape, dtype=np.float32)\n",
    "    index = mask[:, :, 0].sum(1).astype(np.int64) - 1\n",
    "    index_range = np.arange(b)\n",
    "    for j in reversed(range(t_y)):\n",
    "        path[index_range, index, j] = 1\n",
    "        index = index + direction[index_range, index, j] - 1\n",
    "    path = path * mask.astype(np.float32)\n",
    "    path = torch.from_numpy(path).to(device=device, dtype=dtype)\n",
    "    return path\n",
    "\n",
    "\n",
    "def generate_path(duration, mask):\n",
    "    \"\"\"\n",
    "    duration: [b, t_x]\n",
    "    mask: [b, t_x, t_y]\n",
    "    \"\"\"\n",
    "    device = duration.device\n",
    "\n",
    "    b, t_x, t_y = mask.shape # (B, T, F)\n",
    "    cum_duration = torch.cumsum(duration, 1) # 누적합, (B, T) \n",
    "    path = torch.zeros(b, t_x, t_y, dtype=mask.dtype).to(device=device) # (B, T, F)\n",
    "\n",
    "    cum_duration_flat = cum_duration.view(b * t_x) # (B*T)\n",
    "    path = sequence_mask(cum_duration_flat, t_y).to(mask.dtype) # (B*T, F)\n",
    "    path = path.view(b, t_x, t_y) # (B, T, F)\n",
    "    path = path.to(torch.float32)\n",
    "    path = path - F.pad(path, convert_pad_shape([[0, 0], [1, 0], [0, 0]]))[:,:-1] # (B, T, F) # T의 차원 맨 앞을 -1한다.\n",
    "    path = path * mask\n",
    "    return path\n",
    "\n",
    "def sequence_mask(length, max_length=None):\n",
    "    if max_length is None:\n",
    "        max_length = length.max()\n",
    "    x = torch.arange(max_length, dtype=length.dtype, device=length.device)\n",
    "    return x.unsqueeze(0) < length.unsqueeze(1)\n",
    "\n",
    "def convert_pad_shape(pad_shape):\n",
    "    l = pad_shape[::-1] # [[0, 0], [p, p], [0, 0]]\n",
    "    pad_shape = [item for sublist in l for item in sublist] # [0, 0, p, p, 0, 0]\n",
    "    return pad_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dfaa46",
   "metadata": {},
   "source": [
    "### 2.3.2. Main Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6664fe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlowTTS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        \n",
    "    def forward(self, text, text_len, mel=None, mel_len=None, inference=False):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        text: (B, T)\n",
    "        text_len: (B) list\n",
    "        mel: (B, 80, F)\n",
    "        mel_len: (B) list\n",
    "        inference: True/False\n",
    "        =====outputs=====\n",
    "        (tuple) (z, z_mean, z_log_std, log_det, z_mask)\n",
    "            z(training) or y(inference): (B, 80, F) | z: latent representation, y: mel-spectrogram\n",
    "            z_mean: (B, 80, F)\n",
    "            z_log_std: (B, 80, F)\n",
    "            log_det: (B) or None\n",
    "            z_mask: (B, 1, F)\n",
    "        (tuple) (x_mean, x_log_std, x_mask)\n",
    "            x_mean: (B, 80, T)\n",
    "            x_log_std: (B, 80, T)\n",
    "            x_mask: (B, 1, T)\n",
    "        (tuple) (attention_alignment, x_log_dur, log_d)\n",
    "            attention_alignment: (B, T, F)\n",
    "            x_log_dur: (B, 1, T) | 추측한 duration의 log scale\n",
    "            log_d: (B, 1, T) | 적절하다고 추측한 alignment에서의 duration의 log scale\n",
    "        \"\"\"\n",
    "        x_mean, x_log_std, x_log_dur, x_mask = self.encoder(text, text_len)\n",
    "            # x_std, x_dur 에 log를 붙인 이유는, 논문 저자의 구현에서는 log가 취해진 값으로 간주하기 때문이다.\n",
    "        y, y_len = mel, mel_len\n",
    "        \n",
    "        if not inference: # training\n",
    "            y_max_len = y.size(2)\n",
    "        else: # inference\n",
    "            dur = torch.exp(x_log_dur) * x_mask # (B, 1, T)\n",
    "            ceil_dur = torch.ceil(dur) # (B, 1, T)\n",
    "            y_len = torch.clamp_min(torch.sum(ceil_dur, [1, 2]), 1).long() # (B)\n",
    "                # ceil_dur을 [1, 2] 축에 대해 sum한 뒤 최솟값이 1이상이 되도록 설정. 정수 long 타입으로 반환한다.\n",
    "            y_max_len = None\n",
    "        \n",
    "        # preprocessing\n",
    "        if y_max_len is not None:\n",
    "            y_max_len = (y_max_len // 2) * 2 # 홀수면 1을 빼서 짝수로 만든다.\n",
    "            y = y[:, :, :y_max_len] # y_max_len에 맞게 y를 조정\n",
    "            y_len = (y_len // 2) * 2 # y_len이 홀수이면 1을 빼서 짝수로 만든다.\n",
    "        \n",
    "        # make the z_mask\n",
    "        B = len(y_len)\n",
    "        temp_max = max(y_len)\n",
    "        z_mask = torch.zeros((B, 1, temp_max), dtype=torch.bool).to(device) # (B, 1, F)\n",
    "        for idx, length in enumerate(y_len):\n",
    "            z_mask[idx, :, :length] = True\n",
    "        \n",
    "        # make the attention_mask\n",
    "        attention_mask = x_mask.unsqueeze(3) * z_mask.unsqueeze(2) # (B, 1, T, 1) * (B, 1, 1, F) = (B, 1, T, F)\n",
    "            # 주의: Encoder의 attention_mask와는 다른 mask임.\n",
    "        \n",
    "        if not inference: # training\n",
    "            z, log_det = self.decoder(y, z_mask, reverse=False)\n",
    "            with torch.no_grad():\n",
    "                x_std_squared_root = torch.exp(-2 * x_log_std) # (B, 80, T)\n",
    "                logp1 = torch.sum(-0.5 * math.log(2 * math.pi) - x_log_std, [1]).unsqueeze(-1) # [(B, T, F)\n",
    "                logp2 = torch.matmul(x_std_squared_root.transpose(1, 2), -0.5 * (z ** 2)) # [(B, T, 80) * (B, 80, F) = (B, T, F)\n",
    "                logp3 = torch.matmul((x_mean * x_std_squared_root).transpose(1,2), z) # (B, T, 80) * (B, 80, F) = (B, T, F)\n",
    "                logp4 = torch.sum(-0.5 * (x_mean ** 2) * x_std_squared_root, [1]).unsqueeze(-1) # (B, T, F)\n",
    "                logp = logp1 + logp2 + logp3 + logp4 # (B, T, F)\n",
    "                \"\"\"\n",
    "                logp는 normal distribution N(x_mean, x_std)의 maximum log-likelihood이다.\n",
    "                sum(log(N(z;x_mean, x_std)))를 정규분포 식을 이용하여 분배법칙으로 풀어내면 위와 같은 식이 도출된다.\n",
    "                \"\"\"\n",
    "                attention_alignment = maximum_path(logp, attention_mask.squeeze(1)).detach() # alignment (B, T, F)\n",
    "             \n",
    "            z_mean = torch.matmul(attention_alignment.transpose(1, 2), x_mean.transpose(1, 2)) # (B, F, T) * (B, T, 80) -> (B, F, 80)\n",
    "            z_mean = z_mean.transpose(1, 2) # (B, 80, F)\n",
    "            z_log_std = torch.matmul(attention_alignment.transpose(1, 2), x_log_std.transpose(1, 2)) # (B, F, T) * (B, T, 80) -> (B, F, 80)\n",
    "            z_log_std = z_log_std.transpose(1, 2) # (B, 80, F)\n",
    "            log_d = torch.log(1e-8 + torch.sum(attention_alignment, -1)).unsqueeze(1) * x_mask # (B, 1, T) | alignment에서 형성된 duration의 log scale\n",
    "            return (z, z_mean, z_log_std, log_det, z_mask), (x_mean, x_log_std, x_mask), (attention_alignment, x_log_dur, log_d)\n",
    "            \n",
    "        else: # inference\n",
    "            # generate_path (make attention_alignment using ceil(x_dur))\n",
    "            attention_alignment = generate_path(ceil_dur.squeeze(1), attention_mask.squeeze(1)) # (B, T, F)\n",
    "            z_mean = torch.matmul(attention_alignment.transpose(1, 2), x_mean.transpose(1, 2)) # (B, F, T) * (B, T, 80) -> (B, F, 80)\n",
    "            z_mean = z_mean.transpose(1, 2) # (B, 80, F)\n",
    "            z_log_std = torch.matmul(attention_alignment.transpose(1, 2), x_log_std.transpose(1, 2)) # (B, F, T) * (B, T, 80) -> (B, F, 80)\n",
    "            z_log_std = z_log_std.transpose(1, 2) # (B, 80, F)\n",
    "            log_d = torch.log(1e-8 + torch.sum(attention_alignment, -1)).unsqueeze(1) * x_mask # (B, 1, T) | alignment에서 형성된 duration의 log scale\n",
    "            \n",
    "            z = (z_mean + torch.exp(z_log_std) * torch.randn_like(z_mean)) * z_mask # z(latent representation) 생성\n",
    "            y, log_det = self.decoder(z, z_mask, reverse=True) # mel-spectrogram 생성\n",
    "            return (y, z_mean, z_log_std, log_det, z_mask), (x_mean, x_log_std, x_mask), (attention_alignment, x_log_dur, log_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a1dfc4",
   "metadata": {},
   "source": [
    "# 3. HiFi-GAN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962413c8",
   "metadata": {},
   "source": [
    "## 3.1. Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7ba140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V2 model을 기준으로 한다.\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channels, kernel_size):\n",
    "        \"\"\"\n",
    "        channels: \n",
    "        kernel_size: 3, 7, 11 중 하나\n",
    "        \"\"\"\n",
    "        super(ResBlock, self).__init__()\n",
    "        # padding = (kernel_size-1)*dilation//2 (\"same\")\n",
    "        self.convs1 = nn.ModuleList([\n",
    "            weight_norm(nn.Conv1d(channels, channels, kernel_size, stride=1, dilation=1,\n",
    "                                  padding=(kernel_size-1)*1//2)),\n",
    "            weight_norm(nn.Conv1d(channels, channels, kernel_size, stride=1, dilation=1,\n",
    "                                  padding=(kernel_size-1)*1//2))\n",
    "        ])\n",
    "        self.convs2 = nn.ModuleList([\n",
    "            weight_norm(nn.Conv1d(channels, channels, kernel_size, stride=1, dilation=3,\n",
    "                                  padding=(kernel_size-1)*3//2)),\n",
    "            weight_norm(nn.Conv1d(channels, channels, kernel_size, stride=1, dilation=1,\n",
    "                                  padding=(kernel_size-1)*1//2))\n",
    "        ])\n",
    "        self.convs3 = nn.ModuleList([\n",
    "            weight_norm(nn.Conv1d(channels, channels, kernel_size, stride=1, dilation=5,\n",
    "                                  padding=(kernel_size-1)*5//2)),\n",
    "            weight_norm(nn.Conv1d(channels, channels, kernel_size, stride=1, dilation=1,\n",
    "                                  padding=(kernel_size-1)*1//2))\n",
    "        ])\n",
    "        self.modules = [self.convs1, self.convs2, self.convs3]\n",
    "        \n",
    "        # 평균이 0, 표준편차가 0.01인 정규분포로 가중치 초기화\n",
    "        for module in self.modules:\n",
    "            for conv in module:\n",
    "                nn.init.normal_(conv.weight, mean=0.0, std=0.01)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        x: (B, channels, F) # mel-spectrogram으로부터 얻어진 input features\n",
    "        =====outputs=====\n",
    "        x: (B, channels, F) # mel-spectrogram으로부터 얻어진 output features\n",
    "        \"\"\"\n",
    "        for module in self.modules:\n",
    "            for conv in module:\n",
    "                y = F.leaky_relu(x, 0.1)\n",
    "                y = conv(y)\n",
    "            x = x + y\n",
    "        return x\n",
    "\n",
    "class MRF(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        \"\"\"\n",
    "        channels: \n",
    "        \"\"\"\n",
    "        super(MRF, self).__init__()\n",
    "        self.res_blocks = nn.ModuleList([\n",
    "            ResBlock(channels, kernel_size=3),\n",
    "            ResBlock(channels, kernel_size=7),\n",
    "            ResBlock(channels, kernel_size=11),\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        x: (B, channels, F)\n",
    "        =====outputs=====\n",
    "        x: (B, channels, F)\n",
    "        \"\"\"\n",
    "        skip_list = []\n",
    "        for res_block in self.res_blocks:\n",
    "            skip_x = res_block(x)\n",
    "            skip_list.append(skip_x)\n",
    "        x = sum(skip_list) / len(self.res_blocks)\n",
    "        return x\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.pre_conv = weight_norm(nn.Conv1d(80, 128, kernel_size=7, stride=1, dilation=1,\n",
    "                                              padding=(7-1)//2)) # (B, 80, F) -> (B, 128, F)\n",
    "        nn.init.normal_(self.pre_conv.weight, mean=0.0, std=0.01) # 논문 저자 구현에는 없음.\n",
    "        \n",
    "        self.up_convs = nn.ModuleList()\n",
    "        self.mrfs = nn.ModuleList()\n",
    "        ku = [16, 16, 4, 4]\n",
    "        for i in range(4):\n",
    "            # ku//2 배 upsampling\n",
    "            channels = 128//(2**(i+1))\n",
    "            up_conv = weight_norm(nn.ConvTranspose1d(128//(2**i), channels, kernel_size=ku[i], stride=ku[i]//2,\n",
    "                                                     padding=(ku[i]-ku[i]//2)//2))\n",
    "                # (B, 128, F) -(1)-> (B, 64, F*8) -(2)-> (B, 32, F*8*8) -(3)-> (B, 16, F*8*8*2) -(4)-> (B, 8, F*8*8*2*2)\n",
    "            nn.init.normal_(up_conv.weight, mean=0.0, std=0.01)\n",
    "            self.up_convs.append(up_conv)\n",
    "            \n",
    "            # MRF\n",
    "            mrf = MRF(channels) # (B, channels, F) -> (B, channels, F)\n",
    "            self.mrfs.append(mrf)\n",
    "            \n",
    "        self.post_conv = weight_norm(nn.Conv1d(8, 1, kernel_size=7, stride=1, dilation=1,\n",
    "                                               padding=(7-1)//2)) # (B, 8, F*256) -> (B, 1, F*256)\n",
    "        nn.init.normal_(self.post_conv.weight, mean=0.0, std=0.01)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        x: (B, 80, F) # mel_spectrogram\n",
    "        =====outputs=====\n",
    "        x: (B, 1, F*256) # waveform\n",
    "        \"\"\"\n",
    "        x = self.pre_conv(x) # (B, 80, F) -> (B, 128, F)\n",
    "        for i in range(4):\n",
    "            x = F.leaky_relu(x, 0.1)\n",
    "            x = self.up_convs[i](x)\n",
    "            x = self.mrfs[i](x)\n",
    "            # final: (B, 128, F) -> (B, 8, F*256)\n",
    "        x = F.leaky_relu(x, 0.1)\n",
    "        x = self.post_conv(x) # (B, 8, F*256) -> (B, 1, F*256)\n",
    "        x = torch.tanh(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b61a4c",
   "metadata": {},
   "source": [
    "## 3.2. Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca588f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubPD(nn.Module):\n",
    "    def __init__(self, period):\n",
    "        \"\"\"\n",
    "        period: 2, 3, 5, 7, 11 중 하나\n",
    "        \"\"\"\n",
    "        super(SubPD, self).__init__()\n",
    "        self.period = period\n",
    "        \n",
    "        self.convs = nn.ModuleList()\n",
    "        channels = 1\n",
    "        for i in range(1, 5): # 논문 저자의 변형 구현 대신 논문대로 구현함.\n",
    "            conv = weight_norm(nn.Conv2d(channels, 2**(5+i), kernel_size=(5, 1), stride=(3, 1), dilation=1, padding=0))\n",
    "            self.convs.append(conv)\n",
    "            channels = 2**(5+i)\n",
    "            # (B, 1, [T/p]+1, p) -(1)-> (B, 64, ?, p) -(2)-> (B, 128, ?, p) -(3)-> (B, 256, ?, p) -(4)-> (B, 512, ?, p)\n",
    "        last_conv = weight_norm(nn.Conv2d(channels, 1024, kernel_size=(5, 1), stride=(1, 1), dilation=1,\n",
    "                                          padding=(2, 0))) # (B, 512, ?, p) -> (B, 1024, ?, p)\n",
    "        self.convs.append(last_conv)\n",
    "        \n",
    "        self.post_conv = weight_norm(nn.Conv2d(1024, 1, kernel_size=(3, 1), stride=(1, 1), dilation=1,\n",
    "                                               padding=(1, 0))) # (B, 1024, ?, p) -> (B, 1, ?, p)\n",
    "        \n",
    "    def forward(self, waveform):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        waveform: (B, 1, T)\n",
    "        =====outputs=====\n",
    "        x: (B, ?) # flatten된 real/fake 벡터 (0~1(?))\n",
    "        features: feature를 모두 모아놓은 list (Feature Matching Loss를 계산하기 위함.)\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        \n",
    "        B, _, T = waveform.size()\n",
    "        P = self.period\n",
    "        # padding\n",
    "        if T % P != 0:\n",
    "            padding = P - (T % P)\n",
    "            waveform = F.pad(waveform, (0, padding), \"reflect\") # 앞쪽에 0, 뒤쪽에 padding만큼 패딩, reflect는 마치 거울에 반사되듯이 패딩함.\n",
    "                # ex) [1, 2, 3, 4, 5]를 앞쪽에 2, 뒤쪽에 3만큼 reflect 모드로 padding -> [3, 2, 1, 2, 3, 4, 5, 4, 3, 2]\n",
    "            T += padding\n",
    "        # reshape\n",
    "        x = waveform.view(B, 1, T//P, P) # (B, 1, [T/P]+1, P)\n",
    "        \n",
    "        for conv in self.convs:\n",
    "            x = conv(x)\n",
    "            x = F.leaky_relu(x, 0.1)\n",
    "            features.append(x)\n",
    "        x = self.post_conv(x)\n",
    "        features.append(x)\n",
    "        x = torch.flatten(x, 1, -1) # index 1번째 차원부터 마지막 차원까지 flatten | (B, ?)\n",
    "        return x, features\n",
    "\n",
    "class MPD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MPD, self).__init__()\n",
    "        self.sub_pds = nn.ModuleList([\n",
    "            SubPD(2), SubPD(3), SubPD(5), SubPD(7), SubPD(11), \n",
    "        ]) # (B, 1, T) -> (B, ?), features list\n",
    "        \n",
    "    def forward(self, real_waveform, gen_waveform):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        real_waveform: (B, 1, T) # 실제 음성\n",
    "        gen_waveform: (B, 1, T) # 생성 음성\n",
    "        =====outputs=====\n",
    "        real_outputs: (B, ?) list (len=5) # 실제 음성에 대한 SubPD outputs list\n",
    "        gen_outputs: (B, ?) list # 생성 음성에 대한 SubPD outputs list\n",
    "        real_features: features list # 실제 음성에 대한 SubPD features list\n",
    "        gen_features: features list # 생성 음성에 대한 SubPD features list\n",
    "        \"\"\"\n",
    "        real_outputs, gen_outputs, real_features, gen_features = [], [], [], []\n",
    "        for sub_pd in self.sub_pds:\n",
    "            real_output, real_feature = sub_pd(real_waveform)\n",
    "            gen_output, gen_feature = sub_pd(gen_waveform)\n",
    "            real_outputs.append(real_output)\n",
    "            gen_outputs.append(gen_output)\n",
    "            real_features.append(real_feature)\n",
    "            gen_features.append(gen_feature)\n",
    "        return real_outputs, gen_outputs, real_features, gen_features\n",
    "    \n",
    "class SubSD(nn.Module):\n",
    "    def __init__(self, first=False):\n",
    "        \"\"\"\n",
    "        first: boolean (first가 True이면 spectral normalization을 적용한다.)\n",
    "        \"\"\"\n",
    "        super(SubSD, self).__init__()\n",
    "        norm = spectral_norm if first else weight_norm # first가 True이면 spectral_norm, 그렇지 않으면 weight_norm\n",
    "        self.convs = nn.ModuleList([ # Mel-GAN 논문에 맞게 구현\n",
    "            norm(nn.Conv1d(1, 16, kernel_size=15, stride=1, padding=(15-1)//2)), # (B, 1, T) -> (B, 16, T)\n",
    "            norm(nn.Conv1d(16, 64, kernel_size=41, stride=4, groups=4, padding=(41-1)//2)), # (B, 16, T) -> (B, 64, T/4(?))\n",
    "            norm(nn.Conv1d(64, 256, kernel_size=41, stride=4, groups=16, padding=(41-1)//2)), # (B, 64, T/4(?)) -> (B, 256, T/16(?))\n",
    "            norm(nn.Conv1d(256, 1024, kernel_size=41, stride=4, groups=64, padding=(41-1)//2)), # (B, 256, T/16(?)) -> (B, 1024, T/64(?))\n",
    "            norm(nn.Conv1d(1024, 1024, kernel_size=41, stride=4, groups=256, padding=(41-1)//2)), # (B, 1024, T/64(?)) -> (B, 1024, T/256(?))\n",
    "            norm(nn.Conv1d(1024, 1024, kernel_size=5, stride=1, padding=(5-1)//2)) # (B, 1024, T/256(?)) -> (B, 1024, T/256(?))\n",
    "        ])\n",
    "        self.post_conv = norm(nn.Conv1d(1024, 1, kernel_size=3, stride=1, padding=(3-1)//2)) # (B, 1024, ?) -> (B, 1, ?)\n",
    "        \n",
    "    def forward(self, waveform):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        waveform: (B, 1, T)\n",
    "        =====outputs=====\n",
    "        x: (B, ?) # flatten된 real/fake 벡터 (0~1(?))\n",
    "        features: feature를 모두 모아놓은 list (Feature Matching Loss를 계산하기 위함.)\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        x = waveform\n",
    "        for conv in self.convs:\n",
    "            x = conv(x)\n",
    "            x = F.leaky_relu(x, 0.1)\n",
    "            features.append(x)\n",
    "        x = self.post_conv(x) # (B, 1, ?)\n",
    "        features.append(x)\n",
    "        x = x.squeeze(1) # (B, ?)\n",
    "        return x, features\n",
    "    \n",
    "class MSD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MSD, self).__init__()\n",
    "        self.sub_sds = nn.ModuleList([\n",
    "            SubSD(first=True), SubSD(), SubSD() \n",
    "        ]) # (B, 1, T) -> (B, ?), features list\n",
    "        self.avgpool = nn.AvgPool1d(kernel_size=4, stride=2, padding=2) # x2 down sampling\n",
    "        \n",
    "    def forward(self, real_waveform, gen_waveform):\n",
    "        \"\"\"\n",
    "        =====inputs=====\n",
    "        real_waveform: (B, 1, T) # 실제 음성\n",
    "        gen_waveform: (B, 1, T) # 생성 음성\n",
    "        =====outputs=====\n",
    "        real_outputs: (B, ?) list (len=3) # 실제 음성에 대한 SubSD outputs list\n",
    "        gen_outputs: (B, ?) list # 생성 음성에 대한 SubSD outputs list\n",
    "        real_features: features list # 실제 음성에 대한 SubSD features list\n",
    "        gen_features: features list # 생성 음성에 대한 SubSD features list\n",
    "        \"\"\"\n",
    "        real_outputs, gen_outputs, real_features, gen_features = [], [], [], []\n",
    "        for idx, sub_sd in enumerate(self.sub_sds):\n",
    "            if idx != 0:\n",
    "                real_waveform = self.avgpool(real_waveform)\n",
    "                gen_waveform = self.avgpool(gen_waveform)\n",
    "            real_output, real_feature = sub_sd(real_waveform)\n",
    "            gen_output, gen_feature = sub_sd(gen_waveform)\n",
    "            real_outputs.append(real_output)\n",
    "            gen_outputs.append(gen_output)\n",
    "            real_features.append(real_feature)\n",
    "            gen_features.append(gen_feature)\n",
    "        return real_outputs, gen_outputs, real_features, gen_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782a64f0",
   "metadata": {},
   "source": [
    "# 4. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bad54c",
   "metadata": {},
   "source": [
    "## 4.0. Plot mel-spectrogram & Plot Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d51a38",
   "metadata": {},
   "source": [
    "- 각각 HiFi-GAN과 Tacotron2에서 구현을 가져옴. Plot Alignment는 모델에 맞게 변형함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c8ae053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mel(mel, step=None, loss=None):\n",
    "    \"\"\"\n",
    "    =====inputs=====\n",
    "    mel: (80, F) # 입력 시 .detach().cpu().numpy() 를 적용하는 것을 잊지 말 것!\n",
    "    \"\"\"\n",
    "    plt.rcParams['axes.unicode_minus'] = False # 마이너스 부호 오류에 대한 해결\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    im = ax.imshow(mel, origin=\"lower\", aspect=\"auto\")\n",
    "    plt.title(f\"Step: {step} | Loss: {loss}\")\n",
    "    plt.xlabel(\"time\")\n",
    "    plt.ylabel(\"frequency_bin\")\n",
    "    fig.colorbar(im, ax=ax)\n",
    "    \n",
    "    fig.canvas.draw()\n",
    "    plt.close()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c485e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.use('Agg')\n",
    "\n",
    "font_name = fm.FontProperties(fname=\"C:/Users/Poco/Jupyter/PaperReview/dataset/malgun.ttf\").get_name()\n",
    "matplotlib.rc('font', family=font_name, size=14)\n",
    "\n",
    "def plot_alignment(alignment, text, step, loss):\n",
    "    text = text.rstrip('_').rstrip('~')\n",
    "    alignment = alignment[:len(text)]\n",
    "    \n",
    "    # 하나의 그림(fig) 객체와 하나의 축(ax) 객체를 생성\n",
    "    fig, ax = plt.subplots(figsize=(len(text)/3, 5))\n",
    "    # 생성한 축(ax) 객체에 이미지를 출력\n",
    "    im = ax.imshow(np.transpose(alignment), aspect='auto', origin='lower')\n",
    "    \n",
    "    plt.title(f\"step: {step}, loss: {loss:.5f}\", loc=\"center\", pad=10)\n",
    "    plt.xlabel('Encoder timestep')\n",
    "    plt.ylabel('Decoder timestep')\n",
    "    # 공백 문자 ' '를 빈 문자열 ''로 변환\n",
    "    text = [x if x != ' ' else '' for x in list(text)]\n",
    "    # x축의 눈금과 레이블을 설정\n",
    "    plt.xticks(range(len(text)), text)\n",
    "    \n",
    "    # 그래프의 레이아웃을 조정\n",
    "    plt.tight_layout()\n",
    "    fig.colorbar(im, ax=ax)\n",
    "    fig.canvas.draw()\n",
    "    plt.close()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5304a28",
   "metadata": {},
   "source": [
    "## 4.1. Glow-TTS Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3de069a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLE_Loss(z, z_mean, z_log_std, log_det, z_mask): # Maximum Likelihood Estimation\n",
    "    \"\"\"\n",
    "    =====input=====\n",
    "    (1st return tuple of the model)\n",
    "    =====output=====\n",
    "    Loss: (1) | Negative Loglikelihood\n",
    "    \"\"\"\n",
    "    Loss = torch.sum(z_log_std) + 0.5 * torch.sum(torch.exp(-2 * z_log_std) * ((z-z_mean)**2)) # (1)\n",
    "    Loss = Loss - torch.sum(log_det) # (2)\n",
    "    Loss = Loss / torch.sum(torch.ones_like(z) * z_mask) # (3) \n",
    "    Loss = Loss + 0.5 * math.log(2 * math.pi) # (4)\n",
    "    \"\"\"\n",
    "    Negative Loglikelihood\n",
    "    (1) + (3)*(4) + (2) : 논문에서 제시된 Loglikelihood에 음을 취한 값 (Negative Loglikelihood)\n",
    "    여기에서 전체에 (3)으로 나눠주었다고 생각하면 된다. (Averaging across B, C=80, F)\n",
    "    \"\"\"\n",
    "    return Loss\n",
    "\n",
    "def Duration_Loss(x_log_dur, log_d, text_len):\n",
    "    \"\"\"\n",
    "    =====inputs=====\n",
    "    x_log_dur: (B, 1, T) | 예측된 duration () (from 3rd returned tuple)\n",
    "    log_d: (B, 1, T) | Alignment로부터 추출한 duration (from 3rd returned tuple)\n",
    "    text_len: (B) | text의 길이 (from dataset)\n",
    "    =====outputs=====\n",
    "    Loss: (1) | Duration Loss\n",
    "    \"\"\"\n",
    "    Loss = torch.sum((x_log_dur - log_d)**2) / torch.sum(text_len)\n",
    "    # MSE | torch 구현을 사용하지 않은 이유는 text_len이 모두 다르기 때문이다.\n",
    "    return Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20b37a0",
   "metadata": {},
   "source": [
    "## 4.2. Glow-TTS Adam Optimizer + Noam LR Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47ad0e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam():\n",
    "    def __init__(self, params, dim_model=192, warmup_steps=4000, lr=1e0, betas=(0.9, 0.98)):\n",
    "        self.params = params\n",
    "        self.dim_model = dim_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "\n",
    "        self.step_num = 1\n",
    "        self.current_lr = lr * self._get_lr_scale()\n",
    "\n",
    "        self._optim = torch.optim.Adam(params, lr=self.current_lr, betas=betas)\n",
    "    \n",
    "    def _get_lr_scale(self):\n",
    "        return np.power(self.dim_model, -0.5) * np.min([np.power(self.step_num, -0.5),\n",
    "                                                        self.step_num * np.power(self.warmup_steps, -1.5)])\n",
    "\n",
    "    def _update_learning_rate(self):\n",
    "        self.step_num += 1\n",
    "        self.current_lr = self.lr * self._get_lr_scale() # NoamLR\n",
    "        for param_group in self._optim.param_groups:\n",
    "            param_group['lr'] = self.current_lr\n",
    "    \n",
    "    def get_lr(self):\n",
    "        return self.current_lr\n",
    "    \n",
    "    def step(self):\n",
    "        self._optim.step()\n",
    "        self._update_learning_rate()\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self._optim.zero_grad()\n",
    "    \n",
    "    def load_state_dict(self, d):\n",
    "        self._optim.load_state_dict(d)\n",
    "        \n",
    "    def state_dict(self):\n",
    "        return self._optim.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9445b575",
   "metadata": {},
   "source": [
    "## 4.3. Glow-TTS Main Training Function using HiFi-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4402221d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Glow_TTS_train(model_name, check_step, hifi_model_name=None, hifi_check_step=None):\n",
    "    \"\"\"\n",
    "    (NEW) check_step = None\n",
    "    (LOAD) check_step: 불러오고자 하는 모델의 step\n",
    "    \n",
    "    만약 Glow-TTS로부터 생성된 Mel-spectrogram을 pretrained된 HiFi-GAN으로 학습시키길 원한다면\n",
    "    HiFi-GAN 모델명과 check_step을 적으라.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Define Models & Optimizer\n",
    "    model = GlowTTS().to(device)\n",
    "    \n",
    "    optim = Adam(model.parameters())\n",
    "\n",
    "    # Load Models & Optimizer\n",
    "    epoch = 1\n",
    "    step = 1\n",
    "    if check_step is not None:\n",
    "        os.makedirs('ckpt/' + model_name, exist_ok=True)\n",
    "        check_point = './ckpt/' + model_name + \"/ckpt-\" + str(check_step) + \".pt\"\n",
    "        ckpt = torch.load(check_point)\n",
    "        model.load_state_dict(ckpt['model'])\n",
    "        optim.load_state_dict(ckpt['optim'])\n",
    "        epoch = ckpt['epoch']\n",
    "        step = ckpt['step']\n",
    "        print(f'Load Glow-TTS model: {model_name} | Step {check_step}')\n",
    "        \n",
    "    if hifi_check_step is not None:\n",
    "        gen_model = Generator().to(device) # HiFi-GAN\n",
    "        check_point = './hifi_ckpt/' + hifi_model_name + \"/ckpt-\" + str(hifi_check_step) + \".pt\"\n",
    "        ckpt = torch.load(check_point)\n",
    "        gen_model.load_state_dict(ckpt['gen_model'])\n",
    "        print(f'Load HiFi-GAN model: {hifi_model_name} | Step {hifi_check_step}')\n",
    "    \n",
    "    # cuDNN의 벤치마크 모드를 활성화. 합성곱과 풀링 연산 등을 최적화.\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    # Training Mode Activation\n",
    "    model.train()\n",
    "    \n",
    "    # tensorboard\n",
    "    os.makedirs('ckpt/' + model_name + '/logs', exist_ok=True)\n",
    "    sw = SummaryWriter('./ckpt/' + model_name + '/logs')\n",
    "    \n",
    "    # Main Training\n",
    "    start = time.time() # start time 초기화\n",
    "    while True:\n",
    "        print(f\"|| Epoch: {epoch} ||\")\n",
    "        for _, batch in enumerate(train_dataloader):\n",
    "            text = batch[0].to(device)\n",
    "            text_len = torch.tensor(batch[1], dtype=torch.int32)\n",
    "            mel = batch[2].to(device).transpose(1, 2)\n",
    "            mel_len = torch.tensor(batch[3], dtype=torch.int32)\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            (z, z_mean, z_log_std, log_det, z_mask), (x_mean, x_log_std, x_mask), (attention_alignment, x_log_dur, log_d) = \\\n",
    "                model(text, text_len, mel, mel_len)\n",
    "            \n",
    "            mle_loss = MLE_Loss(z, z_mean, z_log_std, log_det, z_mask)\n",
    "            duration_loss = Duration_Loss(x_log_dur, log_d, text_len)\n",
    "            loss = mle_loss + duration_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            grad_norm = nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            optim.step()\n",
    "            step += 1\n",
    "            \n",
    "            # Logging\n",
    "            if step % logging_step == 0:\n",
    "                print(f'| step: {step} | loss: {loss:2.3f} | mle_loss: {mle_loss:2.3f} | dur_loss: {duration_loss:2.3f} | {time.time()-start:.3f} sec / {logging_step} steps |')\n",
    "                # Tensorboard에 기록\n",
    "                sw.add_scalar(\"training/loss\", loss, step)\n",
    "                sw.add_scalar(\"training/mle_loss\", mle_loss, step)\n",
    "                sw.add_scalar(\"training/duration_loss\", duration_loss, step)\n",
    "                sw.add_scalar(\"training/grad_norm\", grad_norm, step)\n",
    "                sw.add_scalar(\"training/learning_rate\", optim.get_lr(), step)\n",
    "                start = time.time() # start time 초기화\n",
    "            \n",
    "            # Validation\n",
    "            if step % validation_step == 0:\n",
    "                model.eval()\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "                loss_sum = 0\n",
    "                mle_loss_sum = 0\n",
    "                duration_loss_sum = 0\n",
    "                with torch.no_grad():\n",
    "                    for i, batch in enumerate(val_dataloader):\n",
    "                        text = batch[0].to(device)\n",
    "                        text_len = torch.tensor(batch[1], dtype=torch.int32)\n",
    "                        mel = batch[2].to(device).transpose(1, 2)\n",
    "                        mel_len = torch.tensor(batch[3], dtype=torch.int32)\n",
    "\n",
    "                        optim.zero_grad()\n",
    "                        (z, z_mean, z_log_std, log_det, z_mask), (x_mean, x_log_std, x_mask), (attention_alignment, x_log_dur, log_d) = \\\n",
    "                            model(text, text_len, mel, mel_len)\n",
    "\n",
    "                        mle_loss = MLE_Loss(z, z_mean, z_log_std, log_det, z_mask)\n",
    "                        duration_loss = Duration_Loss(x_log_dur, log_d, text_len)\n",
    "                        loss = mle_loss + duration_loss\n",
    "                        \n",
    "                        loss_sum += loss\n",
    "                        mle_loss_sum += mle_loss\n",
    "                        duration_loss_sum += duration_loss\n",
    "                        \n",
    "                        # inference\n",
    "                        (y, z_mean, z_log_std, log_det, z_mask), (x_mean, x_log_std, x_mask), (attention_alignment, x_log_dur, log_d) = \\\n",
    "                            model(text, text_len, inference=True)\n",
    "                        \n",
    "                        if (i >= 5 and i <= 9): # 그림은 5개 저장\n",
    "                            sw.add_figure(f'validation/gen_mel_{i})',\n",
    "                                          plot_mel(y[0].detach().cpu().numpy(), step, loss),\n",
    "                                          step)\n",
    "                            sw.add_figure(f'validation/org_mel_{i})',\n",
    "                                          plot_mel(mel[0].detach().cpu().numpy(), step, loss),\n",
    "                                          step)\n",
    "                            input_seq = sequence_to_text(text[0].cpu().numpy())\n",
    "                            input_seq = input_seq[:text_len[0]]\n",
    "                            sw.add_figure(f'validation/gen_align_{i})',\n",
    "                                          plot_alignment(attention_alignment[0].detach().cpu().numpy(), input_seq, step, loss),\n",
    "                                          step)\n",
    "                            if hifi_check_step is not None:\n",
    "                                gen_wav = gen_model(y[0]) # (1, L)\n",
    "                                sw.add_audio(f'generated/gen_wav_{i}', gen_wav[0], step, sample_rate)\n",
    "                                \n",
    "                        \n",
    "                    loss_avg = loss_sum / (i+1)\n",
    "                    mle_loss_avg = mle_loss_sum / (i+1)\n",
    "                    duration_loss_avg = duration_loss_sum / (i+1)\n",
    "                    \n",
    "                    sw.add_scalar(\"validation/loss\", loss_avg, step)\n",
    "                    sw.add_scalar(\"validation/mle_loss\", mle_loss_avg, step)\n",
    "                    sw.add_scalar(\"validation/duration_loss\", duration_loss_avg, step)\n",
    "                    print(f\"[Validation] loss: {loss_avg:2.3f} | mle_loss: {mle_loss_avg:2.3f} | dur_loss: {duration_loss_avg:2.3f} | {time.time()-start:.3f} sec\")\n",
    "                \n",
    "                model.train()\n",
    "                start = time.time() # start time 초기화\n",
    "                        \n",
    "            # Checkpoint\n",
    "            if step % checkpoint_step == 0:\n",
    "                save_dir = './ckpt/' + model_name\n",
    "                torch.save({\n",
    "                    'model': model.state_dict(),\n",
    "                    'optim': optim.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'step': step\n",
    "                }, os.path.join(save_dir, 'ckpt-{}.pt'.format(step)))\n",
    "                \n",
    "                start = time.time() # start time 초기화\n",
    "                \n",
    "        epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a9c4e9",
   "metadata": {},
   "source": [
    "## 4.4. HiFi-GAN Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3cf781d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GAN_Loss_Generator(gen_outputs):\n",
    "    \"\"\"\n",
    "    gen_outputs: (B, ?) list # MPD(len=5) 또는 MSD(len=3)의 출력\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    for DG in gen_outputs:\n",
    "        loss += torch.mean((DG-1)**2)\n",
    "    return loss\n",
    "\n",
    "def GAN_Loss_Discriminator(real_outputs, gen_outputs):\n",
    "    \"\"\"\n",
    "    real_outputs: (B, ?) list # MPD(len=5) 또는 MSD(len=3)의 출력\n",
    "    gen_outputs: (B, ?) list # MPD(len=5) 또는 MSD(len=3)의 출력\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    for D, DG in zip(real_outputs, gen_outputs):\n",
    "        loss += torch.mean((D-1)**2 + DG**2)\n",
    "    return loss\n",
    "\n",
    "def Mel_Spectrogram_Loss(real_mel, gen_mel):\n",
    "    \"\"\"\n",
    "    real_mel: (B, F, 80) # Dataloader로부터 가져온 mel-spectrogram\n",
    "    gen_mel: (B, F, 80) # Generator가 생성한 waveform의 mel-spectrogram\n",
    "    \"\"\"\n",
    "    loss = F.l1_loss(real_mel, gen_mel)\n",
    "    return 45*loss\n",
    "\n",
    "def Feature_Matching_Loss(real_features, gen_features):\n",
    "    \"\"\"\n",
    "    real_features: (?, ..., ?) list of list # MPD(len=[5, 6]) 또는 MSD(len=[3, 7])의 출력\n",
    "    gen_features: (?, ..., ?) list of list # MPD(len=[5, 6]) 또는 MSD(len=[3, 7])의 출력\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    for Ds, DGs in zip(real_features, gen_features):\n",
    "        for D, DG in zip(Ds, DGs):\n",
    "            loss += torch.mean(torch.abs(D - DG))\n",
    "    return 2*loss\n",
    "\n",
    "def Final_Loss_Generator(mpd_gen_outputs, mpd_real_features, mpd_gen_features,\n",
    "                         msd_gen_outputs, msd_real_features, msd_gen_features,\n",
    "                         real_mel, gen_mel):\n",
    "    \"\"\"\n",
    "    =====inputs=====\n",
    "    [:3]: MPD outputs 뒤쪽 3개\n",
    "    [3:6]: MSD outputs 뒤쪽 3개\n",
    "    [7:8]: real_mel and gen_mel\n",
    "    =====outputs=====\n",
    "    Generator_Loss\n",
    "    Mel_Loss\n",
    "    \"\"\"\n",
    "    Gen_Adv1 = GAN_Loss_Generator(mpd_gen_outputs)\n",
    "    Gen_Adv2 = GAN_Loss_Generator(msd_gen_outputs)\n",
    "    Adv = Gen_Adv1 + Gen_Adv2\n",
    "    FM1 = Feature_Matching_Loss(mpd_real_features, mpd_gen_features)\n",
    "    FM2 = Feature_Matching_Loss(msd_real_features, msd_gen_features)\n",
    "    FM = FM1 + FM2\n",
    "    Mel_Loss = Mel_Spectrogram_Loss(real_mel, gen_mel)\n",
    "    Generator_Loss = Adv + FM + Mel_Loss\n",
    "    \n",
    "    return Generator_Loss, Mel_Loss\n",
    "\n",
    "def Final_Loss_Discriminator(mpd_real_outputs, mpd_gen_outputs,\n",
    "                             msd_real_outputs, msd_gen_outputs):\n",
    "    \"\"\"\n",
    "    =====inputs=====\n",
    "    [:2]: MPD outputs 앞쪽 2개\n",
    "    [2:4]: MSD outputs 앞쪽 2개\n",
    "    =====outputs=====\n",
    "    Discriminator_Loss\n",
    "    \"\"\"\n",
    "    Disc_Adv1 = GAN_Loss_Discriminator(mpd_real_outputs, mpd_gen_outputs)\n",
    "    Disc_Adv2 = GAN_Loss_Discriminator(msd_real_outputs, msd_gen_outputs)\n",
    "    Discriminator_Loss = Disc_Adv1 + Disc_Adv2\n",
    "    \n",
    "    return Discriminator_Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2ec912",
   "metadata": {},
   "source": [
    "## 4.5. HiFi-GAN Mel-spectrogram Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff6d35c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HiFi-GAN 저자 구현\n",
    "def mel_spectrogram(y, n_fft=1024, num_mels=80, sampling_rate=22050, hop_size=256, win_size=1024, fmin=0, fmax=8000, center=False):\n",
    "    \"\"\"\n",
    "    if torch.min(y) < -1.:\n",
    "        print('min value is ', torch.min(y))\n",
    "    if torch.max(y) > 1.:\n",
    "        print('max value is ', torch.max(y))\n",
    "    \"\"\"\n",
    "\n",
    "    mel = librosa.filters.mel(sampling_rate, n_fft, num_mels, fmin, fmax)\n",
    "    \n",
    "    y = torch.nn.functional.pad(y.unsqueeze(1), (int((n_fft-hop_size)/2), int((n_fft-hop_size)/2)), mode='reflect')\n",
    "    y = y.squeeze(1)\n",
    "\n",
    "    spec = torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=torch.hann_window(win_size).to(y.device),\n",
    "                      center=center, pad_mode='reflect', normalized=False, onesided=True)\n",
    "\n",
    "    spec = torch.sqrt(spec.pow(2).sum(-1)+(1e-9))\n",
    "\n",
    "    spec = torch.matmul(torch.from_numpy(mel).float().to(y.device), spec)\n",
    "    spec = torch.log(torch.clamp(spec, min=1e-5) * 1)\n",
    "\n",
    "    return spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcacf91",
   "metadata": {},
   "source": [
    "## 4.6. HiFi-GAN Main Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8fd3ddd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HiFi_GAN_train(model_name, check_step):\n",
    "    \"\"\"\n",
    "    (NEW) check_step = None\n",
    "    (LOAD) check_step: 불러오고자 하는 모델의 step\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Define Models & Optimizer\n",
    "    gen_model = Generator().to(device)\n",
    "    mpd_model = MPD().to(device)\n",
    "    msd_model = MSD().to(device)\n",
    "    \n",
    "    gen_optim = torch.optim.AdamW(gen_model.parameters(),\n",
    "                                  lr=0.0002, betas=(0.8, 0.99))\n",
    "    disc_optim = torch.optim.AdamW(itertools.chain(mpd_model.parameters(),\n",
    "                                                   msd_model.parameters()),\n",
    "                                   lr=0.0002, betas=(0.8, 0.99))\n",
    "    # 오류에 따른 수정: initial_lr 지정\n",
    "    for param_group in gen_optim.param_groups:\n",
    "        param_group['initial_lr'] = 0.0002\n",
    "    \n",
    "    for param_group in disc_optim.param_groups:\n",
    "        param_group['initial_lr'] = 0.0002\n",
    "    \n",
    "    # Load Models & Optimizer\n",
    "    epoch = 1\n",
    "    step = 1\n",
    "    if check_step is not None:\n",
    "        os.makedirs('hifi_ckpt/' + model_name, exist_ok=True)\n",
    "        check_point = './hifi_ckpt/' + model_name + \"/ckpt-\" + str(check_step) + \".pt\"\n",
    "        ckpt = torch.load(check_point)\n",
    "        gen_model.load_state_dict(ckpt['gen_model'])\n",
    "        mpd_model.load_state_dict(ckpt['mpd_model'])\n",
    "        msd_model.load_state_dict(ckpt['msd_model'])\n",
    "        gen_optim.load_state_dict(ckpt['gen_optim'])\n",
    "        disc_optim.load_state_dict(ckpt['disc_optim'])\n",
    "        epoch = ckpt['epoch']\n",
    "        step = ckpt['step']\n",
    "        print(f'Load {model_name} | Step {check_step}')\n",
    "    \n",
    "    # cuDNN의 벤치마크 모드를 활성화. 합성곱과 풀링 연산 등을 최적화.\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    # Define Scheduler\n",
    "    gen_scheduler = torch.optim.lr_scheduler.ExponentialLR(gen_optim, gamma=0.999,\n",
    "                                                           last_epoch=epoch)\n",
    "    disc_scheduler = torch.optim.lr_scheduler.ExponentialLR(disc_optim, gamma=0.999,\n",
    "                                                            last_epoch=epoch)\n",
    "    \n",
    "    # Training Mode Activation\n",
    "    gen_model.train()\n",
    "    mpd_model.train()\n",
    "    msd_model.train()\n",
    "    \n",
    "    # tensorboard\n",
    "    os.makedirs('hifi_ckpt/' + model_name + '/logs', exist_ok=True)\n",
    "    sw = SummaryWriter('./hifi_ckpt/' + model_name + '/logs')\n",
    "    \n",
    "    # Main Training\n",
    "    \"\"\"\n",
    "    mel과 mel_loss를 구분하여 논문 저자 구현을 따라감.\n",
    "    \"\"\"\n",
    "    start = time.time() # start time 초기화\n",
    "    while True:\n",
    "        print(f\"|| Epoch: {epoch} ||\")\n",
    "        for _, batch in enumerate(train_dataloader):\n",
    "            _, _, mel, _, wav, _ = batch\n",
    "            real_mel = mel.to(device) # (B, Max_F, 80)\n",
    "            real_wav = wav.to(device) # (B, Max_L)\n",
    "            real_mel = real_mel.transpose(1, 2) # (B, 80, Max_F) # fmax=8000\n",
    "            real_wav = real_wav.unsqueeze(1) # (B, 1, Max_L)\n",
    "            _, _, Max_L = real_wav.size()\n",
    "            \n",
    "            real_mel_for_loss = mel_spectrogram(real_wav.squeeze(1), fmax=11025)\n",
    "            \n",
    "            # Generator\n",
    "            gen_wav = gen_model(real_mel)\n",
    "            _, _, Gen_L = gen_wav.size()\n",
    "            if Max_L < Gen_L:\n",
    "                gen_wav = gen_wav[:, :, :Max_L]\n",
    "            else:\n",
    "                real_wav = real_wav[:, :, :Gen_L]\n",
    "            \n",
    "            gen_mel = mel_spectrogram(gen_wav.squeeze(1), fmax=11025)\n",
    "            \"\"\"\n",
    "            Nyquist Frequency에 의하면 sampling rate의 절반에 해당하는 주파수까지 분석이 가능하다.\n",
    "            현재 sampling rate이 22050이므로, fmax를 그의 절반인 11025로 둔다.\n",
    "            \"\"\"\n",
    "            \n",
    "            # Discriminator Loss\n",
    "            \n",
    "            # MPD\n",
    "            mpd_real_outputs, mpd_gen_outputs, mpd_real_features, mpd_gen_features = mpd_model(real_wav, gen_wav.detach())\n",
    "            # MSD\n",
    "            msd_real_outputs, msd_gen_outputs, msd_real_features, msd_gen_features = msd_model(real_wav, gen_wav.detach())\n",
    "                # Discriminator에서는 gen_wav에 대한 gradient를 계산하지 않음을 명심하라.\n",
    "\n",
    "            disc_optim.zero_grad()\n",
    "            disc_loss = Final_Loss_Discriminator(mpd_real_outputs, mpd_gen_outputs,\n",
    "                                                 msd_real_outputs, msd_gen_outputs)\n",
    "            disc_loss.backward()\n",
    "            disc_optim.step()\n",
    "            \n",
    "            # Generator Loss\n",
    "            \n",
    "            # MPD\n",
    "            mpd_real_outputs, mpd_gen_outputs, mpd_real_features, mpd_gen_features = mpd_model(real_wav, gen_wav)\n",
    "            # MSD\n",
    "            msd_real_outputs, msd_gen_outputs, msd_real_features, msd_gen_features = msd_model(real_wav, gen_wav)\n",
    "            \n",
    "            gen_optim.zero_grad() # 이전 step의 gradient 초기화\n",
    "            gen_loss, mel_loss = Final_Loss_Generator(mpd_gen_outputs, mpd_real_features, mpd_gen_features,\n",
    "                                            msd_gen_outputs, msd_real_features, msd_gen_features,\n",
    "                                            real_mel_for_loss, gen_mel)\n",
    "            gen_loss.backward() # gradient 계산\n",
    "            gen_optim.step() # 가중치 업데이트\n",
    "            \n",
    "            step += 1\n",
    "            \n",
    "            # Logging\n",
    "            if step % logging_step == 0:\n",
    "                print(f'| step: {step} | gen_loss: {gen_loss:4.3f} | mel_loss: {mel_loss:4.3f} | {time.time()-start:.3f} sec / {logging_step} steps |')\n",
    "                # Tensorboard에 기록\n",
    "                sw.add_scalar(\"training/gen_loss\", gen_loss, step)\n",
    "                sw.add_scalar(\"training/mel_loss\", mel_loss, step)\n",
    "                start = time.time() # start time 초기화\n",
    "            \n",
    "            # Validation\n",
    "            if step % validation_step == 0:\n",
    "                gen_model.eval()\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "                mel_loss_sum = 0\n",
    "                with torch.no_grad():\n",
    "                    for i, batch in enumerate(val_dataloader):\n",
    "                        _, _, mel, _, wav, _ = batch\n",
    "                        real_mel = mel.to(device) # (1, F, 80)\n",
    "                        real_wav = wav.to(device) # (1, L)\n",
    "                        real_mel = real_mel.transpose(1, 2) # (1, 80, F)\n",
    "                        real_wav = real_wav.unsqueeze(1) # (1, 1, L)\n",
    "                        _, _, Max_L = real_wav.size()\n",
    "                        \n",
    "                        real_mel_for_loss = mel_spectrogram(real_wav.squeeze(1), fmax=11025)\n",
    "                        \n",
    "                        # Generator\n",
    "                        gen_wav = gen_model(real_mel) # (1, L*)\n",
    "                        _, _, Gen_L = gen_wav.size()\n",
    "                        if Max_L < Gen_L:\n",
    "                            gen_wav = gen_wav[:, :, :Max_L]\n",
    "                        else:\n",
    "                            real_wav = real_wav[:, :, :Gen_L]\n",
    "                        \n",
    "                        gen_mel = mel_spectrogram(gen_wav.squeeze(1), fmax=11025) # (1, 80, F)\n",
    "                        \n",
    "                        # Loss\n",
    "                        mel_loss = Mel_Spectrogram_Loss(real_mel_for_loss, gen_mel)\n",
    "                        mel_loss_sum += mel_loss\n",
    "                        \n",
    "                        if i <= 4: # 오디오와 그림은 5개 저장\n",
    "                            sw.add_audio(f'generated/gen_wav_{i}', gen_wav[0], step, sample_rate)\n",
    "                            sw.add_figure(f'generated/gen_mel_{i})',\n",
    "                                          plot_mel(gen_mel[0].detach().cpu().numpy(), step, mel_loss),\n",
    "                                          step)\n",
    "                        \n",
    "                    mel_loss_avg = mel_loss_sum / (i+1)\n",
    "                    sw.add_scalar(\"validation/mel_loss\", mel_loss_avg, step)\n",
    "                    print(f\"Validation mel_loss: {mel_loss_avg}\")\n",
    "                \n",
    "                gen_model.train()\n",
    "                start = time.time() # start time 초기화\n",
    "                        \n",
    "            # Checkpoint\n",
    "            if step % checkpoint_step == 0:\n",
    "                save_dir = './hifi_ckpt/' + model_name\n",
    "                torch.save({\n",
    "                    'gen_model': gen_model.state_dict(),\n",
    "                    'mpd_model': mpd_model.state_dict(),\n",
    "                    'msd_model': msd_model.state_dict(),\n",
    "                    'gen_optim': gen_optim.state_dict(),\n",
    "                    'disc_optim': disc_optim.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'step': step\n",
    "                }, os.path.join(save_dir, 'ckpt-{}.pt'.format(step)))\n",
    "                \n",
    "                start = time.time() # start time 초기화\n",
    "                \n",
    "        # Epoch 증가에 따라 scheduler 조정\n",
    "        gen_scheduler.step()\n",
    "        disc_scheduler.step()\n",
    "        epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a7f57f",
   "metadata": {},
   "source": [
    "## 4.7. Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a5f764d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Inference(text, model_name, check_step, hifi_model_name, hifi_check_step):\n",
    "    \"\"\"\n",
    "    (NEW) check_step = None\n",
    "    (LOAD) check_step: 불러오고자 하는 모델의 step\n",
    "    \n",
    "    학습된 Glow-TTS와 HiFi-GAN으로부터 목소리를 만들어냅니다.\n",
    "    text에는 text의 리스트가 입력됩니다.\n",
    "    (예) [\"안녕?\", \"반가워.\"]\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Define Models & Optimizer\n",
    "    model = GlowTTS().to(device)\n",
    "    gen_model = Generator().to(device) # HiFi-GAN\n",
    "\n",
    "    # Load Models & Optimizer\n",
    "    if check_step is not None:\n",
    "        os.makedirs('ckpt/' + model_name, exist_ok=True)\n",
    "        check_point = './ckpt/' + model_name + \"/ckpt-\" + str(check_step) + \".pt\"\n",
    "        ckpt = torch.load(check_point)\n",
    "        model.load_state_dict(ckpt['model'])\n",
    "        print(f'Load Glow-TTS model: {model_name} | Step {check_step}')\n",
    "        \n",
    "    if hifi_check_step is not None:\n",
    "        check_point = './hifi_ckpt/' + hifi_model_name + \"/ckpt-\" + str(hifi_check_step) + \".pt\"\n",
    "        ckpt = torch.load(check_point)\n",
    "        gen_model.load_state_dict(ckpt['gen_model'])\n",
    "        print(f'Load HiFi-GAN model: {hifi_model_name} | Step {hifi_check_step}')\n",
    "        \n",
    "    # Text Preprocessing\n",
    "    filters = '([,])'\n",
    "    text_list = []\n",
    "    text_len = []\n",
    "    for idx, s in enumerate(text):\n",
    "        # 문자열에서 정규표현식을 이용하여 특정 문자열을 필터링하고,\n",
    "        # 이를 빈 문자열('')로 대체한다.\n",
    "        sentence = re.sub(re.compile(filters), '', s)\n",
    "        sentence = text_to_sequence(sentence)\n",
    "        text_tensor = torch.tensor(sentence)\n",
    "        text_list.append(text_tensor)\n",
    "        text_len.append(len(text_tensor))\n",
    "\n",
    "    max_text_len = max(text_len)\n",
    "\n",
    "    # text zero_padding\n",
    "    text_batch = torch.zeros((len(text_list), max_text_len), dtype=torch.int32)\n",
    "    for i, x in enumerate(text_list):\n",
    "        text_batch[i, :len(x)] = torch.Tensor(x)\n",
    "    text = text_batch.to(device)\n",
    "\n",
    "    os.makedirs('./save', exist_ok=True)\n",
    "    \n",
    "    for idx in range(text.size(0)):\n",
    "        (y, z_mean, z_log_std, log_det, z_mask), (x_mean, x_log_std, x_mask), (attention_alignment, x_log_dur, log_d) = \\\n",
    "                            model(text[idx].unsqueeze(0), [text_len[idx]], inference=True)\n",
    "        gen_wav = gen_model(y)\n",
    "        gen_wav = gen_wav.cpu().detach().numpy()\n",
    "        sf.write('./save/'+'{}.wav'.format(idx), gen_wav[0][0], sample_rate)\n",
    "        print(f'Save: {idx}.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6693163",
   "metadata": {},
   "source": [
    "# 5. Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023ce1c3",
   "metadata": {},
   "source": [
    "## 5.1. Train HiFi-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c47e3424",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### INPUT #####\n",
    "model_name = \"model_01\"\n",
    "check_step = 401000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb270a02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model_01 | Step 401000\n",
      "|| Epoch: 534 ||\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Poco\\anaconda3\\lib\\site-packages\\torch\\functional.py:632: UserWarning: stft will soon require the return_complex parameter be given for real inputs, and will further require that return_complex=True in a future PyTorch release. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\SpectralOps.cpp:804.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 401010 | gen_loss: 50.194 | mel_loss: 11.990 | 16.208 sec / 10 steps |\n",
      "| step: 401020 | gen_loss: 46.964 | mel_loss: 10.871 | 12.613 sec / 10 steps |\n",
      "| step: 401030 | gen_loss: 49.910 | mel_loss: 12.142 | 11.575 sec / 10 steps |\n",
      "| step: 401040 | gen_loss: 49.482 | mel_loss: 11.263 | 11.198 sec / 10 steps |\n",
      "| step: 401050 | gen_loss: 46.203 | mel_loss: 10.524 | 10.806 sec / 10 steps |\n",
      "| step: 401060 | gen_loss: 49.447 | mel_loss: 11.455 | 9.103 sec / 10 steps |\n",
      "| step: 401070 | gen_loss: 44.032 | mel_loss: 10.602 | 10.471 sec / 10 steps |\n",
      "| step: 401080 | gen_loss: 47.572 | mel_loss: 11.376 | 10.780 sec / 10 steps |\n",
      "| step: 401090 | gen_loss: 45.851 | mel_loss: 10.001 | 9.995 sec / 10 steps |\n",
      "| step: 401100 | gen_loss: 40.981 | mel_loss: 9.198 | 8.672 sec / 10 steps |\n",
      "Validation mel_loss: 14.066046714782715\n",
      "| step: 401110 | gen_loss: 45.495 | mel_loss: 10.709 | 7.939 sec / 10 steps |\n",
      "| step: 401120 | gen_loss: 47.552 | mel_loss: 11.482 | 8.394 sec / 10 steps |\n",
      "| step: 401130 | gen_loss: 48.146 | mel_loss: 11.407 | 8.020 sec / 10 steps |\n",
      "| step: 401140 | gen_loss: 43.671 | mel_loss: 10.299 | 9.906 sec / 10 steps |\n",
      "| step: 401150 | gen_loss: 46.492 | mel_loss: 10.305 | 8.438 sec / 10 steps |\n",
      "| step: 401160 | gen_loss: 48.311 | mel_loss: 12.008 | 5.529 sec / 10 steps |\n",
      "| step: 401170 | gen_loss: 43.455 | mel_loss: 10.548 | 7.082 sec / 10 steps |\n",
      "| step: 401180 | gen_loss: 46.923 | mel_loss: 10.690 | 6.118 sec / 10 steps |\n",
      "| step: 401190 | gen_loss: 43.281 | mel_loss: 9.535 | 5.916 sec / 10 steps |\n",
      "| step: 401200 | gen_loss: 43.391 | mel_loss: 9.775 | 8.656 sec / 10 steps |\n",
      "Validation mel_loss: 14.113856315612793\n",
      "| step: 401210 | gen_loss: 48.108 | mel_loss: 11.623 | 7.885 sec / 10 steps |\n",
      "| step: 401220 | gen_loss: 43.303 | mel_loss: 10.192 | 5.941 sec / 10 steps |\n",
      "| step: 401230 | gen_loss: 46.242 | mel_loss: 11.385 | 8.059 sec / 10 steps |\n",
      "| step: 401240 | gen_loss: 49.507 | mel_loss: 12.198 | 7.250 sec / 10 steps |\n",
      "| step: 401250 | gen_loss: 49.897 | mel_loss: 11.445 | 4.251 sec / 10 steps |\n",
      "| step: 401260 | gen_loss: 42.010 | mel_loss: 9.175 | 6.057 sec / 10 steps |\n",
      "| step: 401270 | gen_loss: 48.425 | mel_loss: 11.808 | 6.335 sec / 10 steps |\n",
      "| step: 401280 | gen_loss: 42.612 | mel_loss: 9.344 | 6.939 sec / 10 steps |\n",
      "| step: 401290 | gen_loss: 38.054 | mel_loss: 8.459 | 6.115 sec / 10 steps |\n",
      "| step: 401300 | gen_loss: 40.896 | mel_loss: 8.972 | 6.058 sec / 10 steps |\n",
      "Validation mel_loss: 13.982892990112305\n",
      "| step: 401310 | gen_loss: 41.331 | mel_loss: 8.756 | 6.741 sec / 10 steps |\n",
      "| step: 401320 | gen_loss: 40.128 | mel_loss: 10.307 | 5.721 sec / 10 steps |\n",
      "| step: 401330 | gen_loss: 45.918 | mel_loss: 9.953 | 6.765 sec / 10 steps |\n",
      "| step: 401340 | gen_loss: 46.018 | mel_loss: 10.463 | 4.921 sec / 10 steps |\n",
      "| step: 401350 | gen_loss: 47.567 | mel_loss: 11.463 | 7.239 sec / 10 steps |\n",
      "| step: 401360 | gen_loss: 46.652 | mel_loss: 10.811 | 6.323 sec / 10 steps |\n",
      "| step: 401370 | gen_loss: 50.692 | mel_loss: 12.386 | 5.524 sec / 10 steps |\n",
      "| step: 401380 | gen_loss: 41.479 | mel_loss: 9.310 | 5.999 sec / 10 steps |\n",
      "| step: 401390 | gen_loss: 43.590 | mel_loss: 9.832 | 5.884 sec / 10 steps |\n",
      "| step: 401400 | gen_loss: 44.242 | mel_loss: 10.322 | 4.222 sec / 10 steps |\n",
      "Validation mel_loss: 13.85518741607666\n",
      "| step: 401410 | gen_loss: 49.076 | mel_loss: 11.629 | 4.587 sec / 10 steps |\n",
      "| step: 401420 | gen_loss: 45.789 | mel_loss: 10.507 | 5.137 sec / 10 steps |\n",
      "| step: 401430 | gen_loss: 45.925 | mel_loss: 11.089 | 4.616 sec / 10 steps |\n",
      "| step: 401440 | gen_loss: 45.600 | mel_loss: 11.055 | 5.574 sec / 10 steps |\n",
      "| step: 401450 | gen_loss: 43.163 | mel_loss: 9.280 | 5.469 sec / 10 steps |\n",
      "| step: 401460 | gen_loss: 45.719 | mel_loss: 10.152 | 3.072 sec / 10 steps |\n",
      "| step: 401470 | gen_loss: 46.112 | mel_loss: 11.563 | 4.591 sec / 10 steps |\n",
      "| step: 401480 | gen_loss: 47.805 | mel_loss: 11.711 | 3.461 sec / 10 steps |\n",
      "| step: 401490 | gen_loss: 47.335 | mel_loss: 11.186 | 5.276 sec / 10 steps |\n",
      "| step: 401500 | gen_loss: 51.070 | mel_loss: 12.786 | 4.488 sec / 10 steps |\n",
      "Validation mel_loss: 14.01104736328125\n",
      "| step: 401510 | gen_loss: 40.478 | mel_loss: 8.929 | 4.024 sec / 10 steps |\n",
      "| step: 401520 | gen_loss: 44.808 | mel_loss: 10.235 | 4.281 sec / 10 steps |\n",
      "| step: 401530 | gen_loss: 47.638 | mel_loss: 12.588 | 5.484 sec / 10 steps |\n",
      "| step: 401540 | gen_loss: 50.341 | mel_loss: 12.226 | 4.916 sec / 10 steps |\n",
      "| step: 401550 | gen_loss: 45.286 | mel_loss: 11.045 | 4.333 sec / 10 steps |\n",
      "| step: 401560 | gen_loss: 44.316 | mel_loss: 10.867 | 4.770 sec / 10 steps |\n",
      "| step: 401570 | gen_loss: 46.064 | mel_loss: 11.292 | 3.208 sec / 10 steps |\n",
      "| step: 401580 | gen_loss: 48.036 | mel_loss: 11.481 | 6.146 sec / 10 steps |\n",
      "| step: 401590 | gen_loss: 51.800 | mel_loss: 12.769 | 5.012 sec / 10 steps |\n",
      "| step: 401600 | gen_loss: 47.592 | mel_loss: 11.796 | 4.305 sec / 10 steps |\n",
      "Validation mel_loss: 14.084053993225098\n",
      "| step: 401610 | gen_loss: 42.386 | mel_loss: 9.409 | 3.772 sec / 10 steps |\n",
      "| step: 401620 | gen_loss: 50.209 | mel_loss: 12.656 | 3.879 sec / 10 steps |\n",
      "| step: 401630 | gen_loss: 45.661 | mel_loss: 10.591 | 4.184 sec / 10 steps |\n",
      "| step: 401640 | gen_loss: 44.822 | mel_loss: 10.397 | 5.316 sec / 10 steps |\n",
      "| step: 401650 | gen_loss: 45.434 | mel_loss: 10.408 | 4.647 sec / 10 steps |\n",
      "| step: 401660 | gen_loss: 52.394 | mel_loss: 13.128 | 3.921 sec / 10 steps |\n",
      "| step: 401670 | gen_loss: 47.587 | mel_loss: 10.801 | 4.146 sec / 10 steps |\n",
      "| step: 401680 | gen_loss: 51.704 | mel_loss: 12.141 | 4.257 sec / 10 steps |\n",
      "| step: 401690 | gen_loss: 43.346 | mel_loss: 10.088 | 3.657 sec / 10 steps |\n",
      "| step: 401700 | gen_loss: 42.365 | mel_loss: 9.840 | 4.022 sec / 10 steps |\n",
      "Validation mel_loss: 13.955120086669922\n",
      "| step: 401710 | gen_loss: 46.876 | mel_loss: 11.475 | 5.341 sec / 10 steps |\n",
      "| step: 401720 | gen_loss: 39.817 | mel_loss: 10.019 | 4.572 sec / 10 steps |\n",
      "| step: 401730 | gen_loss: 48.169 | mel_loss: 12.482 | 6.786 sec / 10 steps |\n",
      "| step: 401740 | gen_loss: 44.605 | mel_loss: 11.015 | 4.235 sec / 10 steps |\n",
      "| step: 401750 | gen_loss: 44.368 | mel_loss: 10.565 | 3.833 sec / 10 steps |\n",
      "|| Epoch: 535 ||\n",
      "| step: 401760 | gen_loss: 52.486 | mel_loss: 12.811 | 2.975 sec / 10 steps |\n",
      "| step: 401770 | gen_loss: 44.415 | mel_loss: 10.673 | 4.098 sec / 10 steps |\n",
      "| step: 401780 | gen_loss: 44.106 | mel_loss: 10.832 | 3.962 sec / 10 steps |\n",
      "| step: 401790 | gen_loss: 47.989 | mel_loss: 11.505 | 3.489 sec / 10 steps |\n",
      "| step: 401800 | gen_loss: 49.545 | mel_loss: 12.235 | 3.239 sec / 10 steps |\n",
      "Validation mel_loss: 13.95794677734375\n",
      "| step: 401810 | gen_loss: 41.921 | mel_loss: 9.783 | 3.726 sec / 10 steps |\n",
      "| step: 401820 | gen_loss: 40.997 | mel_loss: 9.371 | 2.846 sec / 10 steps |\n",
      "| step: 401830 | gen_loss: 46.110 | mel_loss: 10.978 | 3.909 sec / 10 steps |\n",
      "| step: 401840 | gen_loss: 48.505 | mel_loss: 12.223 | 3.220 sec / 10 steps |\n",
      "| step: 401850 | gen_loss: 48.313 | mel_loss: 12.664 | 5.383 sec / 10 steps |\n",
      "| step: 401860 | gen_loss: 49.642 | mel_loss: 11.362 | 3.593 sec / 10 steps |\n",
      "| step: 401870 | gen_loss: 45.358 | mel_loss: 10.690 | 4.095 sec / 10 steps |\n",
      "| step: 401880 | gen_loss: 38.125 | mel_loss: 8.276 | 3.077 sec / 10 steps |\n",
      "| step: 401890 | gen_loss: 36.300 | mel_loss: 8.191 | 3.444 sec / 10 steps |\n",
      "| step: 401900 | gen_loss: 39.552 | mel_loss: 8.798 | 4.005 sec / 10 steps |\n",
      "Validation mel_loss: 13.81834888458252\n",
      "| step: 401910 | gen_loss: 39.318 | mel_loss: 8.514 | 3.741 sec / 10 steps |\n",
      "| step: 401920 | gen_loss: 49.979 | mel_loss: 12.465 | 4.216 sec / 10 steps |\n",
      "| step: 401930 | gen_loss: 43.028 | mel_loss: 9.959 | 3.416 sec / 10 steps |\n",
      "| step: 401940 | gen_loss: 48.521 | mel_loss: 11.235 | 4.594 sec / 10 steps |\n",
      "| step: 401950 | gen_loss: 45.089 | mel_loss: 11.387 | 3.176 sec / 10 steps |\n",
      "| step: 401960 | gen_loss: 45.736 | mel_loss: 10.684 | 3.570 sec / 10 steps |\n",
      "| step: 401970 | gen_loss: 47.155 | mel_loss: 11.312 | 4.377 sec / 10 steps |\n",
      "| step: 401980 | gen_loss: 42.668 | mel_loss: 10.849 | 4.056 sec / 10 steps |\n",
      "| step: 401990 | gen_loss: 42.417 | mel_loss: 10.394 | 3.086 sec / 10 steps |\n",
      "| step: 402000 | gen_loss: 45.484 | mel_loss: 10.360 | 3.569 sec / 10 steps |\n",
      "Validation mel_loss: 14.012191772460938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 402010 | gen_loss: 41.444 | mel_loss: 9.487 | 3.609 sec / 10 steps |\n",
      "| step: 402020 | gen_loss: 49.151 | mel_loss: 12.075 | 3.339 sec / 10 steps |\n",
      "| step: 402030 | gen_loss: 42.375 | mel_loss: 9.527 | 5.227 sec / 10 steps |\n",
      "| step: 402040 | gen_loss: 48.296 | mel_loss: 11.352 | 3.109 sec / 10 steps |\n",
      "| step: 402050 | gen_loss: 45.325 | mel_loss: 11.418 | 3.837 sec / 10 steps |\n",
      "| step: 402060 | gen_loss: 45.806 | mel_loss: 10.414 | 3.195 sec / 10 steps |\n",
      "| step: 402070 | gen_loss: 39.923 | mel_loss: 9.008 | 3.553 sec / 10 steps |\n",
      "| step: 402080 | gen_loss: 43.958 | mel_loss: 10.282 | 3.274 sec / 10 steps |\n",
      "| step: 402090 | gen_loss: 48.274 | mel_loss: 11.435 | 3.321 sec / 10 steps |\n",
      "| step: 402100 | gen_loss: 47.457 | mel_loss: 11.664 | 3.414 sec / 10 steps |\n",
      "Validation mel_loss: 13.963515281677246\n",
      "| step: 402110 | gen_loss: 47.603 | mel_loss: 11.060 | 3.190 sec / 10 steps |\n",
      "| step: 402120 | gen_loss: 43.055 | mel_loss: 9.715 | 3.205 sec / 10 steps |\n",
      "| step: 402130 | gen_loss: 43.441 | mel_loss: 10.201 | 3.552 sec / 10 steps |\n",
      "| step: 402140 | gen_loss: 45.766 | mel_loss: 10.682 | 3.713 sec / 10 steps |\n",
      "| step: 402150 | gen_loss: 47.499 | mel_loss: 11.416 | 3.734 sec / 10 steps |\n",
      "| step: 402160 | gen_loss: 47.742 | mel_loss: 11.649 | 3.466 sec / 10 steps |\n",
      "| step: 402170 | gen_loss: 42.689 | mel_loss: 9.629 | 3.487 sec / 10 steps |\n",
      "| step: 402180 | gen_loss: 49.054 | mel_loss: 11.495 | 3.349 sec / 10 steps |\n",
      "| step: 402190 | gen_loss: 42.888 | mel_loss: 10.167 | 3.333 sec / 10 steps |\n",
      "| step: 402200 | gen_loss: 45.397 | mel_loss: 11.125 | 3.383 sec / 10 steps |\n",
      "Validation mel_loss: 13.94129467010498\n",
      "| step: 402210 | gen_loss: 46.140 | mel_loss: 11.203 | 3.351 sec / 10 steps |\n",
      "| step: 402220 | gen_loss: 47.068 | mel_loss: 11.701 | 3.040 sec / 10 steps |\n",
      "| step: 402230 | gen_loss: 44.928 | mel_loss: 10.695 | 3.446 sec / 10 steps |\n",
      "| step: 402240 | gen_loss: 47.689 | mel_loss: 10.683 | 3.950 sec / 10 steps |\n",
      "| step: 402250 | gen_loss: 49.515 | mel_loss: 11.090 | 3.345 sec / 10 steps |\n",
      "| step: 402260 | gen_loss: 46.609 | mel_loss: 10.984 | 4.215 sec / 10 steps |\n",
      "| step: 402270 | gen_loss: 43.386 | mel_loss: 9.240 | 3.268 sec / 10 steps |\n",
      "| step: 402280 | gen_loss: 43.595 | mel_loss: 10.315 | 4.248 sec / 10 steps |\n",
      "| step: 402290 | gen_loss: 47.513 | mel_loss: 12.363 | 3.432 sec / 10 steps |\n",
      "| step: 402300 | gen_loss: 46.967 | mel_loss: 11.474 | 3.722 sec / 10 steps |\n",
      "Validation mel_loss: 13.839069366455078\n",
      "| step: 402310 | gen_loss: 43.853 | mel_loss: 9.524 | 3.498 sec / 10 steps |\n",
      "| step: 402320 | gen_loss: 51.337 | mel_loss: 12.845 | 3.126 sec / 10 steps |\n",
      "| step: 402330 | gen_loss: 47.525 | mel_loss: 11.358 | 2.949 sec / 10 steps |\n",
      "| step: 402340 | gen_loss: 48.194 | mel_loss: 12.154 | 3.157 sec / 10 steps |\n",
      "| step: 402350 | gen_loss: 44.020 | mel_loss: 9.997 | 3.259 sec / 10 steps |\n",
      "| step: 402360 | gen_loss: 51.571 | mel_loss: 12.992 | 3.159 sec / 10 steps |\n",
      "| step: 402370 | gen_loss: 55.329 | mel_loss: 13.694 | 3.598 sec / 10 steps |\n",
      "| step: 402380 | gen_loss: 46.580 | mel_loss: 10.308 | 3.492 sec / 10 steps |\n",
      "| step: 402390 | gen_loss: 52.905 | mel_loss: 13.030 | 3.164 sec / 10 steps |\n",
      "| step: 402400 | gen_loss: 43.500 | mel_loss: 9.383 | 3.651 sec / 10 steps |\n",
      "Validation mel_loss: 14.061379432678223\n",
      "| step: 402410 | gen_loss: 44.181 | mel_loss: 9.536 | 3.565 sec / 10 steps |\n",
      "| step: 402420 | gen_loss: 46.346 | mel_loss: 11.124 | 3.412 sec / 10 steps |\n",
      "| step: 402430 | gen_loss: 43.908 | mel_loss: 10.131 | 3.311 sec / 10 steps |\n",
      "| step: 402440 | gen_loss: 41.719 | mel_loss: 9.811 | 3.681 sec / 10 steps |\n",
      "| step: 402450 | gen_loss: 46.356 | mel_loss: 10.575 | 3.414 sec / 10 steps |\n",
      "| step: 402460 | gen_loss: 42.945 | mel_loss: 9.836 | 3.211 sec / 10 steps |\n",
      "| step: 402470 | gen_loss: 50.445 | mel_loss: 12.024 | 3.486 sec / 10 steps |\n",
      "| step: 402480 | gen_loss: 44.401 | mel_loss: 10.214 | 4.120 sec / 10 steps |\n",
      "| step: 402490 | gen_loss: 44.632 | mel_loss: 10.861 | 3.134 sec / 10 steps |\n",
      "| step: 402500 | gen_loss: 50.574 | mel_loss: 11.346 | 3.875 sec / 10 steps |\n",
      "Validation mel_loss: 13.863539695739746\n",
      "|| Epoch: 536 ||\n",
      "| step: 402510 | gen_loss: 46.974 | mel_loss: 11.031 | 3.607 sec / 10 steps |\n",
      "| step: 402520 | gen_loss: 48.593 | mel_loss: 10.901 | 3.211 sec / 10 steps |\n",
      "| step: 402530 | gen_loss: 45.086 | mel_loss: 10.422 | 4.155 sec / 10 steps |\n",
      "| step: 402540 | gen_loss: 50.352 | mel_loss: 12.518 | 3.646 sec / 10 steps |\n",
      "| step: 402550 | gen_loss: 48.446 | mel_loss: 11.615 | 3.415 sec / 10 steps |\n",
      "| step: 402560 | gen_loss: 44.334 | mel_loss: 9.853 | 3.311 sec / 10 steps |\n",
      "| step: 402570 | gen_loss: 43.405 | mel_loss: 9.874 | 3.500 sec / 10 steps |\n",
      "| step: 402580 | gen_loss: 38.578 | mel_loss: 9.674 | 4.111 sec / 10 steps |\n",
      "| step: 402590 | gen_loss: 50.378 | mel_loss: 12.371 | 3.026 sec / 10 steps |\n",
      "| step: 402600 | gen_loss: 49.574 | mel_loss: 11.360 | 3.333 sec / 10 steps |\n",
      "Validation mel_loss: 14.118335723876953\n",
      "| step: 402610 | gen_loss: 42.173 | mel_loss: 8.524 | 3.525 sec / 10 steps |\n",
      "| step: 402620 | gen_loss: 44.332 | mel_loss: 9.676 | 3.620 sec / 10 steps |\n",
      "| step: 402630 | gen_loss: 49.586 | mel_loss: 11.787 | 2.910 sec / 10 steps |\n",
      "| step: 402640 | gen_loss: 48.387 | mel_loss: 11.940 | 3.442 sec / 10 steps |\n",
      "| step: 402650 | gen_loss: 45.501 | mel_loss: 10.485 | 3.043 sec / 10 steps |\n",
      "| step: 402660 | gen_loss: 47.579 | mel_loss: 11.937 | 3.091 sec / 10 steps |\n",
      "| step: 402670 | gen_loss: 44.060 | mel_loss: 9.826 | 3.513 sec / 10 steps |\n",
      "| step: 402680 | gen_loss: 45.797 | mel_loss: 10.437 | 3.299 sec / 10 steps |\n",
      "| step: 402690 | gen_loss: 52.165 | mel_loss: 12.448 | 3.494 sec / 10 steps |\n",
      "| step: 402700 | gen_loss: 47.135 | mel_loss: 10.819 | 3.416 sec / 10 steps |\n",
      "Validation mel_loss: 13.958871841430664\n",
      "| step: 402710 | gen_loss: 49.410 | mel_loss: 11.999 | 4.165 sec / 10 steps |\n",
      "| step: 402720 | gen_loss: 45.919 | mel_loss: 10.816 | 3.513 sec / 10 steps |\n",
      "| step: 402730 | gen_loss: 51.825 | mel_loss: 12.594 | 3.262 sec / 10 steps |\n",
      "| step: 402740 | gen_loss: 45.246 | mel_loss: 10.526 | 3.200 sec / 10 steps |\n",
      "| step: 402750 | gen_loss: 42.242 | mel_loss: 9.408 | 3.464 sec / 10 steps |\n",
      "| step: 402760 | gen_loss: 36.330 | mel_loss: 8.126 | 3.268 sec / 10 steps |\n",
      "| step: 402770 | gen_loss: 49.929 | mel_loss: 12.133 | 3.150 sec / 10 steps |\n",
      "| step: 402780 | gen_loss: 43.995 | mel_loss: 9.858 | 3.277 sec / 10 steps |\n",
      "| step: 402790 | gen_loss: 45.949 | mel_loss: 10.766 | 3.516 sec / 10 steps |\n",
      "| step: 402800 | gen_loss: 43.559 | mel_loss: 10.668 | 3.144 sec / 10 steps |\n",
      "Validation mel_loss: 13.925716400146484\n",
      "| step: 402810 | gen_loss: 46.249 | mel_loss: 10.978 | 3.652 sec / 10 steps |\n",
      "| step: 402820 | gen_loss: 47.966 | mel_loss: 11.684 | 3.340 sec / 10 steps |\n",
      "| step: 402830 | gen_loss: 46.977 | mel_loss: 10.351 | 3.243 sec / 10 steps |\n",
      "| step: 402840 | gen_loss: 44.685 | mel_loss: 10.988 | 3.593 sec / 10 steps |\n",
      "| step: 402850 | gen_loss: 52.687 | mel_loss: 11.857 | 4.176 sec / 10 steps |\n",
      "| step: 402860 | gen_loss: 42.476 | mel_loss: 9.607 | 3.505 sec / 10 steps |\n",
      "| step: 402870 | gen_loss: 44.725 | mel_loss: 9.884 | 3.012 sec / 10 steps |\n",
      "| step: 402880 | gen_loss: 51.204 | mel_loss: 11.491 | 3.329 sec / 10 steps |\n",
      "| step: 402890 | gen_loss: 46.213 | mel_loss: 11.653 | 3.299 sec / 10 steps |\n",
      "| step: 402900 | gen_loss: 48.148 | mel_loss: 12.109 | 3.276 sec / 10 steps |\n",
      "Validation mel_loss: 13.902674674987793\n",
      "| step: 402910 | gen_loss: 47.323 | mel_loss: 10.313 | 3.437 sec / 10 steps |\n",
      "| step: 402920 | gen_loss: 44.550 | mel_loss: 10.572 | 3.206 sec / 10 steps |\n",
      "| step: 402930 | gen_loss: 48.888 | mel_loss: 11.912 | 3.604 sec / 10 steps |\n",
      "| step: 402940 | gen_loss: 53.732 | mel_loss: 13.739 | 3.830 sec / 10 steps |\n",
      "| step: 402950 | gen_loss: 42.462 | mel_loss: 9.116 | 3.127 sec / 10 steps |\n",
      "| step: 402960 | gen_loss: 45.008 | mel_loss: 10.375 | 3.033 sec / 10 steps |\n",
      "| step: 402970 | gen_loss: 46.915 | mel_loss: 10.742 | 3.677 sec / 10 steps |\n",
      "| step: 402980 | gen_loss: 45.948 | mel_loss: 11.382 | 3.385 sec / 10 steps |\n",
      "| step: 402990 | gen_loss: 44.561 | mel_loss: 9.726 | 3.244 sec / 10 steps |\n",
      "| step: 403000 | gen_loss: 48.971 | mel_loss: 11.485 | 3.220 sec / 10 steps |\n",
      "Validation mel_loss: 14.011040687561035\n",
      "| step: 403010 | gen_loss: 44.855 | mel_loss: 9.985 | 3.546 sec / 10 steps |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 403020 | gen_loss: 52.838 | mel_loss: 12.654 | 3.245 sec / 10 steps |\n",
      "| step: 403030 | gen_loss: 42.050 | mel_loss: 9.047 | 3.153 sec / 10 steps |\n",
      "| step: 403040 | gen_loss: 48.542 | mel_loss: 11.766 | 3.271 sec / 10 steps |\n",
      "| step: 403050 | gen_loss: 49.246 | mel_loss: 12.229 | 3.296 sec / 10 steps |\n",
      "| step: 403060 | gen_loss: 44.365 | mel_loss: 10.554 | 3.281 sec / 10 steps |\n",
      "| step: 403070 | gen_loss: 43.786 | mel_loss: 10.846 | 3.340 sec / 10 steps |\n",
      "| step: 403080 | gen_loss: 43.631 | mel_loss: 10.604 | 3.599 sec / 10 steps |\n",
      "| step: 403090 | gen_loss: 47.917 | mel_loss: 12.096 | 3.583 sec / 10 steps |\n",
      "| step: 403100 | gen_loss: 42.876 | mel_loss: 9.416 | 3.231 sec / 10 steps |\n",
      "Validation mel_loss: 14.231578826904297\n",
      "| step: 403110 | gen_loss: 46.936 | mel_loss: 10.997 | 3.459 sec / 10 steps |\n",
      "| step: 403120 | gen_loss: 41.607 | mel_loss: 9.007 | 3.206 sec / 10 steps |\n",
      "| step: 403130 | gen_loss: 50.303 | mel_loss: 12.461 | 3.301 sec / 10 steps |\n",
      "| step: 403140 | gen_loss: 51.767 | mel_loss: 12.786 | 3.577 sec / 10 steps |\n",
      "| step: 403150 | gen_loss: 48.697 | mel_loss: 11.764 | 3.292 sec / 10 steps |\n",
      "| step: 403160 | gen_loss: 46.313 | mel_loss: 11.079 | 3.497 sec / 10 steps |\n",
      "| step: 403170 | gen_loss: 46.088 | mel_loss: 10.908 | 3.048 sec / 10 steps |\n",
      "| step: 403180 | gen_loss: 50.719 | mel_loss: 12.901 | 3.145 sec / 10 steps |\n",
      "| step: 403190 | gen_loss: 47.229 | mel_loss: 12.074 | 3.317 sec / 10 steps |\n",
      "| step: 403200 | gen_loss: 47.667 | mel_loss: 11.657 | 2.983 sec / 10 steps |\n",
      "Validation mel_loss: 14.037437438964844\n",
      "| step: 403210 | gen_loss: 44.745 | mel_loss: 11.202 | 3.197 sec / 10 steps |\n",
      "| step: 403220 | gen_loss: 49.541 | mel_loss: 12.400 | 3.519 sec / 10 steps |\n",
      "| step: 403230 | gen_loss: 51.360 | mel_loss: 12.566 | 3.122 sec / 10 steps |\n",
      "| step: 403240 | gen_loss: 42.589 | mel_loss: 9.744 | 3.276 sec / 10 steps |\n",
      "| step: 403250 | gen_loss: 45.939 | mel_loss: 11.547 | 3.299 sec / 10 steps |\n",
      "|| Epoch: 537 ||\n",
      "| step: 403260 | gen_loss: 46.769 | mel_loss: 11.757 | 3.519 sec / 10 steps |\n",
      "| step: 403270 | gen_loss: 44.004 | mel_loss: 10.180 | 3.727 sec / 10 steps |\n",
      "| step: 403280 | gen_loss: 44.366 | mel_loss: 10.645 | 3.923 sec / 10 steps |\n",
      "| step: 403290 | gen_loss: 46.551 | mel_loss: 11.006 | 3.340 sec / 10 steps |\n",
      "| step: 403300 | gen_loss: 44.780 | mel_loss: 11.289 | 3.281 sec / 10 steps |\n",
      "Validation mel_loss: 14.320009231567383\n",
      "| step: 403310 | gen_loss: 49.549 | mel_loss: 12.941 | 3.079 sec / 10 steps |\n",
      "| step: 403320 | gen_loss: 52.173 | mel_loss: 13.810 | 3.384 sec / 10 steps |\n",
      "| step: 403330 | gen_loss: 38.092 | mel_loss: 8.996 | 3.468 sec / 10 steps |\n",
      "| step: 403340 | gen_loss: 51.275 | mel_loss: 12.638 | 3.662 sec / 10 steps |\n",
      "| step: 403350 | gen_loss: 46.657 | mel_loss: 11.235 | 3.268 sec / 10 steps |\n",
      "| step: 403360 | gen_loss: 50.865 | mel_loss: 11.518 | 3.383 sec / 10 steps |\n",
      "| step: 403370 | gen_loss: 45.767 | mel_loss: 11.674 | 3.192 sec / 10 steps |\n",
      "| step: 403380 | gen_loss: 45.427 | mel_loss: 11.252 | 3.183 sec / 10 steps |\n",
      "| step: 403390 | gen_loss: 49.160 | mel_loss: 11.849 | 4.007 sec / 10 steps |\n",
      "| step: 403400 | gen_loss: 43.617 | mel_loss: 10.495 | 3.859 sec / 10 steps |\n",
      "Validation mel_loss: 14.20731258392334\n",
      "| step: 403410 | gen_loss: 46.397 | mel_loss: 10.780 | 3.354 sec / 10 steps |\n",
      "| step: 403420 | gen_loss: 49.104 | mel_loss: 11.689 | 3.207 sec / 10 steps |\n",
      "| step: 403430 | gen_loss: 48.269 | mel_loss: 12.575 | 3.438 sec / 10 steps |\n",
      "| step: 403440 | gen_loss: 40.069 | mel_loss: 9.758 | 3.437 sec / 10 steps |\n",
      "| step: 403450 | gen_loss: 51.939 | mel_loss: 13.351 | 3.405 sec / 10 steps |\n",
      "| step: 403460 | gen_loss: 43.341 | mel_loss: 11.137 | 3.112 sec / 10 steps |\n",
      "| step: 403470 | gen_loss: 43.466 | mel_loss: 10.593 | 3.081 sec / 10 steps |\n",
      "| step: 403480 | gen_loss: 45.955 | mel_loss: 11.659 | 3.032 sec / 10 steps |\n",
      "| step: 403490 | gen_loss: 49.757 | mel_loss: 12.136 | 3.968 sec / 10 steps |\n",
      "| step: 403500 | gen_loss: 46.741 | mel_loss: 11.153 | 3.175 sec / 10 steps |\n",
      "Validation mel_loss: 14.135525703430176\n",
      "| step: 403510 | gen_loss: 41.833 | mel_loss: 9.313 | 3.465 sec / 10 steps |\n",
      "| step: 403520 | gen_loss: 47.780 | mel_loss: 10.510 | 3.429 sec / 10 steps |\n",
      "| step: 403530 | gen_loss: 49.803 | mel_loss: 12.539 | 3.243 sec / 10 steps |\n",
      "| step: 403540 | gen_loss: 45.678 | mel_loss: 11.391 | 3.322 sec / 10 steps |\n",
      "| step: 403550 | gen_loss: 49.792 | mel_loss: 12.795 | 3.186 sec / 10 steps |\n",
      "| step: 403560 | gen_loss: 48.621 | mel_loss: 11.691 | 3.383 sec / 10 steps |\n",
      "| step: 403570 | gen_loss: 45.183 | mel_loss: 10.767 | 3.651 sec / 10 steps |\n",
      "| step: 403580 | gen_loss: 42.573 | mel_loss: 9.612 | 3.580 sec / 10 steps |\n",
      "| step: 403590 | gen_loss: 42.771 | mel_loss: 10.669 | 2.950 sec / 10 steps |\n",
      "| step: 403600 | gen_loss: 47.939 | mel_loss: 10.795 | 3.578 sec / 10 steps |\n",
      "Validation mel_loss: 14.19923210144043\n",
      "| step: 403610 | gen_loss: 41.373 | mel_loss: 9.312 | 3.345 sec / 10 steps |\n",
      "| step: 403620 | gen_loss: 49.004 | mel_loss: 12.033 | 3.505 sec / 10 steps |\n",
      "| step: 403630 | gen_loss: 46.283 | mel_loss: 11.668 | 3.111 sec / 10 steps |\n",
      "| step: 403640 | gen_loss: 46.673 | mel_loss: 10.973 | 3.542 sec / 10 steps |\n",
      "| step: 403650 | gen_loss: 43.397 | mel_loss: 9.112 | 3.142 sec / 10 steps |\n",
      "| step: 403660 | gen_loss: 48.901 | mel_loss: 12.234 | 3.134 sec / 10 steps |\n",
      "| step: 403670 | gen_loss: 48.922 | mel_loss: 11.897 | 2.966 sec / 10 steps |\n",
      "| step: 403680 | gen_loss: 46.560 | mel_loss: 11.965 | 3.447 sec / 10 steps |\n",
      "| step: 403690 | gen_loss: 42.783 | mel_loss: 9.218 | 3.385 sec / 10 steps |\n",
      "| step: 403700 | gen_loss: 42.757 | mel_loss: 9.807 | 3.080 sec / 10 steps |\n",
      "Validation mel_loss: 14.213461875915527\n",
      "| step: 403710 | gen_loss: 41.127 | mel_loss: 9.072 | 3.810 sec / 10 steps |\n",
      "| step: 403720 | gen_loss: 44.398 | mel_loss: 11.151 | 3.467 sec / 10 steps |\n",
      "| step: 403730 | gen_loss: 44.186 | mel_loss: 10.305 | 3.371 sec / 10 steps |\n",
      "| step: 403740 | gen_loss: 46.191 | mel_loss: 12.240 | 3.116 sec / 10 steps |\n",
      "| step: 403750 | gen_loss: 45.786 | mel_loss: 11.163 | 3.225 sec / 10 steps |\n",
      "| step: 403760 | gen_loss: 47.727 | mel_loss: 12.048 | 3.153 sec / 10 steps |\n",
      "| step: 403770 | gen_loss: 49.864 | mel_loss: 12.589 | 3.395 sec / 10 steps |\n",
      "| step: 403780 | gen_loss: 48.558 | mel_loss: 12.251 | 3.155 sec / 10 steps |\n",
      "| step: 403790 | gen_loss: 49.935 | mel_loss: 12.859 | 3.388 sec / 10 steps |\n",
      "| step: 403800 | gen_loss: 47.203 | mel_loss: 12.250 | 3.289 sec / 10 steps |\n",
      "Validation mel_loss: 14.357985496520996\n",
      "| step: 403810 | gen_loss: 43.145 | mel_loss: 9.778 | 3.549 sec / 10 steps |\n",
      "| step: 403820 | gen_loss: 48.661 | mel_loss: 12.436 | 3.329 sec / 10 steps |\n",
      "| step: 403830 | gen_loss: 36.492 | mel_loss: 9.639 | 3.476 sec / 10 steps |\n",
      "| step: 403840 | gen_loss: 44.040 | mel_loss: 10.327 | 3.432 sec / 10 steps |\n",
      "| step: 403850 | gen_loss: 53.272 | mel_loss: 13.277 | 3.159 sec / 10 steps |\n",
      "| step: 403860 | gen_loss: 48.232 | mel_loss: 11.161 | 3.400 sec / 10 steps |\n",
      "| step: 403870 | gen_loss: 48.895 | mel_loss: 11.969 | 3.410 sec / 10 steps |\n",
      "| step: 403880 | gen_loss: 47.915 | mel_loss: 11.442 | 3.236 sec / 10 steps |\n",
      "| step: 403890 | gen_loss: 47.318 | mel_loss: 11.114 | 3.247 sec / 10 steps |\n",
      "| step: 403900 | gen_loss: 43.042 | mel_loss: 11.170 | 3.462 sec / 10 steps |\n",
      "Validation mel_loss: 14.30628490447998\n",
      "| step: 403910 | gen_loss: 46.881 | mel_loss: 11.749 | 3.580 sec / 10 steps |\n",
      "| step: 403920 | gen_loss: 51.957 | mel_loss: 12.524 | 2.973 sec / 10 steps |\n",
      "| step: 403930 | gen_loss: 45.861 | mel_loss: 11.105 | 3.151 sec / 10 steps |\n",
      "| step: 403940 | gen_loss: 48.873 | mel_loss: 12.655 | 3.946 sec / 10 steps |\n",
      "| step: 403950 | gen_loss: 47.264 | mel_loss: 12.228 | 3.116 sec / 10 steps |\n",
      "| step: 403960 | gen_loss: 40.560 | mel_loss: 10.565 | 3.238 sec / 10 steps |\n",
      "| step: 403970 | gen_loss: 40.398 | mel_loss: 9.620 | 3.465 sec / 10 steps |\n",
      "| step: 403980 | gen_loss: 48.843 | mel_loss: 12.183 | 3.007 sec / 10 steps |\n",
      "| step: 403990 | gen_loss: 43.692 | mel_loss: 10.280 | 3.159 sec / 10 steps |\n",
      "| step: 404000 | gen_loss: 49.087 | mel_loss: 12.591 | 3.033 sec / 10 steps |\n",
      "Validation mel_loss: 14.455479621887207\n",
      "|| Epoch: 538 ||\n",
      "| step: 404010 | gen_loss: 46.922 | mel_loss: 10.325 | 3.298 sec / 10 steps |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 404020 | gen_loss: 45.797 | mel_loss: 10.025 | 3.332 sec / 10 steps |\n",
      "| step: 404030 | gen_loss: 50.251 | mel_loss: 11.934 | 3.383 sec / 10 steps |\n",
      "| step: 404040 | gen_loss: 45.954 | mel_loss: 11.007 | 3.315 sec / 10 steps |\n",
      "| step: 404050 | gen_loss: 50.529 | mel_loss: 13.076 | 3.232 sec / 10 steps |\n",
      "| step: 404060 | gen_loss: 47.311 | mel_loss: 13.195 | 3.418 sec / 10 steps |\n",
      "| step: 404070 | gen_loss: 48.408 | mel_loss: 12.416 | 3.738 sec / 10 steps |\n",
      "| step: 404080 | gen_loss: 49.811 | mel_loss: 11.986 | 3.291 sec / 10 steps |\n",
      "| step: 404090 | gen_loss: 45.296 | mel_loss: 11.256 | 3.461 sec / 10 steps |\n",
      "| step: 404100 | gen_loss: 47.575 | mel_loss: 11.188 | 3.107 sec / 10 steps |\n",
      "Validation mel_loss: 14.3267183303833\n",
      "| step: 404110 | gen_loss: 50.984 | mel_loss: 12.552 | 3.022 sec / 10 steps |\n",
      "| step: 404120 | gen_loss: 48.979 | mel_loss: 11.938 | 3.417 sec / 10 steps |\n",
      "| step: 404130 | gen_loss: 43.997 | mel_loss: 9.893 | 3.384 sec / 10 steps |\n",
      "| step: 404140 | gen_loss: 45.237 | mel_loss: 10.291 | 3.588 sec / 10 steps |\n",
      "| step: 404150 | gen_loss: 48.300 | mel_loss: 12.300 | 2.903 sec / 10 steps |\n",
      "| step: 404160 | gen_loss: 45.985 | mel_loss: 11.081 | 3.318 sec / 10 steps |\n",
      "| step: 404170 | gen_loss: 46.085 | mel_loss: 10.508 | 3.309 sec / 10 steps |\n",
      "| step: 404180 | gen_loss: 41.688 | mel_loss: 9.451 | 3.128 sec / 10 steps |\n",
      "| step: 404190 | gen_loss: 50.065 | mel_loss: 12.764 | 3.128 sec / 10 steps |\n",
      "| step: 404200 | gen_loss: 42.065 | mel_loss: 9.676 | 3.254 sec / 10 steps |\n",
      "Validation mel_loss: 14.4603853225708\n",
      "| step: 404210 | gen_loss: 43.105 | mel_loss: 11.047 | 3.283 sec / 10 steps |\n",
      "| step: 404220 | gen_loss: 53.214 | mel_loss: 13.356 | 3.029 sec / 10 steps |\n",
      "| step: 404230 | gen_loss: 47.981 | mel_loss: 11.266 | 3.374 sec / 10 steps |\n",
      "| step: 404240 | gen_loss: 46.395 | mel_loss: 12.529 | 3.102 sec / 10 steps |\n",
      "| step: 404250 | gen_loss: 48.091 | mel_loss: 11.989 | 3.145 sec / 10 steps |\n",
      "| step: 404260 | gen_loss: 50.217 | mel_loss: 12.372 | 2.992 sec / 10 steps |\n",
      "| step: 404270 | gen_loss: 41.498 | mel_loss: 8.816 | 3.806 sec / 10 steps |\n",
      "| step: 404280 | gen_loss: 49.831 | mel_loss: 12.354 | 3.563 sec / 10 steps |\n",
      "| step: 404290 | gen_loss: 43.386 | mel_loss: 10.283 | 3.240 sec / 10 steps |\n",
      "| step: 404300 | gen_loss: 49.763 | mel_loss: 12.297 | 3.044 sec / 10 steps |\n",
      "Validation mel_loss: 14.289976119995117\n",
      "| step: 404310 | gen_loss: 48.436 | mel_loss: 11.774 | 3.461 sec / 10 steps |\n",
      "| step: 404320 | gen_loss: 46.973 | mel_loss: 11.313 | 4.048 sec / 10 steps |\n",
      "| step: 404330 | gen_loss: 43.748 | mel_loss: 9.915 | 3.327 sec / 10 steps |\n",
      "| step: 404340 | gen_loss: 42.721 | mel_loss: 10.417 | 3.279 sec / 10 steps |\n",
      "| step: 404350 | gen_loss: 48.980 | mel_loss: 12.565 | 3.295 sec / 10 steps |\n",
      "| step: 404360 | gen_loss: 47.541 | mel_loss: 11.980 | 3.651 sec / 10 steps |\n",
      "| step: 404370 | gen_loss: 46.755 | mel_loss: 10.696 | 2.881 sec / 10 steps |\n",
      "| step: 404380 | gen_loss: 45.322 | mel_loss: 10.800 | 3.520 sec / 10 steps |\n",
      "| step: 404390 | gen_loss: 46.065 | mel_loss: 11.582 | 3.045 sec / 10 steps |\n",
      "| step: 404400 | gen_loss: 49.512 | mel_loss: 11.572 | 3.320 sec / 10 steps |\n",
      "Validation mel_loss: 14.18259334564209\n",
      "| step: 404410 | gen_loss: 50.700 | mel_loss: 12.475 | 3.601 sec / 10 steps |\n",
      "| step: 404420 | gen_loss: 50.722 | mel_loss: 13.246 | 3.165 sec / 10 steps |\n",
      "| step: 404430 | gen_loss: 48.301 | mel_loss: 12.233 | 3.147 sec / 10 steps |\n",
      "| step: 404440 | gen_loss: 50.268 | mel_loss: 12.431 | 3.577 sec / 10 steps |\n",
      "| step: 404450 | gen_loss: 43.401 | mel_loss: 10.052 | 3.262 sec / 10 steps |\n",
      "| step: 404460 | gen_loss: 50.091 | mel_loss: 11.443 | 3.249 sec / 10 steps |\n",
      "| step: 404470 | gen_loss: 48.090 | mel_loss: 11.841 | 2.974 sec / 10 steps |\n",
      "| step: 404480 | gen_loss: 39.778 | mel_loss: 8.924 | 3.363 sec / 10 steps |\n",
      "| step: 404490 | gen_loss: 45.009 | mel_loss: 11.212 | 3.654 sec / 10 steps |\n",
      "| step: 404500 | gen_loss: 45.815 | mel_loss: 11.495 | 3.216 sec / 10 steps |\n",
      "Validation mel_loss: 14.442399978637695\n",
      "| step: 404510 | gen_loss: 45.245 | mel_loss: 11.488 | 3.472 sec / 10 steps |\n",
      "| step: 404520 | gen_loss: 47.020 | mel_loss: 11.841 | 3.474 sec / 10 steps |\n",
      "| step: 404530 | gen_loss: 44.620 | mel_loss: 10.834 | 3.468 sec / 10 steps |\n",
      "| step: 404540 | gen_loss: 41.999 | mel_loss: 10.256 | 3.505 sec / 10 steps |\n",
      "| step: 404550 | gen_loss: 52.398 | mel_loss: 13.200 | 3.646 sec / 10 steps |\n",
      "| step: 404560 | gen_loss: 44.946 | mel_loss: 10.403 | 3.298 sec / 10 steps |\n",
      "| step: 404570 | gen_loss: 43.334 | mel_loss: 9.662 | 3.347 sec / 10 steps |\n",
      "| step: 404580 | gen_loss: 45.065 | mel_loss: 10.305 | 3.266 sec / 10 steps |\n",
      "| step: 404590 | gen_loss: 50.041 | mel_loss: 11.916 | 3.470 sec / 10 steps |\n",
      "| step: 404600 | gen_loss: 45.325 | mel_loss: 11.197 | 3.125 sec / 10 steps |\n",
      "Validation mel_loss: 14.419731140136719\n",
      "| step: 404610 | gen_loss: 40.087 | mel_loss: 9.123 | 3.699 sec / 10 steps |\n",
      "| step: 404620 | gen_loss: 48.339 | mel_loss: 11.843 | 3.275 sec / 10 steps |\n",
      "| step: 404630 | gen_loss: 47.042 | mel_loss: 11.711 | 3.202 sec / 10 steps |\n",
      "| step: 404640 | gen_loss: 46.616 | mel_loss: 11.827 | 3.325 sec / 10 steps |\n",
      "| step: 404650 | gen_loss: 50.839 | mel_loss: 13.151 | 3.788 sec / 10 steps |\n",
      "| step: 404660 | gen_loss: 45.944 | mel_loss: 11.436 | 3.316 sec / 10 steps |\n",
      "| step: 404670 | gen_loss: 50.265 | mel_loss: 13.062 | 3.002 sec / 10 steps |\n",
      "| step: 404680 | gen_loss: 48.861 | mel_loss: 11.752 | 3.697 sec / 10 steps |\n",
      "| step: 404690 | gen_loss: 48.517 | mel_loss: 12.446 | 3.040 sec / 10 steps |\n",
      "| step: 404700 | gen_loss: 47.715 | mel_loss: 11.319 | 2.887 sec / 10 steps |\n",
      "Validation mel_loss: 14.287741661071777\n",
      "| step: 404710 | gen_loss: 43.105 | mel_loss: 10.043 | 3.424 sec / 10 steps |\n",
      "| step: 404720 | gen_loss: 45.810 | mel_loss: 11.298 | 3.316 sec / 10 steps |\n",
      "| step: 404730 | gen_loss: 48.823 | mel_loss: 11.876 | 3.432 sec / 10 steps |\n",
      "| step: 404740 | gen_loss: 50.608 | mel_loss: 11.793 | 3.410 sec / 10 steps |\n",
      "| step: 404750 | gen_loss: 49.034 | mel_loss: 11.867 | 3.085 sec / 10 steps |\n",
      "|| Epoch: 539 ||\n",
      "| step: 404760 | gen_loss: 44.666 | mel_loss: 9.999 | 3.405 sec / 10 steps |\n",
      "| step: 404770 | gen_loss: 45.171 | mel_loss: 10.879 | 3.421 sec / 10 steps |\n",
      "| step: 404780 | gen_loss: 52.978 | mel_loss: 13.029 | 3.063 sec / 10 steps |\n",
      "| step: 404790 | gen_loss: 46.417 | mel_loss: 10.542 | 4.146 sec / 10 steps |\n",
      "| step: 404800 | gen_loss: 50.385 | mel_loss: 12.190 | 3.047 sec / 10 steps |\n",
      "Validation mel_loss: 14.311100006103516\n",
      "| step: 404810 | gen_loss: 45.893 | mel_loss: 10.999 | 3.193 sec / 10 steps |\n",
      "| step: 404820 | gen_loss: 54.920 | mel_loss: 14.166 | 3.520 sec / 10 steps |\n",
      "| step: 404830 | gen_loss: 49.201 | mel_loss: 12.229 | 3.031 sec / 10 steps |\n",
      "| step: 404840 | gen_loss: 45.359 | mel_loss: 10.849 | 3.689 sec / 10 steps |\n",
      "| step: 404850 | gen_loss: 52.082 | mel_loss: 12.710 | 2.909 sec / 10 steps |\n",
      "| step: 404860 | gen_loss: 49.389 | mel_loss: 12.599 | 3.335 sec / 10 steps |\n",
      "| step: 404870 | gen_loss: 43.557 | mel_loss: 10.037 | 2.914 sec / 10 steps |\n",
      "| step: 404880 | gen_loss: 43.797 | mel_loss: 10.129 | 3.274 sec / 10 steps |\n",
      "| step: 404890 | gen_loss: 47.073 | mel_loss: 11.356 | 3.083 sec / 10 steps |\n",
      "| step: 404900 | gen_loss: 44.920 | mel_loss: 10.309 | 3.529 sec / 10 steps |\n",
      "Validation mel_loss: 14.41270637512207\n",
      "| step: 404910 | gen_loss: 42.198 | mel_loss: 9.449 | 3.724 sec / 10 steps |\n",
      "| step: 404920 | gen_loss: 44.954 | mel_loss: 10.823 | 3.180 sec / 10 steps |\n",
      "| step: 404930 | gen_loss: 44.021 | mel_loss: 9.585 | 3.412 sec / 10 steps |\n",
      "| step: 404940 | gen_loss: 45.294 | mel_loss: 10.809 | 3.608 sec / 10 steps |\n",
      "| step: 404950 | gen_loss: 40.372 | mel_loss: 8.879 | 3.315 sec / 10 steps |\n",
      "| step: 404960 | gen_loss: 48.692 | mel_loss: 12.125 | 3.506 sec / 10 steps |\n",
      "| step: 404970 | gen_loss: 39.476 | mel_loss: 8.453 | 3.117 sec / 10 steps |\n",
      "| step: 404980 | gen_loss: 49.040 | mel_loss: 11.765 | 3.056 sec / 10 steps |\n",
      "| step: 404990 | gen_loss: 50.869 | mel_loss: 12.555 | 3.339 sec / 10 steps |\n",
      "| step: 405000 | gen_loss: 40.983 | mel_loss: 9.959 | 3.454 sec / 10 steps |\n",
      "Validation mel_loss: 14.197698593139648\n",
      "| step: 405010 | gen_loss: 44.734 | mel_loss: 10.637 | 3.411 sec / 10 steps |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 405020 | gen_loss: 49.049 | mel_loss: 12.339 | 3.405 sec / 10 steps |\n",
      "| step: 405030 | gen_loss: 41.561 | mel_loss: 9.609 | 2.943 sec / 10 steps |\n",
      "| step: 405040 | gen_loss: 52.604 | mel_loss: 12.311 | 3.129 sec / 10 steps |\n",
      "| step: 405050 | gen_loss: 47.278 | mel_loss: 11.344 | 3.418 sec / 10 steps |\n",
      "| step: 405060 | gen_loss: 43.559 | mel_loss: 10.540 | 3.245 sec / 10 steps |\n",
      "| step: 405070 | gen_loss: 49.935 | mel_loss: 11.994 | 3.022 sec / 10 steps |\n",
      "| step: 405080 | gen_loss: 44.917 | mel_loss: 10.612 | 3.368 sec / 10 steps |\n",
      "| step: 405090 | gen_loss: 44.815 | mel_loss: 11.323 | 3.324 sec / 10 steps |\n",
      "| step: 405100 | gen_loss: 43.773 | mel_loss: 9.956 | 3.616 sec / 10 steps |\n",
      "Validation mel_loss: 14.166643142700195\n",
      "| step: 405110 | gen_loss: 39.290 | mel_loss: 9.448 | 3.315 sec / 10 steps |\n",
      "| step: 405120 | gen_loss: 51.280 | mel_loss: 13.504 | 3.056 sec / 10 steps |\n",
      "| step: 405130 | gen_loss: 50.005 | mel_loss: 12.447 | 3.176 sec / 10 steps |\n",
      "| step: 405140 | gen_loss: 48.262 | mel_loss: 11.860 | 3.465 sec / 10 steps |\n",
      "| step: 405150 | gen_loss: 48.906 | mel_loss: 12.141 | 3.305 sec / 10 steps |\n",
      "| step: 405160 | gen_loss: 46.727 | mel_loss: 11.944 | 3.099 sec / 10 steps |\n",
      "| step: 405170 | gen_loss: 45.336 | mel_loss: 10.950 | 3.602 sec / 10 steps |\n",
      "| step: 405180 | gen_loss: 45.318 | mel_loss: 10.717 | 3.507 sec / 10 steps |\n",
      "| step: 405190 | gen_loss: 45.304 | mel_loss: 10.722 | 3.431 sec / 10 steps |\n",
      "| step: 405200 | gen_loss: 45.406 | mel_loss: 11.071 | 3.133 sec / 10 steps |\n",
      "Validation mel_loss: 14.375486373901367\n",
      "| step: 405210 | gen_loss: 51.628 | mel_loss: 12.188 | 3.042 sec / 10 steps |\n",
      "| step: 405220 | gen_loss: 43.176 | mel_loss: 10.079 | 3.265 sec / 10 steps |\n",
      "| step: 405230 | gen_loss: 44.869 | mel_loss: 9.705 | 3.474 sec / 10 steps |\n",
      "| step: 405240 | gen_loss: 45.531 | mel_loss: 9.985 | 3.854 sec / 10 steps |\n",
      "| step: 405250 | gen_loss: 45.714 | mel_loss: 10.493 | 3.770 sec / 10 steps |\n",
      "| step: 405260 | gen_loss: 49.061 | mel_loss: 12.480 | 3.357 sec / 10 steps |\n",
      "| step: 405270 | gen_loss: 41.156 | mel_loss: 9.281 | 3.322 sec / 10 steps |\n",
      "| step: 405280 | gen_loss: 45.043 | mel_loss: 10.685 | 3.476 sec / 10 steps |\n",
      "| step: 405290 | gen_loss: 44.174 | mel_loss: 11.153 | 3.082 sec / 10 steps |\n",
      "| step: 405300 | gen_loss: 48.123 | mel_loss: 11.878 | 3.177 sec / 10 steps |\n",
      "Validation mel_loss: 14.268033981323242\n",
      "| step: 405310 | gen_loss: 40.417 | mel_loss: 9.534 | 3.678 sec / 10 steps |\n",
      "| step: 405320 | gen_loss: 43.743 | mel_loss: 10.448 | 3.253 sec / 10 steps |\n",
      "| step: 405330 | gen_loss: 46.729 | mel_loss: 11.146 | 3.470 sec / 10 steps |\n",
      "| step: 405340 | gen_loss: 49.487 | mel_loss: 12.997 | 3.369 sec / 10 steps |\n",
      "| step: 405350 | gen_loss: 41.706 | mel_loss: 9.353 | 3.443 sec / 10 steps |\n",
      "| step: 405360 | gen_loss: 46.431 | mel_loss: 11.311 | 3.459 sec / 10 steps |\n",
      "| step: 405370 | gen_loss: 45.251 | mel_loss: 11.294 | 3.197 sec / 10 steps |\n",
      "| step: 405380 | gen_loss: 49.446 | mel_loss: 12.673 | 2.961 sec / 10 steps |\n",
      "| step: 405390 | gen_loss: 42.258 | mel_loss: 9.588 | 3.406 sec / 10 steps |\n",
      "| step: 405400 | gen_loss: 51.444 | mel_loss: 13.712 | 2.917 sec / 10 steps |\n",
      "Validation mel_loss: 14.406489372253418\n",
      "| step: 405410 | gen_loss: 48.803 | mel_loss: 10.958 | 3.284 sec / 10 steps |\n",
      "| step: 405420 | gen_loss: 47.835 | mel_loss: 10.884 | 3.149 sec / 10 steps |\n",
      "| step: 405430 | gen_loss: 49.553 | mel_loss: 12.002 | 3.228 sec / 10 steps |\n",
      "| step: 405440 | gen_loss: 48.886 | mel_loss: 12.265 | 2.997 sec / 10 steps |\n",
      "| step: 405450 | gen_loss: 43.919 | mel_loss: 10.576 | 3.105 sec / 10 steps |\n",
      "| step: 405460 | gen_loss: 48.317 | mel_loss: 11.615 | 3.368 sec / 10 steps |\n",
      "| step: 405470 | gen_loss: 44.492 | mel_loss: 11.121 | 3.697 sec / 10 steps |\n",
      "| step: 405480 | gen_loss: 48.652 | mel_loss: 12.172 | 3.279 sec / 10 steps |\n",
      "| step: 405490 | gen_loss: 49.143 | mel_loss: 12.205 | 3.176 sec / 10 steps |\n",
      "| step: 405500 | gen_loss: 44.628 | mel_loss: 10.004 | 3.422 sec / 10 steps |\n",
      "Validation mel_loss: 14.324555397033691\n",
      "|| Epoch: 540 ||\n",
      "| step: 405510 | gen_loss: 50.262 | mel_loss: 12.628 | 3.224 sec / 10 steps |\n",
      "| step: 405520 | gen_loss: 41.821 | mel_loss: 8.980 | 3.581 sec / 10 steps |\n",
      "| step: 405530 | gen_loss: 48.859 | mel_loss: 12.543 | 3.065 sec / 10 steps |\n",
      "| step: 405540 | gen_loss: 47.105 | mel_loss: 12.044 | 3.424 sec / 10 steps |\n",
      "| step: 405550 | gen_loss: 47.737 | mel_loss: 11.423 | 3.497 sec / 10 steps |\n",
      "| step: 405560 | gen_loss: 42.710 | mel_loss: 9.726 | 3.348 sec / 10 steps |\n",
      "| step: 405570 | gen_loss: 42.827 | mel_loss: 10.534 | 3.355 sec / 10 steps |\n",
      "| step: 405580 | gen_loss: 45.055 | mel_loss: 11.973 | 2.967 sec / 10 steps |\n",
      "| step: 405590 | gen_loss: 50.706 | mel_loss: 12.753 | 3.491 sec / 10 steps |\n",
      "| step: 405600 | gen_loss: 47.961 | mel_loss: 12.227 | 2.941 sec / 10 steps |\n",
      "Validation mel_loss: 14.247533798217773\n",
      "| step: 405610 | gen_loss: 46.826 | mel_loss: 11.403 | 3.224 sec / 10 steps |\n",
      "| step: 405620 | gen_loss: 50.053 | mel_loss: 12.628 | 3.251 sec / 10 steps |\n",
      "| step: 405630 | gen_loss: 44.789 | mel_loss: 10.925 | 3.332 sec / 10 steps |\n",
      "| step: 405640 | gen_loss: 47.702 | mel_loss: 12.424 | 3.649 sec / 10 steps |\n",
      "| step: 405650 | gen_loss: 44.341 | mel_loss: 9.940 | 3.342 sec / 10 steps |\n",
      "| step: 405660 | gen_loss: 51.501 | mel_loss: 13.211 | 3.317 sec / 10 steps |\n",
      "| step: 405670 | gen_loss: 46.264 | mel_loss: 11.244 | 3.411 sec / 10 steps |\n",
      "| step: 405680 | gen_loss: 42.488 | mel_loss: 9.754 | 3.651 sec / 10 steps |\n",
      "| step: 405690 | gen_loss: 46.086 | mel_loss: 11.227 | 3.284 sec / 10 steps |\n",
      "| step: 405700 | gen_loss: 46.284 | mel_loss: 10.793 | 3.188 sec / 10 steps |\n",
      "Validation mel_loss: 14.533556938171387\n",
      "| step: 405710 | gen_loss: 53.531 | mel_loss: 14.248 | 3.782 sec / 10 steps |\n",
      "| step: 405720 | gen_loss: 45.028 | mel_loss: 10.582 | 3.193 sec / 10 steps |\n",
      "| step: 405730 | gen_loss: 44.030 | mel_loss: 10.404 | 3.634 sec / 10 steps |\n",
      "| step: 405740 | gen_loss: 49.801 | mel_loss: 11.679 | 3.456 sec / 10 steps |\n",
      "| step: 405750 | gen_loss: 50.175 | mel_loss: 11.821 | 3.282 sec / 10 steps |\n",
      "| step: 405760 | gen_loss: 50.996 | mel_loss: 12.345 | 3.314 sec / 10 steps |\n",
      "| step: 405770 | gen_loss: 48.201 | mel_loss: 12.011 | 3.646 sec / 10 steps |\n",
      "| step: 405780 | gen_loss: 45.949 | mel_loss: 11.080 | 3.356 sec / 10 steps |\n",
      "| step: 405790 | gen_loss: 45.946 | mel_loss: 11.370 | 2.951 sec / 10 steps |\n",
      "| step: 405800 | gen_loss: 40.593 | mel_loss: 9.248 | 3.014 sec / 10 steps |\n",
      "Validation mel_loss: 14.27705192565918\n",
      "| step: 405810 | gen_loss: 45.782 | mel_loss: 11.056 | 3.178 sec / 10 steps |\n",
      "| step: 405820 | gen_loss: 48.242 | mel_loss: 11.645 | 3.223 sec / 10 steps |\n",
      "| step: 405830 | gen_loss: 44.639 | mel_loss: 9.895 | 3.008 sec / 10 steps |\n",
      "| step: 405840 | gen_loss: 48.146 | mel_loss: 11.547 | 3.582 sec / 10 steps |\n",
      "| step: 405850 | gen_loss: 45.470 | mel_loss: 10.549 | 3.870 sec / 10 steps |\n",
      "| step: 405860 | gen_loss: 48.054 | mel_loss: 11.675 | 2.881 sec / 10 steps |\n",
      "| step: 405870 | gen_loss: 45.033 | mel_loss: 11.458 | 3.372 sec / 10 steps |\n",
      "| step: 405880 | gen_loss: 43.857 | mel_loss: 9.915 | 3.749 sec / 10 steps |\n",
      "| step: 405890 | gen_loss: 45.736 | mel_loss: 11.264 | 3.105 sec / 10 steps |\n",
      "| step: 405900 | gen_loss: 46.579 | mel_loss: 11.516 | 3.041 sec / 10 steps |\n",
      "Validation mel_loss: 14.336073875427246\n",
      "| step: 405910 | gen_loss: 45.853 | mel_loss: 11.369 | 3.150 sec / 10 steps |\n",
      "| step: 405920 | gen_loss: 48.727 | mel_loss: 12.251 | 3.486 sec / 10 steps |\n",
      "| step: 405930 | gen_loss: 47.071 | mel_loss: 11.615 | 3.222 sec / 10 steps |\n",
      "| step: 405940 | gen_loss: 47.288 | mel_loss: 12.183 | 2.832 sec / 10 steps |\n",
      "| step: 405950 | gen_loss: 42.530 | mel_loss: 9.753 | 3.323 sec / 10 steps |\n",
      "| step: 405960 | gen_loss: 41.881 | mel_loss: 8.452 | 3.680 sec / 10 steps |\n",
      "| step: 405970 | gen_loss: 49.046 | mel_loss: 12.711 | 3.263 sec / 10 steps |\n",
      "| step: 405980 | gen_loss: 47.190 | mel_loss: 12.033 | 3.475 sec / 10 steps |\n",
      "| step: 405990 | gen_loss: 35.847 | mel_loss: 8.851 | 3.408 sec / 10 steps |\n",
      "| step: 406000 | gen_loss: 44.152 | mel_loss: 9.875 | 3.327 sec / 10 steps |\n",
      "Validation mel_loss: 14.357903480529785\n",
      "| step: 406010 | gen_loss: 44.513 | mel_loss: 9.909 | 3.311 sec / 10 steps |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 406020 | gen_loss: 46.892 | mel_loss: 11.623 | 3.295 sec / 10 steps |\n",
      "| step: 406030 | gen_loss: 47.634 | mel_loss: 11.161 | 3.248 sec / 10 steps |\n",
      "| step: 406040 | gen_loss: 45.033 | mel_loss: 10.566 | 3.181 sec / 10 steps |\n",
      "| step: 406050 | gen_loss: 41.987 | mel_loss: 10.528 | 3.306 sec / 10 steps |\n",
      "| step: 406060 | gen_loss: 40.472 | mel_loss: 10.188 | 3.194 sec / 10 steps |\n",
      "| step: 406070 | gen_loss: 51.446 | mel_loss: 13.512 | 3.138 sec / 10 steps |\n",
      "| step: 406080 | gen_loss: 47.114 | mel_loss: 12.805 | 2.915 sec / 10 steps |\n",
      "| step: 406090 | gen_loss: 50.016 | mel_loss: 12.509 | 3.154 sec / 10 steps |\n",
      "| step: 406100 | gen_loss: 44.721 | mel_loss: 11.298 | 3.549 sec / 10 steps |\n",
      "Validation mel_loss: 14.433237075805664\n",
      "| step: 406110 | gen_loss: 42.731 | mel_loss: 10.798 | 3.279 sec / 10 steps |\n",
      "| step: 406120 | gen_loss: 41.794 | mel_loss: 10.482 | 3.522 sec / 10 steps |\n",
      "| step: 406130 | gen_loss: 48.838 | mel_loss: 12.899 | 3.439 sec / 10 steps |\n",
      "| step: 406140 | gen_loss: 47.076 | mel_loss: 11.939 | 3.919 sec / 10 steps |\n",
      "| step: 406150 | gen_loss: 44.372 | mel_loss: 10.498 | 3.093 sec / 10 steps |\n",
      "| step: 406160 | gen_loss: 51.521 | mel_loss: 13.585 | 3.235 sec / 10 steps |\n",
      "| step: 406170 | gen_loss: 49.772 | mel_loss: 12.877 | 3.114 sec / 10 steps |\n",
      "| step: 406180 | gen_loss: 46.586 | mel_loss: 12.249 | 3.133 sec / 10 steps |\n",
      "| step: 406190 | gen_loss: 43.703 | mel_loss: 10.501 | 3.089 sec / 10 steps |\n",
      "| step: 406200 | gen_loss: 36.428 | mel_loss: 9.714 | 3.064 sec / 10 steps |\n",
      "Validation mel_loss: 14.240884780883789\n",
      "| step: 406210 | gen_loss: 49.561 | mel_loss: 12.723 | 3.711 sec / 10 steps |\n",
      "| step: 406220 | gen_loss: 46.386 | mel_loss: 11.553 | 3.267 sec / 10 steps |\n",
      "| step: 406230 | gen_loss: 39.184 | mel_loss: 8.492 | 3.382 sec / 10 steps |\n",
      "| step: 406240 | gen_loss: 42.405 | mel_loss: 9.523 | 3.293 sec / 10 steps |\n",
      "| step: 406250 | gen_loss: 48.509 | mel_loss: 12.518 | 3.024 sec / 10 steps |\n",
      "|| Epoch: 541 ||\n",
      "| step: 406260 | gen_loss: 48.071 | mel_loss: 11.021 | 3.310 sec / 10 steps |\n",
      "| step: 406270 | gen_loss: 46.667 | mel_loss: 10.585 | 3.096 sec / 10 steps |\n",
      "| step: 406280 | gen_loss: 45.886 | mel_loss: 11.009 | 3.048 sec / 10 steps |\n",
      "| step: 406290 | gen_loss: 44.288 | mel_loss: 10.584 | 2.840 sec / 10 steps |\n",
      "| step: 406300 | gen_loss: 36.681 | mel_loss: 8.389 | 3.233 sec / 10 steps |\n",
      "Validation mel_loss: 14.374662399291992\n",
      "| step: 406310 | gen_loss: 41.230 | mel_loss: 9.613 | 3.132 sec / 10 steps |\n",
      "| step: 406320 | gen_loss: 47.865 | mel_loss: 12.635 | 3.114 sec / 10 steps |\n",
      "| step: 406330 | gen_loss: 41.359 | mel_loss: 9.698 | 3.363 sec / 10 steps |\n",
      "| step: 406340 | gen_loss: 47.732 | mel_loss: 11.390 | 3.188 sec / 10 steps |\n",
      "| step: 406350 | gen_loss: 53.157 | mel_loss: 12.895 | 3.103 sec / 10 steps |\n",
      "| step: 406360 | gen_loss: 44.781 | mel_loss: 10.589 | 3.294 sec / 10 steps |\n",
      "| step: 406370 | gen_loss: 47.523 | mel_loss: 10.525 | 3.039 sec / 10 steps |\n",
      "| step: 406380 | gen_loss: 45.458 | mel_loss: 10.486 | 3.491 sec / 10 steps |\n",
      "| step: 406390 | gen_loss: 49.508 | mel_loss: 11.516 | 3.683 sec / 10 steps |\n",
      "| step: 406400 | gen_loss: 46.357 | mel_loss: 11.238 | 3.404 sec / 10 steps |\n",
      "Validation mel_loss: 14.290701866149902\n",
      "| step: 406410 | gen_loss: 43.854 | mel_loss: 10.535 | 3.258 sec / 10 steps |\n",
      "| step: 406420 | gen_loss: 47.295 | mel_loss: 10.926 | 3.192 sec / 10 steps |\n",
      "| step: 406430 | gen_loss: 45.445 | mel_loss: 10.656 | 3.303 sec / 10 steps |\n",
      "| step: 406440 | gen_loss: 45.729 | mel_loss: 10.422 | 3.915 sec / 10 steps |\n",
      "| step: 406450 | gen_loss: 50.288 | mel_loss: 12.936 | 2.965 sec / 10 steps |\n",
      "| step: 406460 | gen_loss: 49.701 | mel_loss: 11.704 | 3.447 sec / 10 steps |\n",
      "| step: 406470 | gen_loss: 45.308 | mel_loss: 10.375 | 3.004 sec / 10 steps |\n",
      "| step: 406480 | gen_loss: 50.951 | mel_loss: 12.273 | 3.285 sec / 10 steps |\n",
      "| step: 406490 | gen_loss: 45.664 | mel_loss: 10.711 | 2.878 sec / 10 steps |\n",
      "| step: 406500 | gen_loss: 47.240 | mel_loss: 11.222 | 3.783 sec / 10 steps |\n",
      "Validation mel_loss: 14.281951904296875\n",
      "| step: 406510 | gen_loss: 48.451 | mel_loss: 12.073 | 2.969 sec / 10 steps |\n",
      "| step: 406520 | gen_loss: 43.011 | mel_loss: 9.853 | 3.049 sec / 10 steps |\n",
      "| step: 406530 | gen_loss: 43.249 | mel_loss: 9.624 | 3.401 sec / 10 steps |\n",
      "| step: 406540 | gen_loss: 43.049 | mel_loss: 9.080 | 3.598 sec / 10 steps |\n",
      "| step: 406550 | gen_loss: 44.114 | mel_loss: 10.954 | 3.237 sec / 10 steps |\n",
      "| step: 406560 | gen_loss: 49.738 | mel_loss: 11.761 | 3.130 sec / 10 steps |\n",
      "| step: 406570 | gen_loss: 48.212 | mel_loss: 11.371 | 2.868 sec / 10 steps |\n",
      "| step: 406580 | gen_loss: 46.071 | mel_loss: 11.423 | 2.941 sec / 10 steps |\n",
      "| step: 406590 | gen_loss: 43.302 | mel_loss: 9.810 | 3.744 sec / 10 steps |\n",
      "| step: 406600 | gen_loss: 53.728 | mel_loss: 13.071 | 3.400 sec / 10 steps |\n",
      "Validation mel_loss: 14.369145393371582\n",
      "| step: 406610 | gen_loss: 43.896 | mel_loss: 10.045 | 3.498 sec / 10 steps |\n",
      "| step: 406620 | gen_loss: 44.963 | mel_loss: 10.001 | 3.387 sec / 10 steps |\n",
      "| step: 406630 | gen_loss: 49.499 | mel_loss: 12.308 | 3.206 sec / 10 steps |\n",
      "| step: 406640 | gen_loss: 39.992 | mel_loss: 9.889 | 3.537 sec / 10 steps |\n",
      "| step: 406650 | gen_loss: 49.023 | mel_loss: 12.025 | 3.461 sec / 10 steps |\n",
      "| step: 406660 | gen_loss: 47.420 | mel_loss: 11.364 | 2.947 sec / 10 steps |\n",
      "| step: 406670 | gen_loss: 47.107 | mel_loss: 10.626 | 3.564 sec / 10 steps |\n",
      "| step: 406680 | gen_loss: 42.595 | mel_loss: 9.414 | 3.876 sec / 10 steps |\n",
      "| step: 406690 | gen_loss: 46.657 | mel_loss: 10.750 | 3.590 sec / 10 steps |\n",
      "| step: 406700 | gen_loss: 51.022 | mel_loss: 13.049 | 3.015 sec / 10 steps |\n",
      "Validation mel_loss: 14.440261840820312\n",
      "| step: 406710 | gen_loss: 46.399 | mel_loss: 10.092 | 3.417 sec / 10 steps |\n",
      "| step: 406720 | gen_loss: 45.921 | mel_loss: 11.718 | 3.237 sec / 10 steps |\n",
      "| step: 406730 | gen_loss: 47.885 | mel_loss: 10.907 | 3.462 sec / 10 steps |\n",
      "| step: 406740 | gen_loss: 52.120 | mel_loss: 12.397 | 3.285 sec / 10 steps |\n",
      "| step: 406750 | gen_loss: 42.641 | mel_loss: 10.251 | 3.012 sec / 10 steps |\n",
      "| step: 406760 | gen_loss: 54.109 | mel_loss: 13.453 | 3.063 sec / 10 steps |\n",
      "| step: 406770 | gen_loss: 44.186 | mel_loss: 10.425 | 3.217 sec / 10 steps |\n",
      "| step: 406780 | gen_loss: 50.631 | mel_loss: 13.010 | 2.973 sec / 10 steps |\n",
      "| step: 406790 | gen_loss: 47.765 | mel_loss: 10.856 | 3.311 sec / 10 steps |\n",
      "| step: 406800 | gen_loss: 45.402 | mel_loss: 11.398 | 2.845 sec / 10 steps |\n",
      "Validation mel_loss: 14.373004913330078\n",
      "| step: 406810 | gen_loss: 46.037 | mel_loss: 10.237 | 3.248 sec / 10 steps |\n",
      "| step: 406820 | gen_loss: 40.908 | mel_loss: 9.154 | 3.688 sec / 10 steps |\n",
      "| step: 406830 | gen_loss: 49.002 | mel_loss: 12.165 | 2.927 sec / 10 steps |\n",
      "| step: 406840 | gen_loss: 47.746 | mel_loss: 11.877 | 3.160 sec / 10 steps |\n",
      "| step: 406850 | gen_loss: 45.319 | mel_loss: 10.893 | 3.317 sec / 10 steps |\n",
      "| step: 406860 | gen_loss: 45.498 | mel_loss: 11.589 | 3.601 sec / 10 steps |\n",
      "| step: 406870 | gen_loss: 39.079 | mel_loss: 8.433 | 3.499 sec / 10 steps |\n",
      "| step: 406880 | gen_loss: 48.815 | mel_loss: 11.931 | 3.179 sec / 10 steps |\n",
      "| step: 406890 | gen_loss: 43.682 | mel_loss: 10.590 | 3.947 sec / 10 steps |\n",
      "| step: 406900 | gen_loss: 37.182 | mel_loss: 8.931 | 3.775 sec / 10 steps |\n",
      "Validation mel_loss: 14.308944702148438\n",
      "| step: 406910 | gen_loss: 46.377 | mel_loss: 10.880 | 3.045 sec / 10 steps |\n",
      "| step: 406920 | gen_loss: 46.403 | mel_loss: 10.984 | 3.185 sec / 10 steps |\n",
      "| step: 406930 | gen_loss: 45.780 | mel_loss: 10.590 | 3.177 sec / 10 steps |\n",
      "| step: 406940 | gen_loss: 44.946 | mel_loss: 10.383 | 3.630 sec / 10 steps |\n",
      "| step: 406950 | gen_loss: 46.505 | mel_loss: 10.979 | 3.359 sec / 10 steps |\n",
      "| step: 406960 | gen_loss: 48.318 | mel_loss: 12.394 | 3.321 sec / 10 steps |\n",
      "| step: 406970 | gen_loss: 45.411 | mel_loss: 11.656 | 3.516 sec / 10 steps |\n",
      "| step: 406980 | gen_loss: 47.973 | mel_loss: 13.028 | 3.376 sec / 10 steps |\n",
      "| step: 406990 | gen_loss: 47.857 | mel_loss: 11.221 | 3.296 sec / 10 steps |\n",
      "| step: 407000 | gen_loss: 46.245 | mel_loss: 10.976 | 3.567 sec / 10 steps |\n",
      "Validation mel_loss: 14.173022270202637\n",
      "|| Epoch: 542 ||\n",
      "| step: 407010 | gen_loss: 42.607 | mel_loss: 10.856 | 3.268 sec / 10 steps |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 407020 | gen_loss: 43.516 | mel_loss: 11.301 | 3.404 sec / 10 steps |\n",
      "| step: 407030 | gen_loss: 50.337 | mel_loss: 12.819 | 3.069 sec / 10 steps |\n",
      "| step: 407040 | gen_loss: 47.433 | mel_loss: 11.386 | 3.266 sec / 10 steps |\n",
      "| step: 407050 | gen_loss: 44.893 | mel_loss: 10.201 | 3.892 sec / 10 steps |\n",
      "| step: 407060 | gen_loss: 50.082 | mel_loss: 12.745 | 3.437 sec / 10 steps |\n",
      "| step: 407070 | gen_loss: 45.478 | mel_loss: 10.941 | 3.062 sec / 10 steps |\n",
      "| step: 407080 | gen_loss: 49.013 | mel_loss: 12.240 | 3.089 sec / 10 steps |\n",
      "| step: 407090 | gen_loss: 43.771 | mel_loss: 9.902 | 2.946 sec / 10 steps |\n",
      "| step: 407100 | gen_loss: 47.696 | mel_loss: 11.533 | 3.138 sec / 10 steps |\n",
      "Validation mel_loss: 14.343074798583984\n",
      "| step: 407110 | gen_loss: 47.102 | mel_loss: 11.540 | 3.117 sec / 10 steps |\n",
      "| step: 407120 | gen_loss: 49.359 | mel_loss: 12.614 | 2.917 sec / 10 steps |\n",
      "| step: 407130 | gen_loss: 46.737 | mel_loss: 11.360 | 3.307 sec / 10 steps |\n",
      "| step: 407140 | gen_loss: 51.839 | mel_loss: 12.590 | 3.463 sec / 10 steps |\n",
      "| step: 407150 | gen_loss: 50.760 | mel_loss: 12.148 | 3.006 sec / 10 steps |\n",
      "| step: 407160 | gen_loss: 46.784 | mel_loss: 12.068 | 3.334 sec / 10 steps |\n",
      "| step: 407170 | gen_loss: 55.519 | mel_loss: 12.985 | 3.346 sec / 10 steps |\n",
      "| step: 407180 | gen_loss: 48.101 | mel_loss: 11.478 | 3.005 sec / 10 steps |\n",
      "| step: 407190 | gen_loss: 46.015 | mel_loss: 10.396 | 3.396 sec / 10 steps |\n",
      "| step: 407200 | gen_loss: 38.080 | mel_loss: 9.021 | 3.500 sec / 10 steps |\n",
      "Validation mel_loss: 14.413694381713867\n",
      "| step: 407210 | gen_loss: 39.388 | mel_loss: 9.000 | 3.253 sec / 10 steps |\n",
      "| step: 407220 | gen_loss: 51.095 | mel_loss: 13.173 | 3.058 sec / 10 steps |\n",
      "| step: 407230 | gen_loss: 46.396 | mel_loss: 11.364 | 3.398 sec / 10 steps |\n",
      "| step: 407240 | gen_loss: 34.639 | mel_loss: 8.587 | 3.221 sec / 10 steps |\n",
      "| step: 407250 | gen_loss: 44.775 | mel_loss: 11.148 | 3.460 sec / 10 steps |\n",
      "| step: 407260 | gen_loss: 42.001 | mel_loss: 10.076 | 3.254 sec / 10 steps |\n",
      "| step: 407270 | gen_loss: 43.643 | mel_loss: 10.378 | 3.072 sec / 10 steps |\n",
      "| step: 407280 | gen_loss: 41.790 | mel_loss: 9.833 | 3.528 sec / 10 steps |\n",
      "| step: 407290 | gen_loss: 47.659 | mel_loss: 11.694 | 3.312 sec / 10 steps |\n",
      "| step: 407300 | gen_loss: 50.108 | mel_loss: 12.435 | 3.146 sec / 10 steps |\n",
      "Validation mel_loss: 14.285853385925293\n",
      "| step: 407310 | gen_loss: 49.966 | mel_loss: 12.795 | 3.338 sec / 10 steps |\n",
      "| step: 407320 | gen_loss: 40.782 | mel_loss: 9.316 | 3.424 sec / 10 steps |\n",
      "| step: 407330 | gen_loss: 47.256 | mel_loss: 12.516 | 3.174 sec / 10 steps |\n",
      "| step: 407340 | gen_loss: 46.419 | mel_loss: 11.066 | 3.366 sec / 10 steps |\n",
      "| step: 407350 | gen_loss: 49.102 | mel_loss: 10.772 | 3.329 sec / 10 steps |\n",
      "| step: 407360 | gen_loss: 44.353 | mel_loss: 9.927 | 3.082 sec / 10 steps |\n",
      "| step: 407370 | gen_loss: 48.276 | mel_loss: 11.658 | 2.947 sec / 10 steps |\n",
      "| step: 407380 | gen_loss: 48.720 | mel_loss: 12.912 | 3.325 sec / 10 steps |\n",
      "| step: 407390 | gen_loss: 51.781 | mel_loss: 12.892 | 3.349 sec / 10 steps |\n",
      "| step: 407400 | gen_loss: 46.599 | mel_loss: 11.034 | 3.105 sec / 10 steps |\n",
      "Validation mel_loss: 14.35239315032959\n",
      "| step: 407410 | gen_loss: 45.836 | mel_loss: 11.379 | 3.526 sec / 10 steps |\n",
      "| step: 407420 | gen_loss: 42.225 | mel_loss: 9.412 | 3.162 sec / 10 steps |\n",
      "| step: 407430 | gen_loss: 47.423 | mel_loss: 10.761 | 3.426 sec / 10 steps |\n",
      "| step: 407440 | gen_loss: 44.731 | mel_loss: 10.121 | 3.400 sec / 10 steps |\n",
      "| step: 407450 | gen_loss: 45.362 | mel_loss: 10.120 | 3.286 sec / 10 steps |\n",
      "| step: 407460 | gen_loss: 47.811 | mel_loss: 11.830 | 3.008 sec / 10 steps |\n",
      "| step: 407470 | gen_loss: 48.168 | mel_loss: 12.079 | 3.423 sec / 10 steps |\n",
      "| step: 407480 | gen_loss: 47.907 | mel_loss: 12.045 | 3.438 sec / 10 steps |\n",
      "| step: 407490 | gen_loss: 51.813 | mel_loss: 13.121 | 3.366 sec / 10 steps |\n",
      "| step: 407500 | gen_loss: 44.207 | mel_loss: 9.755 | 3.223 sec / 10 steps |\n",
      "Validation mel_loss: 14.314627647399902\n",
      "| step: 407510 | gen_loss: 43.805 | mel_loss: 10.904 | 3.788 sec / 10 steps |\n",
      "| step: 407520 | gen_loss: 49.638 | mel_loss: 12.718 | 3.310 sec / 10 steps |\n",
      "| step: 407530 | gen_loss: 40.242 | mel_loss: 9.511 | 3.541 sec / 10 steps |\n",
      "| step: 407540 | gen_loss: 45.490 | mel_loss: 10.887 | 3.306 sec / 10 steps |\n",
      "| step: 407550 | gen_loss: 49.713 | mel_loss: 11.721 | 3.221 sec / 10 steps |\n",
      "| step: 407560 | gen_loss: 45.423 | mel_loss: 10.750 | 3.179 sec / 10 steps |\n",
      "| step: 407570 | gen_loss: 45.614 | mel_loss: 10.799 | 3.319 sec / 10 steps |\n",
      "| step: 407580 | gen_loss: 46.551 | mel_loss: 11.249 | 3.401 sec / 10 steps |\n",
      "| step: 407590 | gen_loss: 44.238 | mel_loss: 10.400 | 3.021 sec / 10 steps |\n",
      "| step: 407600 | gen_loss: 48.682 | mel_loss: 12.214 | 3.033 sec / 10 steps |\n",
      "Validation mel_loss: 14.32258129119873\n",
      "| step: 407610 | gen_loss: 48.754 | mel_loss: 12.390 | 3.395 sec / 10 steps |\n",
      "| step: 407620 | gen_loss: 48.200 | mel_loss: 11.882 | 3.485 sec / 10 steps |\n",
      "| step: 407630 | gen_loss: 46.009 | mel_loss: 11.491 | 3.191 sec / 10 steps |\n",
      "| step: 407640 | gen_loss: 45.697 | mel_loss: 10.993 | 3.358 sec / 10 steps |\n",
      "| step: 407650 | gen_loss: 48.149 | mel_loss: 12.024 | 2.974 sec / 10 steps |\n",
      "| step: 407660 | gen_loss: 45.243 | mel_loss: 12.062 | 3.231 sec / 10 steps |\n",
      "| step: 407670 | gen_loss: 44.555 | mel_loss: 10.445 | 3.497 sec / 10 steps |\n",
      "| step: 407680 | gen_loss: 46.225 | mel_loss: 11.208 | 3.153 sec / 10 steps |\n",
      "| step: 407690 | gen_loss: 37.559 | mel_loss: 9.088 | 3.645 sec / 10 steps |\n",
      "| step: 407700 | gen_loss: 48.820 | mel_loss: 12.558 | 2.980 sec / 10 steps |\n",
      "Validation mel_loss: 14.32004451751709\n",
      "| step: 407710 | gen_loss: 42.647 | mel_loss: 9.792 | 3.484 sec / 10 steps |\n",
      "| step: 407720 | gen_loss: 49.885 | mel_loss: 12.547 | 3.165 sec / 10 steps |\n",
      "| step: 407730 | gen_loss: 44.760 | mel_loss: 11.378 | 3.180 sec / 10 steps |\n",
      "| step: 407740 | gen_loss: 48.236 | mel_loss: 12.072 | 2.931 sec / 10 steps |\n",
      "| step: 407750 | gen_loss: 42.844 | mel_loss: 11.448 | 3.423 sec / 10 steps |\n",
      "|| Epoch: 543 ||\n",
      "| step: 407760 | gen_loss: 45.671 | mel_loss: 11.812 | 3.511 sec / 10 steps |\n",
      "| step: 407770 | gen_loss: 50.040 | mel_loss: 12.962 | 3.069 sec / 10 steps |\n",
      "| step: 407780 | gen_loss: 51.863 | mel_loss: 13.411 | 3.326 sec / 10 steps |\n",
      "| step: 407790 | gen_loss: 44.379 | mel_loss: 10.249 | 2.976 sec / 10 steps |\n",
      "| step: 407800 | gen_loss: 52.251 | mel_loss: 13.321 | 3.382 sec / 10 steps |\n",
      "Validation mel_loss: 14.231029510498047\n",
      "| step: 407810 | gen_loss: 41.384 | mel_loss: 10.780 | 3.384 sec / 10 steps |\n",
      "| step: 407820 | gen_loss: 42.034 | mel_loss: 10.338 | 3.668 sec / 10 steps |\n",
      "| step: 407830 | gen_loss: 44.168 | mel_loss: 10.759 | 3.162 sec / 10 steps |\n",
      "| step: 407840 | gen_loss: 44.622 | mel_loss: 10.264 | 3.266 sec / 10 steps |\n",
      "| step: 407850 | gen_loss: 42.419 | mel_loss: 10.573 | 3.152 sec / 10 steps |\n",
      "| step: 407860 | gen_loss: 44.748 | mel_loss: 11.383 | 3.307 sec / 10 steps |\n",
      "| step: 407870 | gen_loss: 51.663 | mel_loss: 12.367 | 3.162 sec / 10 steps |\n",
      "| step: 407880 | gen_loss: 40.690 | mel_loss: 10.376 | 3.161 sec / 10 steps |\n",
      "| step: 407890 | gen_loss: 46.902 | mel_loss: 12.026 | 3.821 sec / 10 steps |\n",
      "| step: 407900 | gen_loss: 45.628 | mel_loss: 11.334 | 3.301 sec / 10 steps |\n",
      "Validation mel_loss: 14.271944046020508\n",
      "| step: 407910 | gen_loss: 50.330 | mel_loss: 12.323 | 3.231 sec / 10 steps |\n",
      "| step: 407920 | gen_loss: 40.036 | mel_loss: 8.900 | 3.597 sec / 10 steps |\n",
      "| step: 407930 | gen_loss: 47.305 | mel_loss: 11.902 | 3.261 sec / 10 steps |\n",
      "| step: 407940 | gen_loss: 51.355 | mel_loss: 13.130 | 3.110 sec / 10 steps |\n",
      "| step: 407950 | gen_loss: 48.822 | mel_loss: 12.538 | 3.212 sec / 10 steps |\n",
      "| step: 407960 | gen_loss: 42.058 | mel_loss: 9.643 | 3.157 sec / 10 steps |\n",
      "| step: 407970 | gen_loss: 46.771 | mel_loss: 11.357 | 3.460 sec / 10 steps |\n",
      "| step: 407980 | gen_loss: 47.972 | mel_loss: 12.101 | 3.374 sec / 10 steps |\n",
      "| step: 407990 | gen_loss: 46.267 | mel_loss: 10.097 | 3.127 sec / 10 steps |\n",
      "| step: 408000 | gen_loss: 46.285 | mel_loss: 11.659 | 2.954 sec / 10 steps |\n",
      "Validation mel_loss: 14.196467399597168\n",
      "| step: 408010 | gen_loss: 46.266 | mel_loss: 11.439 | 3.195 sec / 10 steps |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 408020 | gen_loss: 40.224 | mel_loss: 10.516 | 3.252 sec / 10 steps |\n",
      "| step: 408030 | gen_loss: 42.275 | mel_loss: 11.052 | 3.202 sec / 10 steps |\n",
      "| step: 408040 | gen_loss: 41.338 | mel_loss: 10.434 | 3.308 sec / 10 steps |\n",
      "| step: 408050 | gen_loss: 41.403 | mel_loss: 8.812 | 3.639 sec / 10 steps |\n",
      "| step: 408060 | gen_loss: 49.432 | mel_loss: 12.413 | 3.061 sec / 10 steps |\n",
      "| step: 408070 | gen_loss: 46.800 | mel_loss: 11.273 | 3.220 sec / 10 steps |\n",
      "| step: 408080 | gen_loss: 43.317 | mel_loss: 9.971 | 3.735 sec / 10 steps |\n",
      "| step: 408090 | gen_loss: 46.944 | mel_loss: 11.074 | 2.996 sec / 10 steps |\n",
      "| step: 408100 | gen_loss: 48.103 | mel_loss: 11.065 | 3.160 sec / 10 steps |\n",
      "Validation mel_loss: 14.254281044006348\n",
      "| step: 408110 | gen_loss: 41.042 | mel_loss: 10.349 | 3.519 sec / 10 steps |\n",
      "| step: 408120 | gen_loss: 38.676 | mel_loss: 9.416 | 3.262 sec / 10 steps |\n",
      "| step: 408130 | gen_loss: 43.854 | mel_loss: 10.281 | 3.141 sec / 10 steps |\n",
      "| step: 408140 | gen_loss: 40.187 | mel_loss: 9.664 | 3.176 sec / 10 steps |\n",
      "| step: 408150 | gen_loss: 40.638 | mel_loss: 10.014 | 3.232 sec / 10 steps |\n",
      "| step: 408160 | gen_loss: 46.406 | mel_loss: 12.389 | 3.234 sec / 10 steps |\n",
      "| step: 408170 | gen_loss: 39.488 | mel_loss: 8.955 | 3.553 sec / 10 steps |\n",
      "| step: 408180 | gen_loss: 45.315 | mel_loss: 10.667 | 3.466 sec / 10 steps |\n",
      "| step: 408190 | gen_loss: 48.299 | mel_loss: 11.805 | 3.275 sec / 10 steps |\n",
      "| step: 408200 | gen_loss: 47.097 | mel_loss: 12.201 | 3.281 sec / 10 steps |\n",
      "Validation mel_loss: 14.387937545776367\n",
      "| step: 408210 | gen_loss: 43.348 | mel_loss: 9.813 | 3.930 sec / 10 steps |\n",
      "| step: 408220 | gen_loss: 42.858 | mel_loss: 11.117 | 3.092 sec / 10 steps |\n",
      "| step: 408230 | gen_loss: 45.867 | mel_loss: 11.896 | 3.797 sec / 10 steps |\n",
      "| step: 408240 | gen_loss: 47.099 | mel_loss: 12.635 | 3.021 sec / 10 steps |\n",
      "| step: 408250 | gen_loss: 46.952 | mel_loss: 11.928 | 3.024 sec / 10 steps |\n",
      "| step: 408260 | gen_loss: 41.131 | mel_loss: 10.496 | 3.013 sec / 10 steps |\n",
      "| step: 408270 | gen_loss: 36.752 | mel_loss: 9.602 | 3.481 sec / 10 steps |\n",
      "| step: 408280 | gen_loss: 41.305 | mel_loss: 10.452 | 3.011 sec / 10 steps |\n",
      "| step: 408290 | gen_loss: 51.275 | mel_loss: 12.922 | 3.106 sec / 10 steps |\n",
      "| step: 408300 | gen_loss: 43.066 | mel_loss: 10.126 | 3.131 sec / 10 steps |\n",
      "Validation mel_loss: 14.277924537658691\n",
      "| step: 408310 | gen_loss: 49.503 | mel_loss: 12.650 | 3.466 sec / 10 steps |\n",
      "| step: 408320 | gen_loss: 45.239 | mel_loss: 10.802 | 3.641 sec / 10 steps |\n",
      "| step: 408330 | gen_loss: 47.932 | mel_loss: 11.036 | 3.288 sec / 10 steps |\n",
      "| step: 408340 | gen_loss: 52.791 | mel_loss: 13.307 | 2.754 sec / 10 steps |\n",
      "| step: 408350 | gen_loss: 45.632 | mel_loss: 11.294 | 3.132 sec / 10 steps |\n",
      "| step: 408360 | gen_loss: 45.363 | mel_loss: 10.770 | 3.489 sec / 10 steps |\n",
      "| step: 408370 | gen_loss: 49.203 | mel_loss: 12.217 | 3.171 sec / 10 steps |\n",
      "| step: 408380 | gen_loss: 48.446 | mel_loss: 12.105 | 3.419 sec / 10 steps |\n",
      "| step: 408390 | gen_loss: 49.517 | mel_loss: 12.890 | 3.707 sec / 10 steps |\n",
      "| step: 408400 | gen_loss: 36.813 | mel_loss: 8.409 | 3.452 sec / 10 steps |\n",
      "Validation mel_loss: 14.241072654724121\n",
      "| step: 408410 | gen_loss: 42.600 | mel_loss: 10.008 | 3.259 sec / 10 steps |\n",
      "| step: 408420 | gen_loss: 40.796 | mel_loss: 10.902 | 2.897 sec / 10 steps |\n",
      "| step: 408430 | gen_loss: 41.444 | mel_loss: 10.361 | 3.494 sec / 10 steps |\n",
      "| step: 408440 | gen_loss: 49.539 | mel_loss: 12.616 | 3.260 sec / 10 steps |\n",
      "| step: 408450 | gen_loss: 44.831 | mel_loss: 11.594 | 3.115 sec / 10 steps |\n",
      "| step: 408460 | gen_loss: 54.131 | mel_loss: 13.988 | 3.172 sec / 10 steps |\n",
      "| step: 408470 | gen_loss: 41.351 | mel_loss: 9.481 | 3.708 sec / 10 steps |\n",
      "| step: 408480 | gen_loss: 43.974 | mel_loss: 10.064 | 3.488 sec / 10 steps |\n",
      "| step: 408490 | gen_loss: 42.898 | mel_loss: 9.446 | 3.683 sec / 10 steps |\n",
      "| step: 408500 | gen_loss: 46.231 | mel_loss: 11.516 | 3.253 sec / 10 steps |\n",
      "Validation mel_loss: 14.30461311340332\n",
      "|| Epoch: 544 ||\n",
      "| step: 408510 | gen_loss: 45.853 | mel_loss: 11.315 | 3.286 sec / 10 steps |\n",
      "| step: 408520 | gen_loss: 40.309 | mel_loss: 9.245 | 3.497 sec / 10 steps |\n",
      "| step: 408530 | gen_loss: 40.898 | mel_loss: 8.962 | 3.343 sec / 10 steps |\n",
      "| step: 408540 | gen_loss: 43.101 | mel_loss: 9.960 | 3.197 sec / 10 steps |\n",
      "| step: 408550 | gen_loss: 43.020 | mel_loss: 8.934 | 3.566 sec / 10 steps |\n",
      "| step: 408560 | gen_loss: 48.120 | mel_loss: 11.731 | 3.226 sec / 10 steps |\n",
      "| step: 408570 | gen_loss: 51.898 | mel_loss: 13.652 | 3.075 sec / 10 steps |\n",
      "| step: 408580 | gen_loss: 43.442 | mel_loss: 9.862 | 3.266 sec / 10 steps |\n",
      "| step: 408590 | gen_loss: 48.137 | mel_loss: 11.977 | 3.592 sec / 10 steps |\n",
      "| step: 408600 | gen_loss: 49.403 | mel_loss: 12.420 | 3.018 sec / 10 steps |\n",
      "Validation mel_loss: 14.300089836120605\n",
      "| step: 408610 | gen_loss: 47.906 | mel_loss: 11.547 | 3.569 sec / 10 steps |\n",
      "| step: 408620 | gen_loss: 46.625 | mel_loss: 10.500 | 3.017 sec / 10 steps |\n",
      "| step: 408630 | gen_loss: 46.314 | mel_loss: 11.760 | 3.284 sec / 10 steps |\n",
      "| step: 408640 | gen_loss: 46.207 | mel_loss: 11.075 | 3.265 sec / 10 steps |\n",
      "| step: 408650 | gen_loss: 45.570 | mel_loss: 11.374 | 3.153 sec / 10 steps |\n",
      "| step: 408660 | gen_loss: 49.451 | mel_loss: 12.519 | 3.102 sec / 10 steps |\n",
      "| step: 408670 | gen_loss: 47.123 | mel_loss: 11.167 | 2.965 sec / 10 steps |\n",
      "| step: 408680 | gen_loss: 54.814 | mel_loss: 13.124 | 3.008 sec / 10 steps |\n",
      "| step: 408690 | gen_loss: 37.209 | mel_loss: 8.535 | 3.350 sec / 10 steps |\n",
      "| step: 408700 | gen_loss: 47.473 | mel_loss: 11.831 | 3.457 sec / 10 steps |\n",
      "Validation mel_loss: 14.56662654876709\n",
      "| step: 408710 | gen_loss: 40.097 | mel_loss: 10.102 | 3.179 sec / 10 steps |\n",
      "| step: 408720 | gen_loss: 43.496 | mel_loss: 10.718 | 3.252 sec / 10 steps |\n",
      "| step: 408730 | gen_loss: 44.044 | mel_loss: 11.476 | 3.368 sec / 10 steps |\n",
      "| step: 408740 | gen_loss: 40.445 | mel_loss: 10.360 | 3.182 sec / 10 steps |\n",
      "| step: 408750 | gen_loss: 50.448 | mel_loss: 12.909 | 3.056 sec / 10 steps |\n",
      "| step: 408760 | gen_loss: 47.776 | mel_loss: 12.165 | 3.247 sec / 10 steps |\n",
      "| step: 408770 | gen_loss: 47.503 | mel_loss: 11.507 | 3.653 sec / 10 steps |\n",
      "| step: 408780 | gen_loss: 48.851 | mel_loss: 11.236 | 3.570 sec / 10 steps |\n",
      "| step: 408790 | gen_loss: 43.286 | mel_loss: 10.956 | 3.717 sec / 10 steps |\n",
      "| step: 408800 | gen_loss: 46.460 | mel_loss: 11.652 | 3.278 sec / 10 steps |\n",
      "Validation mel_loss: 14.21996784210205\n",
      "| step: 408810 | gen_loss: 39.882 | mel_loss: 10.355 | 3.544 sec / 10 steps |\n",
      "| step: 408820 | gen_loss: 51.461 | mel_loss: 12.876 | 2.935 sec / 10 steps |\n",
      "| step: 408830 | gen_loss: 45.852 | mel_loss: 11.613 | 3.532 sec / 10 steps |\n",
      "| step: 408840 | gen_loss: 38.464 | mel_loss: 11.027 | 3.367 sec / 10 steps |\n",
      "| step: 408850 | gen_loss: 45.118 | mel_loss: 11.241 | 3.509 sec / 10 steps |\n",
      "| step: 408860 | gen_loss: 48.536 | mel_loss: 12.075 | 3.116 sec / 10 steps |\n",
      "| step: 408870 | gen_loss: 40.340 | mel_loss: 8.617 | 3.667 sec / 10 steps |\n",
      "| step: 408880 | gen_loss: 49.396 | mel_loss: 12.215 | 3.119 sec / 10 steps |\n",
      "| step: 408890 | gen_loss: 42.450 | mel_loss: 9.543 | 3.163 sec / 10 steps |\n",
      "| step: 408900 | gen_loss: 50.216 | mel_loss: 12.488 | 3.242 sec / 10 steps |\n",
      "Validation mel_loss: 14.215482711791992\n",
      "| step: 408910 | gen_loss: 47.595 | mel_loss: 11.335 | 3.593 sec / 10 steps |\n",
      "| step: 408920 | gen_loss: 40.161 | mel_loss: 10.408 | 3.479 sec / 10 steps |\n",
      "| step: 408930 | gen_loss: 48.930 | mel_loss: 13.030 | 3.566 sec / 10 steps |\n",
      "| step: 408940 | gen_loss: 43.439 | mel_loss: 10.477 | 3.268 sec / 10 steps |\n",
      "| step: 408950 | gen_loss: 45.659 | mel_loss: 10.948 | 3.153 sec / 10 steps |\n",
      "| step: 408960 | gen_loss: 44.000 | mel_loss: 10.155 | 3.253 sec / 10 steps |\n",
      "| step: 408970 | gen_loss: 44.746 | mel_loss: 11.707 | 3.380 sec / 10 steps |\n",
      "| step: 408980 | gen_loss: 40.240 | mel_loss: 8.999 | 3.553 sec / 10 steps |\n",
      "| step: 408990 | gen_loss: 51.897 | mel_loss: 13.243 | 3.420 sec / 10 steps |\n",
      "| step: 409000 | gen_loss: 38.499 | mel_loss: 8.687 | 3.209 sec / 10 steps |\n",
      "Validation mel_loss: 14.466541290283203\n",
      "| step: 409010 | gen_loss: 42.504 | mel_loss: 10.285 | 3.349 sec / 10 steps |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 409020 | gen_loss: 52.584 | mel_loss: 12.758 | 2.910 sec / 10 steps |\n",
      "| step: 409030 | gen_loss: 49.280 | mel_loss: 12.442 | 3.228 sec / 10 steps |\n",
      "| step: 409040 | gen_loss: 49.098 | mel_loss: 12.922 | 3.419 sec / 10 steps |\n",
      "| step: 409050 | gen_loss: 41.124 | mel_loss: 10.181 | 3.485 sec / 10 steps |\n",
      "| step: 409060 | gen_loss: 52.678 | mel_loss: 13.709 | 3.144 sec / 10 steps |\n",
      "| step: 409070 | gen_loss: 47.987 | mel_loss: 12.174 | 2.987 sec / 10 steps |\n",
      "| step: 409080 | gen_loss: 47.111 | mel_loss: 10.470 | 3.408 sec / 10 steps |\n",
      "| step: 409090 | gen_loss: 41.401 | mel_loss: 9.503 | 3.200 sec / 10 steps |\n",
      "| step: 409100 | gen_loss: 49.177 | mel_loss: 11.952 | 3.548 sec / 10 steps |\n",
      "Validation mel_loss: 14.14561939239502\n",
      "| step: 409110 | gen_loss: 44.313 | mel_loss: 10.597 | 3.469 sec / 10 steps |\n",
      "| step: 409120 | gen_loss: 52.692 | mel_loss: 13.108 | 3.263 sec / 10 steps |\n",
      "| step: 409130 | gen_loss: 52.704 | mel_loss: 12.515 | 3.348 sec / 10 steps |\n",
      "| step: 409140 | gen_loss: 48.654 | mel_loss: 11.926 | 3.126 sec / 10 steps |\n",
      "| step: 409150 | gen_loss: 53.687 | mel_loss: 12.993 | 3.119 sec / 10 steps |\n",
      "| step: 409160 | gen_loss: 46.569 | mel_loss: 11.281 | 3.114 sec / 10 steps |\n",
      "| step: 409170 | gen_loss: 46.879 | mel_loss: 11.079 | 3.200 sec / 10 steps |\n",
      "| step: 409180 | gen_loss: 43.056 | mel_loss: 9.289 | 3.518 sec / 10 steps |\n",
      "| step: 409190 | gen_loss: 47.739 | mel_loss: 10.776 | 3.247 sec / 10 steps |\n",
      "| step: 409200 | gen_loss: 48.610 | mel_loss: 12.272 | 3.004 sec / 10 steps |\n",
      "Validation mel_loss: 14.262773513793945\n",
      "| step: 409210 | gen_loss: 44.553 | mel_loss: 9.950 | 3.271 sec / 10 steps |\n",
      "| step: 409220 | gen_loss: 46.571 | mel_loss: 11.131 | 3.342 sec / 10 steps |\n",
      "| step: 409230 | gen_loss: 47.010 | mel_loss: 10.970 | 3.660 sec / 10 steps |\n",
      "| step: 409240 | gen_loss: 50.193 | mel_loss: 12.343 | 3.225 sec / 10 steps |\n",
      "| step: 409250 | gen_loss: 40.088 | mel_loss: 8.631 | 3.721 sec / 10 steps |\n",
      "|| Epoch: 545 ||\n",
      "| step: 409260 | gen_loss: 45.948 | mel_loss: 11.106 | 3.074 sec / 10 steps |\n",
      "| step: 409270 | gen_loss: 45.569 | mel_loss: 11.509 | 3.290 sec / 10 steps |\n",
      "| step: 409280 | gen_loss: 43.422 | mel_loss: 9.708 | 3.818 sec / 10 steps |\n",
      "| step: 409290 | gen_loss: 43.186 | mel_loss: 10.389 | 3.049 sec / 10 steps |\n",
      "| step: 409300 | gen_loss: 42.989 | mel_loss: 9.568 | 3.480 sec / 10 steps |\n",
      "Validation mel_loss: 14.184293746948242\n",
      "| step: 409310 | gen_loss: 45.628 | mel_loss: 10.695 | 3.444 sec / 10 steps |\n",
      "| step: 409320 | gen_loss: 44.290 | mel_loss: 10.961 | 3.395 sec / 10 steps |\n",
      "| step: 409330 | gen_loss: 45.612 | mel_loss: 11.001 | 3.373 sec / 10 steps |\n",
      "| step: 409340 | gen_loss: 46.961 | mel_loss: 11.818 | 3.080 sec / 10 steps |\n",
      "| step: 409350 | gen_loss: 46.477 | mel_loss: 12.565 | 3.208 sec / 10 steps |\n",
      "| step: 409360 | gen_loss: 38.817 | mel_loss: 8.410 | 3.024 sec / 10 steps |\n",
      "| step: 409370 | gen_loss: 46.953 | mel_loss: 11.273 | 3.619 sec / 10 steps |\n",
      "| step: 409380 | gen_loss: 47.604 | mel_loss: 12.396 | 3.629 sec / 10 steps |\n",
      "| step: 409390 | gen_loss: 44.440 | mel_loss: 10.399 | 3.376 sec / 10 steps |\n",
      "| step: 409400 | gen_loss: 49.450 | mel_loss: 12.193 | 3.404 sec / 10 steps |\n",
      "Validation mel_loss: 14.101350784301758\n",
      "| step: 409410 | gen_loss: 46.653 | mel_loss: 10.383 | 3.290 sec / 10 steps |\n",
      "| step: 409420 | gen_loss: 41.930 | mel_loss: 9.529 | 3.216 sec / 10 steps |\n",
      "| step: 409430 | gen_loss: 45.558 | mel_loss: 11.315 | 3.123 sec / 10 steps |\n",
      "| step: 409440 | gen_loss: 47.392 | mel_loss: 11.634 | 3.202 sec / 10 steps |\n",
      "| step: 409450 | gen_loss: 48.501 | mel_loss: 12.898 | 2.973 sec / 10 steps |\n",
      "| step: 409460 | gen_loss: 47.022 | mel_loss: 11.274 | 3.408 sec / 10 steps |\n",
      "| step: 409470 | gen_loss: 37.839 | mel_loss: 9.897 | 3.440 sec / 10 steps |\n",
      "| step: 409480 | gen_loss: 48.697 | mel_loss: 11.868 | 3.325 sec / 10 steps |\n",
      "| step: 409490 | gen_loss: 43.170 | mel_loss: 9.601 | 3.447 sec / 10 steps |\n",
      "| step: 409500 | gen_loss: 43.729 | mel_loss: 10.748 | 3.504 sec / 10 steps |\n",
      "Validation mel_loss: 14.22701358795166\n",
      "| step: 409510 | gen_loss: 49.693 | mel_loss: 12.582 | 3.305 sec / 10 steps |\n",
      "| step: 409520 | gen_loss: 49.493 | mel_loss: 12.606 | 3.515 sec / 10 steps |\n",
      "| step: 409530 | gen_loss: 50.190 | mel_loss: 13.174 | 3.193 sec / 10 steps |\n",
      "| step: 409540 | gen_loss: 46.173 | mel_loss: 11.404 | 3.096 sec / 10 steps |\n",
      "| step: 409550 | gen_loss: 39.158 | mel_loss: 9.759 | 3.044 sec / 10 steps |\n",
      "| step: 409560 | gen_loss: 49.030 | mel_loss: 12.235 | 3.140 sec / 10 steps |\n",
      "| step: 409570 | gen_loss: 50.385 | mel_loss: 12.160 | 3.384 sec / 10 steps |\n",
      "| step: 409580 | gen_loss: 46.699 | mel_loss: 12.455 | 3.494 sec / 10 steps |\n",
      "| step: 409590 | gen_loss: 43.819 | mel_loss: 10.439 | 3.383 sec / 10 steps |\n",
      "| step: 409600 | gen_loss: 46.529 | mel_loss: 12.055 | 3.018 sec / 10 steps |\n",
      "Validation mel_loss: 14.272631645202637\n",
      "| step: 409610 | gen_loss: 48.004 | mel_loss: 11.509 | 3.337 sec / 10 steps |\n",
      "| step: 409620 | gen_loss: 46.055 | mel_loss: 10.419 | 3.509 sec / 10 steps |\n",
      "| step: 409630 | gen_loss: 51.050 | mel_loss: 12.285 | 3.079 sec / 10 steps |\n",
      "| step: 409640 | gen_loss: 44.266 | mel_loss: 10.559 | 3.555 sec / 10 steps |\n",
      "| step: 409650 | gen_loss: 45.932 | mel_loss: 11.302 | 3.443 sec / 10 steps |\n",
      "| step: 409660 | gen_loss: 48.491 | mel_loss: 12.388 | 3.083 sec / 10 steps |\n",
      "| step: 409670 | gen_loss: 44.738 | mel_loss: 9.781 | 3.454 sec / 10 steps |\n",
      "| step: 409680 | gen_loss: 48.685 | mel_loss: 12.195 | 3.088 sec / 10 steps |\n",
      "| step: 409690 | gen_loss: 42.906 | mel_loss: 10.099 | 3.164 sec / 10 steps |\n",
      "| step: 409700 | gen_loss: 45.684 | mel_loss: 10.981 | 3.311 sec / 10 steps |\n",
      "Validation mel_loss: 14.266167640686035\n",
      "| step: 409710 | gen_loss: 50.482 | mel_loss: 13.282 | 3.262 sec / 10 steps |\n",
      "| step: 409720 | gen_loss: 52.873 | mel_loss: 13.244 | 3.426 sec / 10 steps |\n",
      "| step: 409730 | gen_loss: 46.215 | mel_loss: 10.954 | 3.359 sec / 10 steps |\n",
      "| step: 409740 | gen_loss: 50.878 | mel_loss: 12.706 | 3.423 sec / 10 steps |\n",
      "| step: 409750 | gen_loss: 45.334 | mel_loss: 11.354 | 3.829 sec / 10 steps |\n",
      "| step: 409760 | gen_loss: 47.268 | mel_loss: 11.011 | 3.196 sec / 10 steps |\n",
      "| step: 409770 | gen_loss: 44.309 | mel_loss: 10.143 | 3.375 sec / 10 steps |\n",
      "| step: 409780 | gen_loss: 47.658 | mel_loss: 11.398 | 3.205 sec / 10 steps |\n",
      "| step: 409790 | gen_loss: 53.251 | mel_loss: 13.192 | 3.027 sec / 10 steps |\n",
      "| step: 409800 | gen_loss: 53.734 | mel_loss: 13.113 | 3.102 sec / 10 steps |\n",
      "Validation mel_loss: 14.482023239135742\n",
      "| step: 409810 | gen_loss: 50.697 | mel_loss: 12.505 | 3.406 sec / 10 steps |\n",
      "| step: 409820 | gen_loss: 49.016 | mel_loss: 11.794 | 3.617 sec / 10 steps |\n",
      "| step: 409830 | gen_loss: 51.234 | mel_loss: 11.871 | 3.096 sec / 10 steps |\n",
      "| step: 409840 | gen_loss: 45.166 | mel_loss: 10.151 | 3.596 sec / 10 steps |\n",
      "| step: 409850 | gen_loss: 51.478 | mel_loss: 12.222 | 3.398 sec / 10 steps |\n",
      "| step: 409860 | gen_loss: 48.027 | mel_loss: 12.200 | 3.222 sec / 10 steps |\n",
      "| step: 409870 | gen_loss: 48.952 | mel_loss: 12.140 | 3.223 sec / 10 steps |\n",
      "| step: 409880 | gen_loss: 46.547 | mel_loss: 11.309 | 3.352 sec / 10 steps |\n",
      "| step: 409890 | gen_loss: 48.169 | mel_loss: 12.034 | 3.256 sec / 10 steps |\n",
      "| step: 409900 | gen_loss: 47.859 | mel_loss: 12.715 | 3.164 sec / 10 steps |\n",
      "Validation mel_loss: 14.199646949768066\n",
      "| step: 409910 | gen_loss: 47.303 | mel_loss: 11.658 | 3.184 sec / 10 steps |\n",
      "| step: 409920 | gen_loss: 47.050 | mel_loss: 11.546 | 3.386 sec / 10 steps |\n",
      "| step: 409930 | gen_loss: 49.789 | mel_loss: 12.861 | 3.049 sec / 10 steps |\n",
      "| step: 409940 | gen_loss: 43.256 | mel_loss: 10.986 | 3.820 sec / 10 steps |\n",
      "| step: 409950 | gen_loss: 48.222 | mel_loss: 11.920 | 3.006 sec / 10 steps |\n",
      "| step: 409960 | gen_loss: 42.136 | mel_loss: 9.818 | 3.746 sec / 10 steps |\n",
      "| step: 409970 | gen_loss: 46.659 | mel_loss: 10.431 | 3.434 sec / 10 steps |\n",
      "| step: 409980 | gen_loss: 49.450 | mel_loss: 12.401 | 3.192 sec / 10 steps |\n",
      "| step: 409990 | gen_loss: 47.544 | mel_loss: 11.690 | 3.007 sec / 10 steps |\n",
      "| step: 410000 | gen_loss: 46.183 | mel_loss: 12.297 | 3.485 sec / 10 steps |\n",
      "Validation mel_loss: 14.37835693359375\n",
      "|| Epoch: 546 ||\n",
      "| step: 410010 | gen_loss: 47.945 | mel_loss: 11.525 | 3.560 sec / 10 steps |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 410020 | gen_loss: 45.502 | mel_loss: 9.308 | 3.046 sec / 10 steps |\n",
      "| step: 410030 | gen_loss: 48.846 | mel_loss: 12.298 | 3.402 sec / 10 steps |\n",
      "| step: 410040 | gen_loss: 48.354 | mel_loss: 11.681 | 3.258 sec / 10 steps |\n",
      "| step: 410050 | gen_loss: 45.816 | mel_loss: 10.915 | 3.242 sec / 10 steps |\n",
      "| step: 410060 | gen_loss: 46.407 | mel_loss: 10.830 | 3.952 sec / 10 steps |\n",
      "| step: 410070 | gen_loss: 46.874 | mel_loss: 11.994 | 3.116 sec / 10 steps |\n",
      "| step: 410080 | gen_loss: 45.367 | mel_loss: 11.249 | 3.778 sec / 10 steps |\n",
      "| step: 410090 | gen_loss: 44.388 | mel_loss: 9.688 | 3.542 sec / 10 steps |\n",
      "| step: 410100 | gen_loss: 49.678 | mel_loss: 12.800 | 3.485 sec / 10 steps |\n",
      "Validation mel_loss: 14.14156723022461\n",
      "| step: 410110 | gen_loss: 47.649 | mel_loss: 11.356 | 3.493 sec / 10 steps |\n",
      "| step: 410120 | gen_loss: 44.731 | mel_loss: 11.838 | 3.012 sec / 10 steps |\n",
      "| step: 410130 | gen_loss: 47.895 | mel_loss: 10.960 | 3.355 sec / 10 steps |\n",
      "| step: 410140 | gen_loss: 49.667 | mel_loss: 11.529 | 3.232 sec / 10 steps |\n",
      "| step: 410150 | gen_loss: 45.280 | mel_loss: 10.550 | 3.292 sec / 10 steps |\n",
      "| step: 410160 | gen_loss: 44.681 | mel_loss: 10.172 | 3.030 sec / 10 steps |\n",
      "| step: 410170 | gen_loss: 36.956 | mel_loss: 9.715 | 2.989 sec / 10 steps |\n",
      "| step: 410180 | gen_loss: 50.214 | mel_loss: 12.883 | 3.141 sec / 10 steps |\n",
      "| step: 410190 | gen_loss: 52.547 | mel_loss: 13.577 | 3.169 sec / 10 steps |\n",
      "| step: 410200 | gen_loss: 49.150 | mel_loss: 12.311 | 3.127 sec / 10 steps |\n",
      "Validation mel_loss: 14.373087882995605\n",
      "| step: 410210 | gen_loss: 50.919 | mel_loss: 12.489 | 3.547 sec / 10 steps |\n",
      "| step: 410220 | gen_loss: 50.733 | mel_loss: 13.392 | 3.288 sec / 10 steps |\n",
      "| step: 410230 | gen_loss: 45.535 | mel_loss: 11.490 | 3.186 sec / 10 steps |\n",
      "| step: 410240 | gen_loss: 40.465 | mel_loss: 8.793 | 3.171 sec / 10 steps |\n",
      "| step: 410250 | gen_loss: 42.477 | mel_loss: 10.248 | 3.342 sec / 10 steps |\n",
      "| step: 410260 | gen_loss: 47.336 | mel_loss: 11.666 | 3.298 sec / 10 steps |\n",
      "| step: 410270 | gen_loss: 48.210 | mel_loss: 11.588 | 3.392 sec / 10 steps |\n",
      "| step: 410280 | gen_loss: 51.686 | mel_loss: 13.653 | 2.729 sec / 10 steps |\n",
      "| step: 410290 | gen_loss: 45.619 | mel_loss: 11.330 | 3.153 sec / 10 steps |\n",
      "| step: 410300 | gen_loss: 47.198 | mel_loss: 11.796 | 3.004 sec / 10 steps |\n",
      "Validation mel_loss: 14.477595329284668\n",
      "| step: 410310 | gen_loss: 48.362 | mel_loss: 12.001 | 3.447 sec / 10 steps |\n",
      "| step: 410320 | gen_loss: 46.809 | mel_loss: 11.682 | 3.360 sec / 10 steps |\n",
      "| step: 410330 | gen_loss: 46.827 | mel_loss: 11.146 | 3.718 sec / 10 steps |\n",
      "| step: 410340 | gen_loss: 49.449 | mel_loss: 12.701 | 3.054 sec / 10 steps |\n",
      "| step: 410350 | gen_loss: 42.462 | mel_loss: 9.029 | 3.244 sec / 10 steps |\n",
      "| step: 410360 | gen_loss: 41.875 | mel_loss: 10.258 | 3.375 sec / 10 steps |\n",
      "| step: 410370 | gen_loss: 44.649 | mel_loss: 10.559 | 3.111 sec / 10 steps |\n",
      "| step: 410380 | gen_loss: 45.549 | mel_loss: 10.375 | 3.193 sec / 10 steps |\n",
      "| step: 410390 | gen_loss: 49.447 | mel_loss: 11.654 | 3.202 sec / 10 steps |\n",
      "| step: 410400 | gen_loss: 35.846 | mel_loss: 8.687 | 3.458 sec / 10 steps |\n",
      "Validation mel_loss: 14.228351593017578\n",
      "| step: 410410 | gen_loss: 45.789 | mel_loss: 11.799 | 3.678 sec / 10 steps |\n",
      "| step: 410420 | gen_loss: 38.257 | mel_loss: 9.570 | 3.112 sec / 10 steps |\n",
      "| step: 410430 | gen_loss: 45.776 | mel_loss: 10.837 | 3.207 sec / 10 steps |\n",
      "| step: 410440 | gen_loss: 48.755 | mel_loss: 12.025 | 3.410 sec / 10 steps |\n",
      "| step: 410450 | gen_loss: 47.094 | mel_loss: 11.077 | 3.463 sec / 10 steps |\n",
      "| step: 410460 | gen_loss: 42.016 | mel_loss: 8.984 | 3.132 sec / 10 steps |\n",
      "| step: 410470 | gen_loss: 47.449 | mel_loss: 11.127 | 3.630 sec / 10 steps |\n",
      "| step: 410480 | gen_loss: 41.781 | mel_loss: 9.994 | 3.043 sec / 10 steps |\n",
      "| step: 410490 | gen_loss: 38.306 | mel_loss: 9.568 | 3.560 sec / 10 steps |\n",
      "| step: 410500 | gen_loss: 41.541 | mel_loss: 9.108 | 3.484 sec / 10 steps |\n",
      "Validation mel_loss: 14.300032615661621\n",
      "| step: 410510 | gen_loss: 47.101 | mel_loss: 11.869 | 3.547 sec / 10 steps |\n",
      "| step: 410520 | gen_loss: 47.727 | mel_loss: 11.837 | 3.279 sec / 10 steps |\n",
      "| step: 410530 | gen_loss: 41.659 | mel_loss: 9.026 | 3.123 sec / 10 steps |\n",
      "| step: 410540 | gen_loss: 48.425 | mel_loss: 11.696 | 3.009 sec / 10 steps |\n",
      "| step: 410550 | gen_loss: 47.872 | mel_loss: 11.564 | 3.152 sec / 10 steps |\n",
      "| step: 410560 | gen_loss: 48.154 | mel_loss: 10.696 | 3.247 sec / 10 steps |\n",
      "| step: 410570 | gen_loss: 42.112 | mel_loss: 9.565 | 3.372 sec / 10 steps |\n",
      "| step: 410580 | gen_loss: 42.365 | mel_loss: 9.417 | 3.518 sec / 10 steps |\n",
      "| step: 410590 | gen_loss: 44.356 | mel_loss: 9.583 | 3.100 sec / 10 steps |\n",
      "| step: 410600 | gen_loss: 44.518 | mel_loss: 11.051 | 3.317 sec / 10 steps |\n",
      "Validation mel_loss: 14.204123497009277\n",
      "| step: 410610 | gen_loss: 47.879 | mel_loss: 12.186 | 3.375 sec / 10 steps |\n",
      "| step: 410620 | gen_loss: 45.728 | mel_loss: 11.231 | 3.327 sec / 10 steps |\n",
      "| step: 410630 | gen_loss: 46.573 | mel_loss: 10.688 | 3.229 sec / 10 steps |\n",
      "| step: 410640 | gen_loss: 49.957 | mel_loss: 12.777 | 3.388 sec / 10 steps |\n",
      "| step: 410650 | gen_loss: 49.066 | mel_loss: 11.897 | 3.512 sec / 10 steps |\n",
      "| step: 410660 | gen_loss: 47.889 | mel_loss: 12.202 | 3.734 sec / 10 steps |\n",
      "| step: 410670 | gen_loss: 50.627 | mel_loss: 13.030 | 2.996 sec / 10 steps |\n",
      "| step: 410680 | gen_loss: 47.869 | mel_loss: 12.907 | 3.573 sec / 10 steps |\n",
      "| step: 410690 | gen_loss: 46.305 | mel_loss: 10.079 | 3.173 sec / 10 steps |\n",
      "| step: 410700 | gen_loss: 53.047 | mel_loss: 13.654 | 4.658 sec / 10 steps |\n",
      "Validation mel_loss: 14.400922775268555\n",
      "| step: 410710 | gen_loss: 46.452 | mel_loss: 10.829 | 3.391 sec / 10 steps |\n",
      "| step: 410720 | gen_loss: 52.949 | mel_loss: 13.015 | 3.498 sec / 10 steps |\n",
      "| step: 410730 | gen_loss: 45.680 | mel_loss: 10.994 | 3.482 sec / 10 steps |\n",
      "| step: 410740 | gen_loss: 48.230 | mel_loss: 12.352 | 3.037 sec / 10 steps |\n",
      "| step: 410750 | gen_loss: 47.925 | mel_loss: 11.679 | 3.358 sec / 10 steps |\n",
      "|| Epoch: 547 ||\n",
      "| step: 410760 | gen_loss: 40.340 | mel_loss: 8.964 | 3.134 sec / 10 steps |\n",
      "| step: 410770 | gen_loss: 49.597 | mel_loss: 12.915 | 3.171 sec / 10 steps |\n",
      "| step: 410780 | gen_loss: 46.237 | mel_loss: 10.670 | 3.080 sec / 10 steps |\n",
      "| step: 410790 | gen_loss: 49.673 | mel_loss: 12.207 | 3.398 sec / 10 steps |\n",
      "| step: 410800 | gen_loss: 43.317 | mel_loss: 9.761 | 3.360 sec / 10 steps |\n",
      "Validation mel_loss: 14.1385498046875\n",
      "| step: 410810 | gen_loss: 50.781 | mel_loss: 13.508 | 3.310 sec / 10 steps |\n",
      "| step: 410820 | gen_loss: 45.656 | mel_loss: 10.886 | 3.441 sec / 10 steps |\n",
      "| step: 410830 | gen_loss: 47.997 | mel_loss: 11.799 | 3.205 sec / 10 steps |\n",
      "| step: 410840 | gen_loss: 44.901 | mel_loss: 10.073 | 3.414 sec / 10 steps |\n",
      "| step: 410850 | gen_loss: 51.528 | mel_loss: 13.237 | 3.774 sec / 10 steps |\n",
      "| step: 410860 | gen_loss: 40.590 | mel_loss: 9.597 | 3.431 sec / 10 steps |\n",
      "| step: 410870 | gen_loss: 49.823 | mel_loss: 12.359 | 2.988 sec / 10 steps |\n",
      "| step: 410880 | gen_loss: 47.579 | mel_loss: 11.622 | 3.328 sec / 10 steps |\n",
      "| step: 410890 | gen_loss: 47.141 | mel_loss: 10.883 | 3.124 sec / 10 steps |\n",
      "| step: 410900 | gen_loss: 41.037 | mel_loss: 9.746 | 3.177 sec / 10 steps |\n",
      "Validation mel_loss: 14.232744216918945\n",
      "| step: 410910 | gen_loss: 50.579 | mel_loss: 12.272 | 3.286 sec / 10 steps |\n",
      "| step: 410920 | gen_loss: 49.980 | mel_loss: 11.917 | 3.310 sec / 10 steps |\n",
      "| step: 410930 | gen_loss: 47.538 | mel_loss: 11.366 | 3.129 sec / 10 steps |\n",
      "| step: 410940 | gen_loss: 46.613 | mel_loss: 11.548 | 2.840 sec / 10 steps |\n",
      "| step: 410950 | gen_loss: 45.708 | mel_loss: 11.115 | 3.101 sec / 10 steps |\n",
      "| step: 410960 | gen_loss: 49.296 | mel_loss: 11.465 | 3.417 sec / 10 steps |\n",
      "| step: 410970 | gen_loss: 46.400 | mel_loss: 10.702 | 3.474 sec / 10 steps |\n",
      "| step: 410980 | gen_loss: 49.070 | mel_loss: 10.775 | 3.489 sec / 10 steps |\n",
      "| step: 410990 | gen_loss: 47.854 | mel_loss: 11.370 | 3.310 sec / 10 steps |\n",
      "| step: 411000 | gen_loss: 48.779 | mel_loss: 11.959 | 3.450 sec / 10 steps |\n",
      "Validation mel_loss: 14.257317543029785\n",
      "| step: 411010 | gen_loss: 45.047 | mel_loss: 11.425 | 3.615 sec / 10 steps |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 411020 | gen_loss: 47.906 | mel_loss: 11.987 | 3.411 sec / 10 steps |\n",
      "| step: 411030 | gen_loss: 39.545 | mel_loss: 10.321 | 3.197 sec / 10 steps |\n",
      "| step: 411040 | gen_loss: 50.008 | mel_loss: 11.495 | 3.199 sec / 10 steps |\n",
      "| step: 411050 | gen_loss: 49.009 | mel_loss: 11.383 | 3.386 sec / 10 steps |\n",
      "| step: 411060 | gen_loss: 50.104 | mel_loss: 12.285 | 3.109 sec / 10 steps |\n",
      "| step: 411070 | gen_loss: 48.117 | mel_loss: 11.607 | 3.141 sec / 10 steps |\n",
      "| step: 411080 | gen_loss: 43.233 | mel_loss: 9.329 | 3.353 sec / 10 steps |\n",
      "| step: 411090 | gen_loss: 42.366 | mel_loss: 9.945 | 3.311 sec / 10 steps |\n",
      "| step: 411100 | gen_loss: 49.196 | mel_loss: 11.792 | 3.200 sec / 10 steps |\n",
      "Validation mel_loss: 14.148661613464355\n",
      "| step: 411110 | gen_loss: 49.780 | mel_loss: 11.515 | 3.390 sec / 10 steps |\n",
      "| step: 411120 | gen_loss: 48.467 | mel_loss: 11.590 | 3.296 sec / 10 steps |\n",
      "| step: 411130 | gen_loss: 45.508 | mel_loss: 11.083 | 3.164 sec / 10 steps |\n",
      "| step: 411140 | gen_loss: 44.305 | mel_loss: 11.667 | 2.898 sec / 10 steps |\n",
      "| step: 411150 | gen_loss: 44.069 | mel_loss: 10.389 | 3.252 sec / 10 steps |\n",
      "| step: 411160 | gen_loss: 41.929 | mel_loss: 9.215 | 3.490 sec / 10 steps |\n",
      "| step: 411170 | gen_loss: 45.311 | mel_loss: 10.312 | 2.975 sec / 10 steps |\n",
      "| step: 411180 | gen_loss: 45.237 | mel_loss: 11.037 | 3.289 sec / 10 steps |\n",
      "| step: 411190 | gen_loss: 47.442 | mel_loss: 11.693 | 3.179 sec / 10 steps |\n",
      "| step: 411200 | gen_loss: 43.329 | mel_loss: 10.311 | 3.331 sec / 10 steps |\n",
      "Validation mel_loss: 14.161032676696777\n",
      "| step: 411210 | gen_loss: 44.792 | mel_loss: 10.024 | 3.793 sec / 10 steps |\n",
      "| step: 411220 | gen_loss: 40.242 | mel_loss: 8.303 | 3.467 sec / 10 steps |\n",
      "| step: 411230 | gen_loss: 46.364 | mel_loss: 10.608 | 3.398 sec / 10 steps |\n",
      "| step: 411240 | gen_loss: 47.301 | mel_loss: 11.436 | 3.010 sec / 10 steps |\n",
      "| step: 411250 | gen_loss: 52.981 | mel_loss: 12.994 | 3.090 sec / 10 steps |\n",
      "| step: 411260 | gen_loss: 50.228 | mel_loss: 12.376 | 3.525 sec / 10 steps |\n",
      "| step: 411270 | gen_loss: 48.930 | mel_loss: 11.842 | 2.947 sec / 10 steps |\n",
      "| step: 411280 | gen_loss: 44.128 | mel_loss: 10.314 | 3.460 sec / 10 steps |\n",
      "| step: 411290 | gen_loss: 47.250 | mel_loss: 11.598 | 3.283 sec / 10 steps |\n",
      "| step: 411300 | gen_loss: 45.530 | mel_loss: 10.857 | 3.160 sec / 10 steps |\n",
      "Validation mel_loss: 14.175276756286621\n",
      "| step: 411310 | gen_loss: 49.488 | mel_loss: 13.076 | 3.493 sec / 10 steps |\n",
      "| step: 411320 | gen_loss: 42.921 | mel_loss: 9.934 | 3.469 sec / 10 steps |\n",
      "| step: 411330 | gen_loss: 49.140 | mel_loss: 12.158 | 3.559 sec / 10 steps |\n",
      "| step: 411340 | gen_loss: 46.626 | mel_loss: 12.106 | 2.925 sec / 10 steps |\n",
      "| step: 411350 | gen_loss: 46.014 | mel_loss: 11.060 | 3.679 sec / 10 steps |\n",
      "| step: 411360 | gen_loss: 47.471 | mel_loss: 12.109 | 3.405 sec / 10 steps |\n",
      "| step: 411370 | gen_loss: 48.273 | mel_loss: 12.140 | 3.380 sec / 10 steps |\n",
      "| step: 411380 | gen_loss: 43.081 | mel_loss: 9.969 | 3.069 sec / 10 steps |\n",
      "| step: 411390 | gen_loss: 51.678 | mel_loss: 12.941 | 3.211 sec / 10 steps |\n",
      "| step: 411400 | gen_loss: 48.236 | mel_loss: 11.172 | 3.328 sec / 10 steps |\n",
      "Validation mel_loss: 14.140045166015625\n",
      "| step: 411410 | gen_loss: 48.763 | mel_loss: 11.314 | 2.954 sec / 10 steps |\n",
      "| step: 411420 | gen_loss: 49.601 | mel_loss: 12.798 | 3.320 sec / 10 steps |\n",
      "| step: 411430 | gen_loss: 43.035 | mel_loss: 10.381 | 3.362 sec / 10 steps |\n",
      "| step: 411440 | gen_loss: 43.159 | mel_loss: 9.827 | 3.200 sec / 10 steps |\n",
      "| step: 411450 | gen_loss: 44.715 | mel_loss: 10.854 | 3.321 sec / 10 steps |\n",
      "| step: 411460 | gen_loss: 48.797 | mel_loss: 11.383 | 3.256 sec / 10 steps |\n",
      "| step: 411470 | gen_loss: 49.021 | mel_loss: 11.569 | 3.022 sec / 10 steps |\n",
      "| step: 411480 | gen_loss: 47.157 | mel_loss: 11.318 | 2.997 sec / 10 steps |\n",
      "| step: 411490 | gen_loss: 50.677 | mel_loss: 12.567 | 3.154 sec / 10 steps |\n",
      "| step: 411500 | gen_loss: 43.029 | mel_loss: 10.152 | 3.482 sec / 10 steps |\n",
      "Validation mel_loss: 14.201292037963867\n",
      "|| Epoch: 548 ||\n",
      "| step: 411510 | gen_loss: 49.608 | mel_loss: 12.403 | 3.506 sec / 10 steps |\n",
      "| step: 411520 | gen_loss: 45.032 | mel_loss: 11.489 | 3.127 sec / 10 steps |\n",
      "| step: 411530 | gen_loss: 52.742 | mel_loss: 13.157 | 3.158 sec / 10 steps |\n",
      "| step: 411540 | gen_loss: 47.187 | mel_loss: 10.646 | 3.195 sec / 10 steps |\n",
      "| step: 411550 | gen_loss: 45.133 | mel_loss: 11.106 | 3.433 sec / 10 steps |\n",
      "| step: 411560 | gen_loss: 43.345 | mel_loss: 10.820 | 3.123 sec / 10 steps |\n",
      "| step: 411570 | gen_loss: 44.396 | mel_loss: 10.221 | 3.433 sec / 10 steps |\n",
      "| step: 411580 | gen_loss: 46.264 | mel_loss: 11.440 | 3.529 sec / 10 steps |\n",
      "| step: 411590 | gen_loss: 49.831 | mel_loss: 12.265 | 3.231 sec / 10 steps |\n",
      "| step: 411600 | gen_loss: 45.475 | mel_loss: 11.277 | 3.265 sec / 10 steps |\n",
      "Validation mel_loss: 14.19914436340332\n",
      "| step: 411610 | gen_loss: 43.503 | mel_loss: 10.126 | 3.342 sec / 10 steps |\n",
      "| step: 411620 | gen_loss: 48.299 | mel_loss: 12.009 | 3.250 sec / 10 steps |\n",
      "| step: 411630 | gen_loss: 43.580 | mel_loss: 10.730 | 3.177 sec / 10 steps |\n",
      "| step: 411640 | gen_loss: 43.684 | mel_loss: 9.697 | 3.196 sec / 10 steps |\n",
      "| step: 411650 | gen_loss: 44.992 | mel_loss: 10.836 | 3.440 sec / 10 steps |\n",
      "| step: 411660 | gen_loss: 42.628 | mel_loss: 8.872 | 3.428 sec / 10 steps |\n",
      "| step: 411670 | gen_loss: 50.443 | mel_loss: 12.551 | 3.221 sec / 10 steps |\n",
      "| step: 411680 | gen_loss: 46.846 | mel_loss: 10.808 | 3.260 sec / 10 steps |\n",
      "| step: 411690 | gen_loss: 45.308 | mel_loss: 10.507 | 3.026 sec / 10 steps |\n",
      "| step: 411700 | gen_loss: 47.414 | mel_loss: 12.030 | 3.128 sec / 10 steps |\n",
      "Validation mel_loss: 14.230583190917969\n",
      "| step: 411710 | gen_loss: 44.704 | mel_loss: 10.023 | 3.261 sec / 10 steps |\n",
      "| step: 411720 | gen_loss: 47.291 | mel_loss: 11.515 | 3.389 sec / 10 steps |\n",
      "| step: 411730 | gen_loss: 45.156 | mel_loss: 11.105 | 3.055 sec / 10 steps |\n",
      "| step: 411740 | gen_loss: 44.267 | mel_loss: 10.958 | 3.354 sec / 10 steps |\n",
      "| step: 411750 | gen_loss: 43.761 | mel_loss: 10.222 | 4.171 sec / 10 steps |\n",
      "| step: 411760 | gen_loss: 43.484 | mel_loss: 11.292 | 2.986 sec / 10 steps |\n",
      "| step: 411770 | gen_loss: 47.490 | mel_loss: 10.785 | 3.211 sec / 10 steps |\n",
      "| step: 411780 | gen_loss: 37.602 | mel_loss: 9.801 | 3.241 sec / 10 steps |\n",
      "| step: 411790 | gen_loss: 49.297 | mel_loss: 11.599 | 2.965 sec / 10 steps |\n",
      "| step: 411800 | gen_loss: 47.593 | mel_loss: 11.232 | 3.450 sec / 10 steps |\n",
      "Validation mel_loss: 14.212583541870117\n",
      "| step: 411810 | gen_loss: 43.704 | mel_loss: 10.042 | 3.238 sec / 10 steps |\n",
      "| step: 411820 | gen_loss: 52.090 | mel_loss: 12.561 | 3.161 sec / 10 steps |\n",
      "| step: 411830 | gen_loss: 50.485 | mel_loss: 12.156 | 3.285 sec / 10 steps |\n",
      "| step: 411840 | gen_loss: 45.135 | mel_loss: 11.050 | 3.543 sec / 10 steps |\n",
      "| step: 411850 | gen_loss: 47.941 | mel_loss: 11.977 | 3.313 sec / 10 steps |\n",
      "| step: 411860 | gen_loss: 52.185 | mel_loss: 12.600 | 3.123 sec / 10 steps |\n",
      "| step: 411870 | gen_loss: 48.663 | mel_loss: 11.385 | 3.101 sec / 10 steps |\n",
      "| step: 411880 | gen_loss: 46.577 | mel_loss: 11.287 | 2.992 sec / 10 steps |\n",
      "| step: 411890 | gen_loss: 45.493 | mel_loss: 11.097 | 3.440 sec / 10 steps |\n",
      "| step: 411900 | gen_loss: 50.218 | mel_loss: 12.286 | 3.007 sec / 10 steps |\n",
      "Validation mel_loss: 14.269335746765137\n",
      "| step: 411910 | gen_loss: 46.355 | mel_loss: 11.304 | 3.352 sec / 10 steps |\n",
      "| step: 411920 | gen_loss: 49.428 | mel_loss: 12.257 | 3.225 sec / 10 steps |\n",
      "| step: 411930 | gen_loss: 45.345 | mel_loss: 10.338 | 3.544 sec / 10 steps |\n",
      "| step: 411940 | gen_loss: 44.708 | mel_loss: 9.799 | 3.148 sec / 10 steps |\n",
      "| step: 411950 | gen_loss: 44.183 | mel_loss: 11.178 | 3.443 sec / 10 steps |\n",
      "| step: 411960 | gen_loss: 41.476 | mel_loss: 9.954 | 3.065 sec / 10 steps |\n",
      "| step: 411970 | gen_loss: 45.464 | mel_loss: 11.636 | 3.406 sec / 10 steps |\n",
      "| step: 411980 | gen_loss: 48.881 | mel_loss: 11.240 | 3.667 sec / 10 steps |\n",
      "| step: 411990 | gen_loss: 43.110 | mel_loss: 9.593 | 3.481 sec / 10 steps |\n",
      "| step: 412000 | gen_loss: 49.416 | mel_loss: 13.060 | 4.056 sec / 10 steps |\n",
      "Validation mel_loss: 14.238030433654785\n",
      "| step: 412010 | gen_loss: 48.071 | mel_loss: 10.977 | 3.297 sec / 10 steps |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 412020 | gen_loss: 47.186 | mel_loss: 11.819 | 3.278 sec / 10 steps |\n",
      "| step: 412030 | gen_loss: 49.018 | mel_loss: 11.964 | 3.222 sec / 10 steps |\n",
      "| step: 412040 | gen_loss: 47.056 | mel_loss: 11.393 | 3.404 sec / 10 steps |\n",
      "| step: 412050 | gen_loss: 41.675 | mel_loss: 9.645 | 3.204 sec / 10 steps |\n",
      "| step: 412060 | gen_loss: 41.976 | mel_loss: 9.755 | 3.277 sec / 10 steps |\n",
      "| step: 412070 | gen_loss: 45.226 | mel_loss: 11.058 | 3.243 sec / 10 steps |\n",
      "| step: 412080 | gen_loss: 49.507 | mel_loss: 12.149 | 3.306 sec / 10 steps |\n",
      "| step: 412090 | gen_loss: 43.728 | mel_loss: 9.896 | 3.116 sec / 10 steps |\n",
      "| step: 412100 | gen_loss: 52.867 | mel_loss: 13.166 | 3.311 sec / 10 steps |\n",
      "Validation mel_loss: 14.138110160827637\n",
      "| step: 412110 | gen_loss: 43.580 | mel_loss: 10.144 | 3.165 sec / 10 steps |\n",
      "| step: 412120 | gen_loss: 43.708 | mel_loss: 10.270 | 3.569 sec / 10 steps |\n",
      "| step: 412130 | gen_loss: 47.295 | mel_loss: 11.457 | 3.370 sec / 10 steps |\n",
      "| step: 412140 | gen_loss: 46.920 | mel_loss: 11.082 | 3.205 sec / 10 steps |\n",
      "| step: 412150 | gen_loss: 45.794 | mel_loss: 10.876 | 3.089 sec / 10 steps |\n",
      "| step: 412160 | gen_loss: 51.439 | mel_loss: 12.705 | 3.049 sec / 10 steps |\n",
      "| step: 412170 | gen_loss: 50.620 | mel_loss: 12.287 | 2.938 sec / 10 steps |\n",
      "| step: 412180 | gen_loss: 46.960 | mel_loss: 11.855 | 3.174 sec / 10 steps |\n",
      "| step: 412190 | gen_loss: 44.854 | mel_loss: 10.788 | 3.320 sec / 10 steps |\n",
      "| step: 412200 | gen_loss: 47.131 | mel_loss: 10.821 | 3.175 sec / 10 steps |\n",
      "Validation mel_loss: 14.309846878051758\n",
      "| step: 412210 | gen_loss: 49.369 | mel_loss: 12.805 | 3.382 sec / 10 steps |\n",
      "| step: 412220 | gen_loss: 48.502 | mel_loss: 12.287 | 3.079 sec / 10 steps |\n",
      "| step: 412230 | gen_loss: 46.596 | mel_loss: 10.982 | 3.580 sec / 10 steps |\n",
      "| step: 412240 | gen_loss: 46.467 | mel_loss: 11.066 | 3.097 sec / 10 steps |\n",
      "| step: 412250 | gen_loss: 49.655 | mel_loss: 12.120 | 3.112 sec / 10 steps |\n",
      "|| Epoch: 549 ||\n",
      "| step: 412260 | gen_loss: 44.922 | mel_loss: 10.333 | 3.723 sec / 10 steps |\n",
      "| step: 412270 | gen_loss: 49.350 | mel_loss: 12.391 | 3.298 sec / 10 steps |\n",
      "| step: 412280 | gen_loss: 54.317 | mel_loss: 13.532 | 3.397 sec / 10 steps |\n",
      "| step: 412290 | gen_loss: 46.256 | mel_loss: 10.423 | 3.475 sec / 10 steps |\n",
      "| step: 412300 | gen_loss: 40.849 | mel_loss: 9.346 | 3.354 sec / 10 steps |\n",
      "Validation mel_loss: 14.284616470336914\n",
      "| step: 412310 | gen_loss: 40.691 | mel_loss: 10.588 | 3.441 sec / 10 steps |\n",
      "| step: 412320 | gen_loss: 43.093 | mel_loss: 10.664 | 3.305 sec / 10 steps |\n",
      "| step: 412330 | gen_loss: 55.939 | mel_loss: 13.924 | 3.085 sec / 10 steps |\n",
      "| step: 412340 | gen_loss: 49.090 | mel_loss: 11.226 | 3.230 sec / 10 steps |\n",
      "| step: 412350 | gen_loss: 49.848 | mel_loss: 11.877 | 3.325 sec / 10 steps |\n",
      "| step: 412360 | gen_loss: 53.491 | mel_loss: 13.296 | 3.066 sec / 10 steps |\n",
      "| step: 412370 | gen_loss: 44.822 | mel_loss: 9.810 | 3.188 sec / 10 steps |\n",
      "| step: 412380 | gen_loss: 48.814 | mel_loss: 11.769 | 3.168 sec / 10 steps |\n",
      "| step: 412390 | gen_loss: 48.295 | mel_loss: 11.147 | 3.210 sec / 10 steps |\n",
      "| step: 412400 | gen_loss: 43.877 | mel_loss: 9.903 | 3.403 sec / 10 steps |\n",
      "Validation mel_loss: 14.221868515014648\n",
      "| step: 412410 | gen_loss: 46.829 | mel_loss: 11.408 | 3.501 sec / 10 steps |\n",
      "| step: 412420 | gen_loss: 46.504 | mel_loss: 11.252 | 3.222 sec / 10 steps |\n",
      "| step: 412430 | gen_loss: 53.384 | mel_loss: 13.062 | 3.332 sec / 10 steps |\n",
      "| step: 412440 | gen_loss: 46.901 | mel_loss: 11.859 | 3.122 sec / 10 steps |\n",
      "| step: 412450 | gen_loss: 48.960 | mel_loss: 11.377 | 3.091 sec / 10 steps |\n",
      "| step: 412460 | gen_loss: 43.627 | mel_loss: 9.445 | 3.094 sec / 10 steps |\n",
      "| step: 412470 | gen_loss: 44.059 | mel_loss: 10.875 | 2.959 sec / 10 steps |\n",
      "| step: 412480 | gen_loss: 42.801 | mel_loss: 10.367 | 3.081 sec / 10 steps |\n",
      "| step: 412490 | gen_loss: 42.234 | mel_loss: 9.891 | 3.535 sec / 10 steps |\n",
      "| step: 412500 | gen_loss: 50.927 | mel_loss: 13.607 | 2.971 sec / 10 steps |\n",
      "Validation mel_loss: 14.213512420654297\n",
      "| step: 412510 | gen_loss: 48.821 | mel_loss: 12.553 | 3.251 sec / 10 steps |\n",
      "| step: 412520 | gen_loss: 46.850 | mel_loss: 11.960 | 3.184 sec / 10 steps |\n",
      "| step: 412530 | gen_loss: 46.353 | mel_loss: 11.007 | 3.515 sec / 10 steps |\n",
      "| step: 412540 | gen_loss: 37.197 | mel_loss: 9.690 | 3.213 sec / 10 steps |\n",
      "| step: 412550 | gen_loss: 45.158 | mel_loss: 10.162 | 2.963 sec / 10 steps |\n",
      "| step: 412560 | gen_loss: 45.089 | mel_loss: 10.224 | 3.211 sec / 10 steps |\n",
      "| step: 412570 | gen_loss: 47.861 | mel_loss: 11.824 | 2.954 sec / 10 steps |\n",
      "| step: 412580 | gen_loss: 48.975 | mel_loss: 12.748 | 3.300 sec / 10 steps |\n",
      "| step: 412590 | gen_loss: 47.341 | mel_loss: 11.825 | 3.076 sec / 10 steps |\n",
      "| step: 412600 | gen_loss: 47.949 | mel_loss: 12.205 | 3.343 sec / 10 steps |\n",
      "Validation mel_loss: 14.262754440307617\n",
      "| step: 412610 | gen_loss: 39.963 | mel_loss: 9.042 | 3.385 sec / 10 steps |\n",
      "| step: 412620 | gen_loss: 48.928 | mel_loss: 12.265 | 3.133 sec / 10 steps |\n",
      "| step: 412630 | gen_loss: 48.088 | mel_loss: 12.405 | 2.934 sec / 10 steps |\n",
      "| step: 412640 | gen_loss: 43.526 | mel_loss: 9.437 | 3.043 sec / 10 steps |\n",
      "| step: 412650 | gen_loss: 47.641 | mel_loss: 11.669 | 3.420 sec / 10 steps |\n",
      "| step: 412660 | gen_loss: 47.217 | mel_loss: 10.918 | 3.112 sec / 10 steps |\n",
      "| step: 412670 | gen_loss: 37.987 | mel_loss: 9.169 | 3.456 sec / 10 steps |\n",
      "| step: 412680 | gen_loss: 46.250 | mel_loss: 11.715 | 3.099 sec / 10 steps |\n",
      "| step: 412690 | gen_loss: 43.167 | mel_loss: 10.702 | 3.454 sec / 10 steps |\n",
      "| step: 412700 | gen_loss: 43.620 | mel_loss: 10.945 | 3.277 sec / 10 steps |\n",
      "Validation mel_loss: 14.216755867004395\n",
      "| step: 412710 | gen_loss: 51.729 | mel_loss: 12.501 | 2.876 sec / 10 steps |\n",
      "| step: 412720 | gen_loss: 53.030 | mel_loss: 13.233 | 3.119 sec / 10 steps |\n",
      "| step: 412730 | gen_loss: 48.890 | mel_loss: 11.391 | 3.038 sec / 10 steps |\n",
      "| step: 412740 | gen_loss: 46.274 | mel_loss: 11.655 | 3.717 sec / 10 steps |\n",
      "| step: 412750 | gen_loss: 45.560 | mel_loss: 9.995 | 3.267 sec / 10 steps |\n",
      "| step: 412760 | gen_loss: 48.527 | mel_loss: 11.631 | 3.138 sec / 10 steps |\n",
      "| step: 412770 | gen_loss: 46.683 | mel_loss: 9.830 | 3.149 sec / 10 steps |\n",
      "| step: 412780 | gen_loss: 48.600 | mel_loss: 11.558 | 3.197 sec / 10 steps |\n",
      "| step: 412790 | gen_loss: 42.282 | mel_loss: 9.383 | 3.358 sec / 10 steps |\n",
      "| step: 412800 | gen_loss: 51.106 | mel_loss: 12.188 | 3.522 sec / 10 steps |\n",
      "Validation mel_loss: 14.133387565612793\n",
      "| step: 412810 | gen_loss: 47.203 | mel_loss: 11.123 | 3.334 sec / 10 steps |\n",
      "| step: 412820 | gen_loss: 49.709 | mel_loss: 11.578 | 3.361 sec / 10 steps |\n",
      "| step: 412830 | gen_loss: 45.740 | mel_loss: 9.540 | 3.405 sec / 10 steps |\n",
      "| step: 412840 | gen_loss: 44.351 | mel_loss: 9.757 | 3.372 sec / 10 steps |\n",
      "| step: 412850 | gen_loss: 47.661 | mel_loss: 11.062 | 3.364 sec / 10 steps |\n",
      "| step: 412860 | gen_loss: 45.473 | mel_loss: 9.592 | 3.294 sec / 10 steps |\n",
      "| step: 412870 | gen_loss: 46.303 | mel_loss: 11.152 | 3.051 sec / 10 steps |\n",
      "| step: 412880 | gen_loss: 50.056 | mel_loss: 12.034 | 3.192 sec / 10 steps |\n",
      "| step: 412890 | gen_loss: 48.502 | mel_loss: 11.384 | 3.480 sec / 10 steps |\n",
      "| step: 412900 | gen_loss: 44.260 | mel_loss: 9.597 | 3.801 sec / 10 steps |\n",
      "Validation mel_loss: 14.12154483795166\n",
      "| step: 412910 | gen_loss: 48.333 | mel_loss: 11.372 | 3.589 sec / 10 steps |\n",
      "| step: 412920 | gen_loss: 49.027 | mel_loss: 12.127 | 3.444 sec / 10 steps |\n",
      "| step: 412930 | gen_loss: 47.507 | mel_loss: 11.747 | 3.083 sec / 10 steps |\n",
      "| step: 412940 | gen_loss: 44.764 | mel_loss: 9.683 | 3.350 sec / 10 steps |\n",
      "| step: 412950 | gen_loss: 46.734 | mel_loss: 10.842 | 3.403 sec / 10 steps |\n",
      "| step: 412960 | gen_loss: 53.127 | mel_loss: 13.904 | 3.183 sec / 10 steps |\n",
      "| step: 412970 | gen_loss: 47.070 | mel_loss: 10.607 | 3.411 sec / 10 steps |\n",
      "| step: 412980 | gen_loss: 42.546 | mel_loss: 10.268 | 3.394 sec / 10 steps |\n",
      "| step: 412990 | gen_loss: 47.088 | mel_loss: 11.004 | 3.474 sec / 10 steps |\n",
      "| step: 413000 | gen_loss: 43.651 | mel_loss: 10.317 | 3.418 sec / 10 steps |\n",
      "Validation mel_loss: 14.355928421020508\n",
      "|| Epoch: 550 ||\n",
      "| step: 413010 | gen_loss: 41.328 | mel_loss: 9.068 | 3.594 sec / 10 steps |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 413020 | gen_loss: 44.802 | mel_loss: 9.996 | 3.464 sec / 10 steps |\n",
      "| step: 413030 | gen_loss: 46.868 | mel_loss: 11.430 | 3.019 sec / 10 steps |\n",
      "| step: 413040 | gen_loss: 51.737 | mel_loss: 13.321 | 3.125 sec / 10 steps |\n",
      "| step: 413050 | gen_loss: 45.779 | mel_loss: 10.824 | 2.986 sec / 10 steps |\n",
      "| step: 413060 | gen_loss: 41.001 | mel_loss: 10.291 | 3.656 sec / 10 steps |\n",
      "| step: 413070 | gen_loss: 40.505 | mel_loss: 10.031 | 3.383 sec / 10 steps |\n",
      "| step: 413080 | gen_loss: 44.875 | mel_loss: 10.901 | 3.727 sec / 10 steps |\n",
      "| step: 413090 | gen_loss: 45.281 | mel_loss: 9.899 | 3.814 sec / 10 steps |\n",
      "| step: 413100 | gen_loss: 46.096 | mel_loss: 10.629 | 3.044 sec / 10 steps |\n",
      "Validation mel_loss: 14.2535982131958\n",
      "| step: 413110 | gen_loss: 48.379 | mel_loss: 11.828 | 3.093 sec / 10 steps |\n",
      "| step: 413120 | gen_loss: 48.085 | mel_loss: 11.525 | 3.221 sec / 10 steps |\n",
      "| step: 413130 | gen_loss: 47.528 | mel_loss: 12.302 | 3.278 sec / 10 steps |\n",
      "| step: 413140 | gen_loss: 48.405 | mel_loss: 12.132 | 3.385 sec / 10 steps |\n",
      "| step: 413150 | gen_loss: 50.016 | mel_loss: 12.608 | 3.089 sec / 10 steps |\n",
      "| step: 413160 | gen_loss: 51.343 | mel_loss: 12.512 | 3.304 sec / 10 steps |\n",
      "| step: 413170 | gen_loss: 48.467 | mel_loss: 12.013 | 3.095 sec / 10 steps |\n",
      "| step: 413180 | gen_loss: 40.101 | mel_loss: 8.580 | 3.202 sec / 10 steps |\n",
      "| step: 413190 | gen_loss: 45.604 | mel_loss: 10.501 | 3.646 sec / 10 steps |\n",
      "| step: 413200 | gen_loss: 50.933 | mel_loss: 11.654 | 3.269 sec / 10 steps |\n",
      "Validation mel_loss: 14.159707069396973\n",
      "| step: 413210 | gen_loss: 46.272 | mel_loss: 11.229 | 3.107 sec / 10 steps |\n",
      "| step: 413220 | gen_loss: 49.930 | mel_loss: 12.542 | 3.194 sec / 10 steps |\n",
      "| step: 413230 | gen_loss: 43.923 | mel_loss: 10.778 | 3.019 sec / 10 steps |\n",
      "| step: 413240 | gen_loss: 42.512 | mel_loss: 10.733 | 3.401 sec / 10 steps |\n",
      "| step: 413250 | gen_loss: 48.422 | mel_loss: 11.807 | 3.629 sec / 10 steps |\n",
      "| step: 413260 | gen_loss: 47.825 | mel_loss: 10.891 | 3.240 sec / 10 steps |\n",
      "| step: 413270 | gen_loss: 48.094 | mel_loss: 11.672 | 3.292 sec / 10 steps |\n",
      "| step: 413280 | gen_loss: 50.935 | mel_loss: 12.708 | 3.377 sec / 10 steps |\n",
      "| step: 413290 | gen_loss: 49.715 | mel_loss: 12.076 | 3.010 sec / 10 steps |\n",
      "| step: 413300 | gen_loss: 42.365 | mel_loss: 9.306 | 2.985 sec / 10 steps |\n",
      "Validation mel_loss: 14.090048789978027\n",
      "| step: 413310 | gen_loss: 51.925 | mel_loss: 12.661 | 3.331 sec / 10 steps |\n",
      "| step: 413320 | gen_loss: 45.067 | mel_loss: 10.752 | 3.573 sec / 10 steps |\n",
      "| step: 413330 | gen_loss: 47.232 | mel_loss: 11.417 | 3.423 sec / 10 steps |\n",
      "| step: 413340 | gen_loss: 48.988 | mel_loss: 11.764 | 3.271 sec / 10 steps |\n",
      "| step: 413350 | gen_loss: 41.241 | mel_loss: 9.087 | 3.129 sec / 10 steps |\n",
      "| step: 413360 | gen_loss: 47.100 | mel_loss: 11.245 | 3.130 sec / 10 steps |\n",
      "| step: 413370 | gen_loss: 47.542 | mel_loss: 11.451 | 3.446 sec / 10 steps |\n",
      "| step: 413380 | gen_loss: 53.687 | mel_loss: 13.363 | 3.126 sec / 10 steps |\n",
      "| step: 413390 | gen_loss: 41.621 | mel_loss: 10.163 | 3.134 sec / 10 steps |\n",
      "| step: 413400 | gen_loss: 44.545 | mel_loss: 10.771 | 2.970 sec / 10 steps |\n",
      "Validation mel_loss: 14.127214431762695\n",
      "| step: 413410 | gen_loss: 46.520 | mel_loss: 11.411 | 3.404 sec / 10 steps |\n",
      "| step: 413420 | gen_loss: 49.258 | mel_loss: 12.185 | 3.418 sec / 10 steps |\n",
      "| step: 413430 | gen_loss: 45.762 | mel_loss: 11.068 | 3.951 sec / 10 steps |\n",
      "| step: 413440 | gen_loss: 47.327 | mel_loss: 11.744 | 3.319 sec / 10 steps |\n",
      "| step: 413450 | gen_loss: 46.853 | mel_loss: 11.963 | 3.344 sec / 10 steps |\n",
      "| step: 413460 | gen_loss: 48.924 | mel_loss: 11.766 | 3.267 sec / 10 steps |\n",
      "| step: 413470 | gen_loss: 45.753 | mel_loss: 10.660 | 3.094 sec / 10 steps |\n",
      "| step: 413480 | gen_loss: 43.630 | mel_loss: 9.547 | 2.998 sec / 10 steps |\n",
      "| step: 413490 | gen_loss: 51.213 | mel_loss: 12.230 | 3.422 sec / 10 steps |\n",
      "| step: 413500 | gen_loss: 49.219 | mel_loss: 12.409 | 3.406 sec / 10 steps |\n",
      "Validation mel_loss: 14.326037406921387\n",
      "| step: 413510 | gen_loss: 48.292 | mel_loss: 12.174 | 3.350 sec / 10 steps |\n",
      "| step: 413520 | gen_loss: 45.415 | mel_loss: 10.580 | 3.171 sec / 10 steps |\n",
      "| step: 413530 | gen_loss: 46.728 | mel_loss: 10.623 | 3.532 sec / 10 steps |\n",
      "| step: 413540 | gen_loss: 48.889 | mel_loss: 12.323 | 3.099 sec / 10 steps |\n",
      "| step: 413550 | gen_loss: 46.992 | mel_loss: 11.771 | 3.331 sec / 10 steps |\n",
      "| step: 413560 | gen_loss: 49.705 | mel_loss: 11.208 | 3.224 sec / 10 steps |\n",
      "| step: 413570 | gen_loss: 48.227 | mel_loss: 12.687 | 3.572 sec / 10 steps |\n",
      "| step: 413580 | gen_loss: 49.103 | mel_loss: 11.857 | 3.094 sec / 10 steps |\n",
      "| step: 413590 | gen_loss: 45.318 | mel_loss: 10.330 | 3.398 sec / 10 steps |\n",
      "| step: 413600 | gen_loss: 47.423 | mel_loss: 11.383 | 3.276 sec / 10 steps |\n",
      "Validation mel_loss: 14.112666130065918\n",
      "| step: 413610 | gen_loss: 47.496 | mel_loss: 11.520 | 3.446 sec / 10 steps |\n",
      "| step: 413620 | gen_loss: 50.729 | mel_loss: 12.348 | 3.307 sec / 10 steps |\n",
      "| step: 413630 | gen_loss: 45.643 | mel_loss: 9.470 | 3.201 sec / 10 steps |\n",
      "| step: 413640 | gen_loss: 49.318 | mel_loss: 12.465 | 3.093 sec / 10 steps |\n",
      "| step: 413650 | gen_loss: 42.208 | mel_loss: 10.790 | 3.117 sec / 10 steps |\n",
      "| step: 413660 | gen_loss: 46.188 | mel_loss: 11.411 | 3.241 sec / 10 steps |\n",
      "| step: 413670 | gen_loss: 47.238 | mel_loss: 11.977 | 3.079 sec / 10 steps |\n",
      "| step: 413680 | gen_loss: 45.700 | mel_loss: 10.260 | 3.317 sec / 10 steps |\n",
      "| step: 413690 | gen_loss: 50.536 | mel_loss: 12.698 | 3.035 sec / 10 steps |\n",
      "| step: 413700 | gen_loss: 45.194 | mel_loss: 11.043 | 3.491 sec / 10 steps |\n",
      "Validation mel_loss: 14.249683380126953\n",
      "| step: 413710 | gen_loss: 50.213 | mel_loss: 12.802 | 3.286 sec / 10 steps |\n",
      "| step: 413720 | gen_loss: 44.701 | mel_loss: 10.625 | 3.474 sec / 10 steps |\n",
      "| step: 413730 | gen_loss: 48.689 | mel_loss: 11.501 | 3.353 sec / 10 steps |\n",
      "| step: 413740 | gen_loss: 44.153 | mel_loss: 9.798 | 3.328 sec / 10 steps |\n",
      "| step: 413750 | gen_loss: 46.064 | mel_loss: 10.897 | 3.158 sec / 10 steps |\n",
      "|| Epoch: 551 ||\n",
      "| step: 413760 | gen_loss: 48.103 | mel_loss: 11.063 | 3.294 sec / 10 steps |\n",
      "| step: 413770 | gen_loss: 49.037 | mel_loss: 12.263 | 2.859 sec / 10 steps |\n",
      "| step: 413780 | gen_loss: 49.089 | mel_loss: 12.198 | 3.435 sec / 10 steps |\n",
      "| step: 413790 | gen_loss: 52.817 | mel_loss: 12.578 | 3.189 sec / 10 steps |\n",
      "| step: 413800 | gen_loss: 46.286 | mel_loss: 11.066 | 3.285 sec / 10 steps |\n",
      "Validation mel_loss: 14.143818855285645\n",
      "| step: 413810 | gen_loss: 42.521 | mel_loss: 9.763 | 3.588 sec / 10 steps |\n",
      "| step: 413820 | gen_loss: 50.161 | mel_loss: 11.727 | 3.329 sec / 10 steps |\n",
      "| step: 413830 | gen_loss: 44.803 | mel_loss: 10.284 | 3.330 sec / 10 steps |\n",
      "| step: 413840 | gen_loss: 52.834 | mel_loss: 13.167 | 3.004 sec / 10 steps |\n",
      "| step: 413850 | gen_loss: 43.132 | mel_loss: 10.147 | 3.327 sec / 10 steps |\n",
      "| step: 413860 | gen_loss: 45.947 | mel_loss: 10.844 | 3.228 sec / 10 steps |\n",
      "| step: 413870 | gen_loss: 47.132 | mel_loss: 11.596 | 3.048 sec / 10 steps |\n",
      "| step: 413880 | gen_loss: 50.546 | mel_loss: 11.790 | 3.239 sec / 10 steps |\n",
      "| step: 413890 | gen_loss: 51.014 | mel_loss: 13.265 | 3.594 sec / 10 steps |\n",
      "| step: 413900 | gen_loss: 49.223 | mel_loss: 11.105 | 3.291 sec / 10 steps |\n",
      "Validation mel_loss: 14.199590682983398\n",
      "| step: 413910 | gen_loss: 47.145 | mel_loss: 11.974 | 3.188 sec / 10 steps |\n",
      "| step: 413920 | gen_loss: 51.125 | mel_loss: 13.046 | 3.564 sec / 10 steps |\n",
      "| step: 413930 | gen_loss: 44.986 | mel_loss: 9.604 | 3.170 sec / 10 steps |\n",
      "| step: 413940 | gen_loss: 42.691 | mel_loss: 9.611 | 3.399 sec / 10 steps |\n",
      "| step: 413950 | gen_loss: 49.103 | mel_loss: 12.211 | 3.397 sec / 10 steps |\n",
      "| step: 413960 | gen_loss: 52.340 | mel_loss: 12.547 | 3.342 sec / 10 steps |\n",
      "| step: 413970 | gen_loss: 46.027 | mel_loss: 11.126 | 3.007 sec / 10 steps |\n",
      "| step: 413980 | gen_loss: 45.836 | mel_loss: 11.162 | 3.176 sec / 10 steps |\n",
      "| step: 413990 | gen_loss: 45.280 | mel_loss: 10.726 | 3.124 sec / 10 steps |\n",
      "| step: 414000 | gen_loss: 47.284 | mel_loss: 11.431 | 3.228 sec / 10 steps |\n",
      "Validation mel_loss: 14.14306640625\n",
      "| step: 414010 | gen_loss: 44.752 | mel_loss: 10.155 | 3.154 sec / 10 steps |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 414020 | gen_loss: 46.958 | mel_loss: 12.013 | 2.782 sec / 10 steps |\n",
      "| step: 414030 | gen_loss: 40.721 | mel_loss: 8.998 | 3.351 sec / 10 steps |\n",
      "| step: 414040 | gen_loss: 46.042 | mel_loss: 11.397 | 3.654 sec / 10 steps |\n",
      "| step: 414050 | gen_loss: 49.842 | mel_loss: 12.074 | 3.152 sec / 10 steps |\n",
      "| step: 414060 | gen_loss: 38.093 | mel_loss: 9.809 | 3.460 sec / 10 steps |\n",
      "| step: 414070 | gen_loss: 44.983 | mel_loss: 10.708 | 3.106 sec / 10 steps |\n",
      "| step: 414080 | gen_loss: 51.883 | mel_loss: 12.910 | 2.980 sec / 10 steps |\n",
      "| step: 414090 | gen_loss: 43.488 | mel_loss: 10.233 | 3.548 sec / 10 steps |\n",
      "| step: 414100 | gen_loss: 45.745 | mel_loss: 10.518 | 3.239 sec / 10 steps |\n",
      "Validation mel_loss: 14.243683815002441\n",
      "| step: 414110 | gen_loss: 45.604 | mel_loss: 10.699 | 3.223 sec / 10 steps |\n",
      "| step: 414120 | gen_loss: 48.935 | mel_loss: 11.513 | 3.303 sec / 10 steps |\n",
      "| step: 414130 | gen_loss: 44.332 | mel_loss: 9.341 | 3.703 sec / 10 steps |\n",
      "| step: 414140 | gen_loss: 46.865 | mel_loss: 11.425 | 3.104 sec / 10 steps |\n",
      "| step: 414150 | gen_loss: 46.238 | mel_loss: 10.970 | 3.179 sec / 10 steps |\n",
      "| step: 414160 | gen_loss: 44.356 | mel_loss: 9.992 | 3.611 sec / 10 steps |\n",
      "| step: 414170 | gen_loss: 46.573 | mel_loss: 10.715 | 3.317 sec / 10 steps |\n",
      "| step: 414180 | gen_loss: 38.983 | mel_loss: 9.599 | 3.332 sec / 10 steps |\n",
      "| step: 414190 | gen_loss: 51.190 | mel_loss: 13.117 | 3.116 sec / 10 steps |\n",
      "| step: 414200 | gen_loss: 42.301 | mel_loss: 9.724 | 3.330 sec / 10 steps |\n",
      "Validation mel_loss: 14.298880577087402\n",
      "| step: 414210 | gen_loss: 47.636 | mel_loss: 11.271 | 3.581 sec / 10 steps |\n",
      "| step: 414220 | gen_loss: 46.298 | mel_loss: 11.084 | 3.178 sec / 10 steps |\n",
      "| step: 414230 | gen_loss: 52.122 | mel_loss: 12.908 | 3.533 sec / 10 steps |\n",
      "| step: 414240 | gen_loss: 42.637 | mel_loss: 9.772 | 3.455 sec / 10 steps |\n",
      "| step: 414250 | gen_loss: 54.297 | mel_loss: 13.414 | 3.216 sec / 10 steps |\n",
      "| step: 414260 | gen_loss: 52.932 | mel_loss: 13.623 | 3.289 sec / 10 steps |\n",
      "| step: 414270 | gen_loss: 47.774 | mel_loss: 11.362 | 3.003 sec / 10 steps |\n",
      "| step: 414280 | gen_loss: 43.110 | mel_loss: 9.755 | 3.142 sec / 10 steps |\n",
      "| step: 414290 | gen_loss: 47.205 | mel_loss: 11.484 | 3.036 sec / 10 steps |\n",
      "| step: 414300 | gen_loss: 45.346 | mel_loss: 10.683 | 3.436 sec / 10 steps |\n",
      "Validation mel_loss: 14.35230541229248\n",
      "| step: 414310 | gen_loss: 51.571 | mel_loss: 12.405 | 3.350 sec / 10 steps |\n",
      "| step: 414320 | gen_loss: 47.039 | mel_loss: 11.012 | 3.275 sec / 10 steps |\n",
      "| step: 414330 | gen_loss: 48.420 | mel_loss: 11.616 | 3.186 sec / 10 steps |\n",
      "| step: 414340 | gen_loss: 49.355 | mel_loss: 12.585 | 3.209 sec / 10 steps |\n",
      "| step: 414350 | gen_loss: 39.824 | mel_loss: 9.662 | 3.768 sec / 10 steps |\n",
      "| step: 414360 | gen_loss: 48.895 | mel_loss: 12.477 | 3.168 sec / 10 steps |\n",
      "| step: 414370 | gen_loss: 44.145 | mel_loss: 11.265 | 3.503 sec / 10 steps |\n",
      "| step: 414380 | gen_loss: 48.363 | mel_loss: 11.797 | 3.382 sec / 10 steps |\n",
      "| step: 414390 | gen_loss: 47.177 | mel_loss: 10.305 | 3.015 sec / 10 steps |\n",
      "| step: 414400 | gen_loss: 48.518 | mel_loss: 12.524 | 3.370 sec / 10 steps |\n",
      "Validation mel_loss: 14.281949996948242\n",
      "| step: 414410 | gen_loss: 48.886 | mel_loss: 11.885 | 3.162 sec / 10 steps |\n",
      "| step: 414420 | gen_loss: 43.853 | mel_loss: 11.295 | 3.375 sec / 10 steps |\n",
      "| step: 414430 | gen_loss: 49.893 | mel_loss: 11.935 | 3.240 sec / 10 steps |\n",
      "| step: 414440 | gen_loss: 48.170 | mel_loss: 11.535 | 3.709 sec / 10 steps |\n",
      "| step: 414450 | gen_loss: 45.939 | mel_loss: 10.959 | 3.183 sec / 10 steps |\n",
      "| step: 414460 | gen_loss: 44.152 | mel_loss: 9.643 | 3.461 sec / 10 steps |\n",
      "| step: 414470 | gen_loss: 48.658 | mel_loss: 12.137 | 2.997 sec / 10 steps |\n",
      "| step: 414480 | gen_loss: 46.009 | mel_loss: 11.025 | 3.204 sec / 10 steps |\n",
      "| step: 414490 | gen_loss: 43.578 | mel_loss: 10.128 | 3.549 sec / 10 steps |\n",
      "| step: 414500 | gen_loss: 48.792 | mel_loss: 12.382 | 3.298 sec / 10 steps |\n",
      "Validation mel_loss: 14.283100128173828\n",
      "|| Epoch: 552 ||\n",
      "| step: 414510 | gen_loss: 46.784 | mel_loss: 10.725 | 3.381 sec / 10 steps |\n",
      "| step: 414520 | gen_loss: 47.377 | mel_loss: 11.273 | 2.973 sec / 10 steps |\n",
      "| step: 414530 | gen_loss: 45.174 | mel_loss: 10.714 | 3.239 sec / 10 steps |\n",
      "| step: 414540 | gen_loss: 42.847 | mel_loss: 9.950 | 3.211 sec / 10 steps |\n",
      "| step: 414550 | gen_loss: 49.457 | mel_loss: 11.857 | 3.178 sec / 10 steps |\n",
      "| step: 414560 | gen_loss: 47.748 | mel_loss: 11.711 | 3.245 sec / 10 steps |\n",
      "| step: 414570 | gen_loss: 47.896 | mel_loss: 12.392 | 3.384 sec / 10 steps |\n",
      "| step: 414580 | gen_loss: 46.682 | mel_loss: 12.084 | 2.994 sec / 10 steps |\n",
      "| step: 414590 | gen_loss: 45.858 | mel_loss: 11.531 | 3.227 sec / 10 steps |\n",
      "| step: 414600 | gen_loss: 48.968 | mel_loss: 12.086 | 3.156 sec / 10 steps |\n",
      "Validation mel_loss: 14.172674179077148\n",
      "| step: 414610 | gen_loss: 39.021 | mel_loss: 8.969 | 3.475 sec / 10 steps |\n",
      "| step: 414620 | gen_loss: 47.719 | mel_loss: 11.222 | 3.453 sec / 10 steps |\n",
      "| step: 414630 | gen_loss: 49.168 | mel_loss: 12.225 | 3.232 sec / 10 steps |\n",
      "| step: 414640 | gen_loss: 47.754 | mel_loss: 12.486 | 3.485 sec / 10 steps |\n",
      "| step: 414650 | gen_loss: 46.188 | mel_loss: 11.055 | 3.192 sec / 10 steps |\n",
      "| step: 414660 | gen_loss: 46.854 | mel_loss: 11.063 | 3.351 sec / 10 steps |\n",
      "| step: 414670 | gen_loss: 52.395 | mel_loss: 13.491 | 3.516 sec / 10 steps |\n",
      "| step: 414680 | gen_loss: 46.594 | mel_loss: 11.439 | 3.246 sec / 10 steps |\n",
      "| step: 414690 | gen_loss: 51.265 | mel_loss: 13.076 | 3.296 sec / 10 steps |\n",
      "| step: 414700 | gen_loss: 45.148 | mel_loss: 10.670 | 3.762 sec / 10 steps |\n",
      "Validation mel_loss: 14.140583992004395\n",
      "| step: 414710 | gen_loss: 45.207 | mel_loss: 10.697 | 4.009 sec / 10 steps |\n",
      "| step: 414720 | gen_loss: 52.815 | mel_loss: 12.997 | 3.317 sec / 10 steps |\n",
      "| step: 414730 | gen_loss: 51.178 | mel_loss: 12.379 | 3.020 sec / 10 steps |\n",
      "| step: 414740 | gen_loss: 44.854 | mel_loss: 10.054 | 3.594 sec / 10 steps |\n",
      "| step: 414750 | gen_loss: 39.038 | mel_loss: 9.779 | 3.539 sec / 10 steps |\n",
      "| step: 414760 | gen_loss: 47.334 | mel_loss: 12.336 | 3.632 sec / 10 steps |\n",
      "| step: 414770 | gen_loss: 49.591 | mel_loss: 12.012 | 3.283 sec / 10 steps |\n",
      "| step: 414780 | gen_loss: 49.308 | mel_loss: 12.418 | 3.451 sec / 10 steps |\n",
      "| step: 414790 | gen_loss: 43.191 | mel_loss: 10.572 | 3.230 sec / 10 steps |\n",
      "| step: 414800 | gen_loss: 39.875 | mel_loss: 8.667 | 3.248 sec / 10 steps |\n",
      "Validation mel_loss: 14.131790161132812\n",
      "| step: 414810 | gen_loss: 46.908 | mel_loss: 11.234 | 3.296 sec / 10 steps |\n",
      "| step: 414820 | gen_loss: 34.017 | mel_loss: 7.766 | 4.699 sec / 10 steps |\n",
      "| step: 414830 | gen_loss: 48.383 | mel_loss: 11.931 | 3.136 sec / 10 steps |\n",
      "| step: 414840 | gen_loss: 46.796 | mel_loss: 10.495 | 3.489 sec / 10 steps |\n",
      "| step: 414850 | gen_loss: 45.470 | mel_loss: 11.098 | 3.482 sec / 10 steps |\n",
      "| step: 414860 | gen_loss: 46.220 | mel_loss: 10.402 | 3.433 sec / 10 steps |\n",
      "| step: 414870 | gen_loss: 50.767 | mel_loss: 12.545 | 3.082 sec / 10 steps |\n",
      "| step: 414880 | gen_loss: 45.815 | mel_loss: 10.251 | 3.241 sec / 10 steps |\n",
      "| step: 414890 | gen_loss: 46.587 | mel_loss: 10.719 | 3.694 sec / 10 steps |\n",
      "| step: 414900 | gen_loss: 45.895 | mel_loss: 9.739 | 3.303 sec / 10 steps |\n",
      "Validation mel_loss: 14.059629440307617\n",
      "| step: 414910 | gen_loss: 48.432 | mel_loss: 10.949 | 3.469 sec / 10 steps |\n",
      "| step: 414920 | gen_loss: 49.989 | mel_loss: 13.356 | 3.340 sec / 10 steps |\n",
      "| step: 414930 | gen_loss: 41.071 | mel_loss: 9.211 | 3.391 sec / 10 steps |\n",
      "| step: 414940 | gen_loss: 47.253 | mel_loss: 11.848 | 3.126 sec / 10 steps |\n",
      "| step: 414950 | gen_loss: 46.644 | mel_loss: 11.197 | 3.098 sec / 10 steps |\n",
      "| step: 414960 | gen_loss: 42.895 | mel_loss: 9.770 | 3.333 sec / 10 steps |\n",
      "| step: 414970 | gen_loss: 49.349 | mel_loss: 11.867 | 3.494 sec / 10 steps |\n",
      "| step: 414980 | gen_loss: 46.701 | mel_loss: 11.895 | 2.973 sec / 10 steps |\n",
      "| step: 414990 | gen_loss: 46.628 | mel_loss: 11.247 | 3.530 sec / 10 steps |\n",
      "| step: 415000 | gen_loss: 41.011 | mel_loss: 8.902 | 3.623 sec / 10 steps |\n",
      "Validation mel_loss: 14.13349437713623\n",
      "| step: 415010 | gen_loss: 44.594 | mel_loss: 10.495 | 3.677 sec / 10 steps |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 415020 | gen_loss: 53.215 | mel_loss: 13.124 | 2.919 sec / 10 steps |\n",
      "| step: 415030 | gen_loss: 43.532 | mel_loss: 10.985 | 3.138 sec / 10 steps |\n",
      "| step: 415040 | gen_loss: 49.876 | mel_loss: 12.266 | 3.593 sec / 10 steps |\n",
      "| step: 415050 | gen_loss: 51.686 | mel_loss: 11.800 | 3.275 sec / 10 steps |\n",
      "| step: 415060 | gen_loss: 47.650 | mel_loss: 11.496 | 3.258 sec / 10 steps |\n",
      "| step: 415070 | gen_loss: 46.845 | mel_loss: 10.865 | 3.418 sec / 10 steps |\n",
      "| step: 415080 | gen_loss: 50.186 | mel_loss: 11.746 | 3.219 sec / 10 steps |\n",
      "| step: 415090 | gen_loss: 50.376 | mel_loss: 12.816 | 3.385 sec / 10 steps |\n",
      "| step: 415100 | gen_loss: 44.779 | mel_loss: 10.825 | 3.361 sec / 10 steps |\n",
      "Validation mel_loss: 14.23444652557373\n",
      "| step: 415110 | gen_loss: 49.132 | mel_loss: 12.488 | 3.045 sec / 10 steps |\n",
      "| step: 415120 | gen_loss: 43.345 | mel_loss: 9.341 | 3.202 sec / 10 steps |\n",
      "| step: 415130 | gen_loss: 36.038 | mel_loss: 8.696 | 3.439 sec / 10 steps |\n",
      "| step: 415140 | gen_loss: 48.688 | mel_loss: 12.080 | 3.044 sec / 10 steps |\n",
      "| step: 415150 | gen_loss: 48.689 | mel_loss: 12.325 | 3.331 sec / 10 steps |\n",
      "| step: 415160 | gen_loss: 48.092 | mel_loss: 12.688 | 3.159 sec / 10 steps |\n",
      "| step: 415170 | gen_loss: 42.731 | mel_loss: 9.905 | 3.021 sec / 10 steps |\n",
      "| step: 415180 | gen_loss: 47.700 | mel_loss: 10.790 | 3.284 sec / 10 steps |\n",
      "| step: 415190 | gen_loss: 47.803 | mel_loss: 11.377 | 3.019 sec / 10 steps |\n",
      "| step: 415200 | gen_loss: 52.419 | mel_loss: 13.102 | 2.935 sec / 10 steps |\n",
      "Validation mel_loss: 14.110479354858398\n",
      "| step: 415210 | gen_loss: 46.102 | mel_loss: 11.388 | 3.198 sec / 10 steps |\n",
      "| step: 415220 | gen_loss: 42.279 | mel_loss: 9.904 | 3.161 sec / 10 steps |\n",
      "| step: 415230 | gen_loss: 47.072 | mel_loss: 11.712 | 3.328 sec / 10 steps |\n",
      "| step: 415240 | gen_loss: 49.030 | mel_loss: 12.893 | 3.241 sec / 10 steps |\n",
      "| step: 415250 | gen_loss: 48.392 | mel_loss: 11.766 | 3.110 sec / 10 steps |\n",
      "|| Epoch: 553 ||\n",
      "| step: 415260 | gen_loss: 41.941 | mel_loss: 9.764 | 3.006 sec / 10 steps |\n",
      "| step: 415270 | gen_loss: 49.846 | mel_loss: 12.447 | 3.146 sec / 10 steps |\n",
      "| step: 415280 | gen_loss: 50.114 | mel_loss: 12.752 | 3.229 sec / 10 steps |\n",
      "| step: 415290 | gen_loss: 47.719 | mel_loss: 11.006 | 3.245 sec / 10 steps |\n",
      "| step: 415300 | gen_loss: 47.684 | mel_loss: 11.304 | 3.122 sec / 10 steps |\n",
      "Validation mel_loss: 14.138490676879883\n",
      "| step: 415310 | gen_loss: 44.035 | mel_loss: 10.124 | 3.573 sec / 10 steps |\n",
      "| step: 415320 | gen_loss: 41.500 | mel_loss: 10.400 | 3.689 sec / 10 steps |\n",
      "| step: 415330 | gen_loss: 46.365 | mel_loss: 10.628 | 3.217 sec / 10 steps |\n",
      "| step: 415340 | gen_loss: 51.817 | mel_loss: 13.548 | 3.405 sec / 10 steps |\n",
      "| step: 415350 | gen_loss: 49.717 | mel_loss: 12.017 | 3.258 sec / 10 steps |\n",
      "| step: 415360 | gen_loss: 50.059 | mel_loss: 12.498 | 3.203 sec / 10 steps |\n",
      "| step: 415370 | gen_loss: 49.138 | mel_loss: 11.428 | 3.114 sec / 10 steps |\n",
      "| step: 415380 | gen_loss: 51.868 | mel_loss: 13.099 | 3.179 sec / 10 steps |\n",
      "| step: 415390 | gen_loss: 47.314 | mel_loss: 11.063 | 3.129 sec / 10 steps |\n",
      "| step: 415400 | gen_loss: 42.794 | mel_loss: 9.961 | 3.425 sec / 10 steps |\n",
      "Validation mel_loss: 14.149789810180664\n",
      "| step: 415410 | gen_loss: 54.750 | mel_loss: 13.805 | 3.421 sec / 10 steps |\n",
      "| step: 415420 | gen_loss: 44.625 | mel_loss: 10.055 | 3.230 sec / 10 steps |\n",
      "| step: 415430 | gen_loss: 41.872 | mel_loss: 9.268 | 3.118 sec / 10 steps |\n",
      "| step: 415440 | gen_loss: 43.014 | mel_loss: 10.634 | 3.490 sec / 10 steps |\n",
      "| step: 415450 | gen_loss: 45.915 | mel_loss: 10.817 | 3.102 sec / 10 steps |\n",
      "| step: 415460 | gen_loss: 45.530 | mel_loss: 9.901 | 3.172 sec / 10 steps |\n",
      "| step: 415470 | gen_loss: 51.268 | mel_loss: 13.027 | 3.277 sec / 10 steps |\n",
      "| step: 415480 | gen_loss: 47.428 | mel_loss: 11.077 | 3.609 sec / 10 steps |\n",
      "| step: 415490 | gen_loss: 44.234 | mel_loss: 11.094 | 3.126 sec / 10 steps |\n",
      "| step: 415500 | gen_loss: 44.142 | mel_loss: 10.023 | 3.615 sec / 10 steps |\n",
      "Validation mel_loss: 14.164085388183594\n",
      "| step: 415510 | gen_loss: 49.653 | mel_loss: 12.407 | 3.171 sec / 10 steps |\n",
      "| step: 415520 | gen_loss: 40.841 | mel_loss: 8.865 | 3.653 sec / 10 steps |\n",
      "| step: 415530 | gen_loss: 41.731 | mel_loss: 10.312 | 3.219 sec / 10 steps |\n",
      "| step: 415540 | gen_loss: 44.740 | mel_loss: 10.028 | 3.211 sec / 10 steps |\n",
      "| step: 415550 | gen_loss: 43.531 | mel_loss: 10.243 | 3.643 sec / 10 steps |\n",
      "| step: 415560 | gen_loss: 44.237 | mel_loss: 10.011 | 3.249 sec / 10 steps |\n",
      "| step: 415570 | gen_loss: 47.806 | mel_loss: 10.809 | 3.301 sec / 10 steps |\n",
      "| step: 415580 | gen_loss: 46.915 | mel_loss: 11.217 | 3.019 sec / 10 steps |\n",
      "| step: 415590 | gen_loss: 46.335 | mel_loss: 11.077 | 3.249 sec / 10 steps |\n",
      "| step: 415600 | gen_loss: 49.728 | mel_loss: 12.910 | 3.331 sec / 10 steps |\n",
      "Validation mel_loss: 14.227984428405762\n",
      "| step: 415610 | gen_loss: 45.940 | mel_loss: 11.056 | 3.684 sec / 10 steps |\n",
      "| step: 415620 | gen_loss: 44.538 | mel_loss: 10.743 | 3.333 sec / 10 steps |\n",
      "| step: 415630 | gen_loss: 43.856 | mel_loss: 9.891 | 3.173 sec / 10 steps |\n",
      "| step: 415640 | gen_loss: 38.651 | mel_loss: 10.126 | 3.704 sec / 10 steps |\n",
      "| step: 415650 | gen_loss: 44.581 | mel_loss: 11.566 | 3.184 sec / 10 steps |\n",
      "| step: 415660 | gen_loss: 42.782 | mel_loss: 9.154 | 3.526 sec / 10 steps |\n",
      "| step: 415670 | gen_loss: 41.215 | mel_loss: 10.165 | 3.342 sec / 10 steps |\n",
      "| step: 415680 | gen_loss: 40.934 | mel_loss: 10.178 | 3.356 sec / 10 steps |\n",
      "| step: 415690 | gen_loss: 48.855 | mel_loss: 11.659 | 2.998 sec / 10 steps |\n",
      "| step: 415700 | gen_loss: 52.174 | mel_loss: 13.275 | 3.027 sec / 10 steps |\n",
      "Validation mel_loss: 14.201147079467773\n",
      "| step: 415710 | gen_loss: 52.458 | mel_loss: 12.506 | 3.310 sec / 10 steps |\n",
      "| step: 415720 | gen_loss: 40.618 | mel_loss: 9.167 | 3.279 sec / 10 steps |\n",
      "| step: 415730 | gen_loss: 42.057 | mel_loss: 9.431 | 3.091 sec / 10 steps |\n",
      "| step: 415740 | gen_loss: 49.487 | mel_loss: 11.843 | 3.211 sec / 10 steps |\n",
      "| step: 415750 | gen_loss: 45.553 | mel_loss: 10.489 | 3.524 sec / 10 steps |\n",
      "| step: 415760 | gen_loss: 48.326 | mel_loss: 10.538 | 3.718 sec / 10 steps |\n",
      "| step: 415770 | gen_loss: 42.377 | mel_loss: 9.933 | 3.588 sec / 10 steps |\n",
      "| step: 415780 | gen_loss: 47.697 | mel_loss: 11.550 | 3.466 sec / 10 steps |\n",
      "| step: 415790 | gen_loss: 50.667 | mel_loss: 13.070 | 3.377 sec / 10 steps |\n",
      "| step: 415800 | gen_loss: 48.974 | mel_loss: 11.614 | 3.386 sec / 10 steps |\n",
      "Validation mel_loss: 14.182012557983398\n",
      "| step: 415810 | gen_loss: 44.856 | mel_loss: 9.937 | 3.472 sec / 10 steps |\n",
      "| step: 415820 | gen_loss: 45.747 | mel_loss: 11.178 | 3.561 sec / 10 steps |\n",
      "| step: 415830 | gen_loss: 43.846 | mel_loss: 10.142 | 3.413 sec / 10 steps |\n",
      "| step: 415840 | gen_loss: 43.968 | mel_loss: 10.389 | 3.086 sec / 10 steps |\n",
      "| step: 415850 | gen_loss: 45.413 | mel_loss: 10.123 | 3.102 sec / 10 steps |\n",
      "| step: 415860 | gen_loss: 42.894 | mel_loss: 10.279 | 3.407 sec / 10 steps |\n",
      "| step: 415870 | gen_loss: 47.981 | mel_loss: 10.974 | 3.354 sec / 10 steps |\n",
      "| step: 415880 | gen_loss: 47.694 | mel_loss: 12.141 | 3.474 sec / 10 steps |\n",
      "| step: 415890 | gen_loss: 52.970 | mel_loss: 12.730 | 3.222 sec / 10 steps |\n",
      "| step: 415900 | gen_loss: 41.990 | mel_loss: 9.056 | 3.315 sec / 10 steps |\n",
      "Validation mel_loss: 14.226659774780273\n",
      "| step: 415910 | gen_loss: 50.534 | mel_loss: 13.138 | 3.116 sec / 10 steps |\n",
      "| step: 415920 | gen_loss: 51.455 | mel_loss: 11.989 | 3.177 sec / 10 steps |\n",
      "| step: 415930 | gen_loss: 49.137 | mel_loss: 12.473 | 3.238 sec / 10 steps |\n",
      "| step: 415940 | gen_loss: 46.590 | mel_loss: 11.338 | 3.393 sec / 10 steps |\n",
      "| step: 415950 | gen_loss: 48.947 | mel_loss: 11.734 | 3.839 sec / 10 steps |\n",
      "| step: 415960 | gen_loss: 45.998 | mel_loss: 11.638 | 3.051 sec / 10 steps |\n",
      "| step: 415970 | gen_loss: 44.286 | mel_loss: 10.331 | 3.039 sec / 10 steps |\n",
      "| step: 415980 | gen_loss: 47.518 | mel_loss: 11.789 | 3.886 sec / 10 steps |\n",
      "| step: 415990 | gen_loss: 44.282 | mel_loss: 9.794 | 3.372 sec / 10 steps |\n",
      "| step: 416000 | gen_loss: 41.386 | mel_loss: 10.261 | 3.173 sec / 10 steps |\n",
      "Validation mel_loss: 14.249456405639648\n",
      "|| Epoch: 554 ||\n",
      "| step: 416010 | gen_loss: 46.029 | mel_loss: 9.997 | 3.480 sec / 10 steps |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 416020 | gen_loss: 41.917 | mel_loss: 8.818 | 3.951 sec / 10 steps |\n",
      "| step: 416030 | gen_loss: 55.198 | mel_loss: 13.157 | 3.085 sec / 10 steps |\n",
      "| step: 416040 | gen_loss: 49.892 | mel_loss: 12.990 | 3.078 sec / 10 steps |\n",
      "| step: 416050 | gen_loss: 47.410 | mel_loss: 11.814 | 2.921 sec / 10 steps |\n",
      "| step: 416060 | gen_loss: 43.819 | mel_loss: 9.468 | 3.313 sec / 10 steps |\n",
      "| step: 416070 | gen_loss: 46.723 | mel_loss: 11.114 | 3.018 sec / 10 steps |\n",
      "| step: 416080 | gen_loss: 42.826 | mel_loss: 9.159 | 3.534 sec / 10 steps |\n",
      "| step: 416090 | gen_loss: 43.123 | mel_loss: 9.953 | 3.468 sec / 10 steps |\n",
      "| step: 416100 | gen_loss: 49.887 | mel_loss: 13.073 | 4.328 sec / 10 steps |\n",
      "Validation mel_loss: 14.160025596618652\n",
      "| step: 416110 | gen_loss: 44.142 | mel_loss: 10.107 | 3.398 sec / 10 steps |\n",
      "| step: 416120 | gen_loss: 49.123 | mel_loss: 11.937 | 3.163 sec / 10 steps |\n",
      "| step: 416130 | gen_loss: 43.193 | mel_loss: 11.679 | 3.200 sec / 10 steps |\n",
      "| step: 416140 | gen_loss: 48.929 | mel_loss: 12.458 | 3.093 sec / 10 steps |\n",
      "| step: 416150 | gen_loss: 50.905 | mel_loss: 12.634 | 3.186 sec / 10 steps |\n",
      "| step: 416160 | gen_loss: 44.279 | mel_loss: 9.563 | 3.615 sec / 10 steps |\n",
      "| step: 416170 | gen_loss: 47.494 | mel_loss: 12.106 | 3.383 sec / 10 steps |\n",
      "| step: 416180 | gen_loss: 51.885 | mel_loss: 12.105 | 3.096 sec / 10 steps |\n",
      "| step: 416190 | gen_loss: 51.569 | mel_loss: 12.752 | 3.293 sec / 10 steps |\n",
      "| step: 416200 | gen_loss: 48.442 | mel_loss: 12.490 | 3.091 sec / 10 steps |\n",
      "Validation mel_loss: 14.299715042114258\n",
      "| step: 416210 | gen_loss: 48.983 | mel_loss: 11.222 | 3.897 sec / 10 steps |\n",
      "| step: 416220 | gen_loss: 42.051 | mel_loss: 9.018 | 3.359 sec / 10 steps |\n",
      "| step: 416230 | gen_loss: 52.299 | mel_loss: 12.473 | 3.003 sec / 10 steps |\n",
      "| step: 416240 | gen_loss: 52.708 | mel_loss: 12.099 | 2.890 sec / 10 steps |\n",
      "| step: 416250 | gen_loss: 45.823 | mel_loss: 10.750 | 3.200 sec / 10 steps |\n",
      "| step: 416260 | gen_loss: 46.725 | mel_loss: 10.748 | 3.461 sec / 10 steps |\n",
      "| step: 416270 | gen_loss: 43.747 | mel_loss: 10.664 | 3.355 sec / 10 steps |\n",
      "| step: 416280 | gen_loss: 46.713 | mel_loss: 11.338 | 3.690 sec / 10 steps |\n",
      "| step: 416290 | gen_loss: 51.498 | mel_loss: 13.141 | 3.031 sec / 10 steps |\n",
      "| step: 416300 | gen_loss: 44.614 | mel_loss: 10.477 | 3.261 sec / 10 steps |\n",
      "Validation mel_loss: 14.036588668823242\n",
      "| step: 416310 | gen_loss: 46.918 | mel_loss: 11.926 | 3.437 sec / 10 steps |\n",
      "| step: 416320 | gen_loss: 41.180 | mel_loss: 9.830 | 2.936 sec / 10 steps |\n",
      "| step: 416330 | gen_loss: 45.630 | mel_loss: 11.741 | 3.102 sec / 10 steps |\n",
      "| step: 416340 | gen_loss: 46.535 | mel_loss: 11.447 | 3.121 sec / 10 steps |\n",
      "| step: 416350 | gen_loss: 48.491 | mel_loss: 11.190 | 3.238 sec / 10 steps |\n",
      "| step: 416360 | gen_loss: 46.342 | mel_loss: 10.615 | 3.425 sec / 10 steps |\n",
      "| step: 416370 | gen_loss: 46.154 | mel_loss: 11.049 | 3.331 sec / 10 steps |\n",
      "| step: 416380 | gen_loss: 43.652 | mel_loss: 9.608 | 3.421 sec / 10 steps |\n",
      "| step: 416390 | gen_loss: 45.762 | mel_loss: 10.629 | 3.128 sec / 10 steps |\n",
      "| step: 416400 | gen_loss: 44.663 | mel_loss: 9.660 | 3.431 sec / 10 steps |\n",
      "Validation mel_loss: 14.187565803527832\n",
      "| step: 416410 | gen_loss: 49.822 | mel_loss: 12.221 | 3.336 sec / 10 steps |\n",
      "| step: 416420 | gen_loss: 47.101 | mel_loss: 10.878 | 3.316 sec / 10 steps |\n",
      "| step: 416430 | gen_loss: 48.631 | mel_loss: 11.787 | 3.251 sec / 10 steps |\n",
      "| step: 416440 | gen_loss: 46.913 | mel_loss: 10.767 | 3.085 sec / 10 steps |\n",
      "| step: 416450 | gen_loss: 47.180 | mel_loss: 11.104 | 3.018 sec / 10 steps |\n",
      "| step: 416460 | gen_loss: 40.262 | mel_loss: 8.697 | 3.246 sec / 10 steps |\n",
      "| step: 416470 | gen_loss: 42.769 | mel_loss: 9.823 | 3.405 sec / 10 steps |\n",
      "| step: 416480 | gen_loss: 47.748 | mel_loss: 11.704 | 3.296 sec / 10 steps |\n",
      "| step: 416490 | gen_loss: 49.013 | mel_loss: 12.863 | 3.135 sec / 10 steps |\n",
      "| step: 416500 | gen_loss: 46.560 | mel_loss: 10.964 | 3.030 sec / 10 steps |\n",
      "Validation mel_loss: 14.118409156799316\n",
      "| step: 416510 | gen_loss: 45.919 | mel_loss: 9.946 | 3.539 sec / 10 steps |\n",
      "| step: 416520 | gen_loss: 40.832 | mel_loss: 8.941 | 3.360 sec / 10 steps |\n",
      "| step: 416530 | gen_loss: 44.082 | mel_loss: 10.642 | 3.235 sec / 10 steps |\n",
      "| step: 416540 | gen_loss: 47.012 | mel_loss: 11.306 | 3.240 sec / 10 steps |\n",
      "| step: 416550 | gen_loss: 47.302 | mel_loss: 12.538 | 3.306 sec / 10 steps |\n",
      "| step: 416560 | gen_loss: 48.529 | mel_loss: 11.917 | 3.380 sec / 10 steps |\n",
      "| step: 416570 | gen_loss: 47.745 | mel_loss: 11.635 | 3.420 sec / 10 steps |\n",
      "| step: 416580 | gen_loss: 46.323 | mel_loss: 10.805 | 3.000 sec / 10 steps |\n",
      "| step: 416590 | gen_loss: 42.564 | mel_loss: 9.310 | 3.294 sec / 10 steps |\n",
      "| step: 416600 | gen_loss: 49.980 | mel_loss: 12.802 | 3.085 sec / 10 steps |\n",
      "Validation mel_loss: 14.078840255737305\n",
      "| step: 416610 | gen_loss: 50.311 | mel_loss: 11.623 | 3.247 sec / 10 steps |\n",
      "| step: 416620 | gen_loss: 47.682 | mel_loss: 11.455 | 3.300 sec / 10 steps |\n",
      "| step: 416630 | gen_loss: 50.117 | mel_loss: 11.929 | 2.769 sec / 10 steps |\n",
      "| step: 416640 | gen_loss: 44.483 | mel_loss: 10.750 | 2.910 sec / 10 steps |\n",
      "| step: 416650 | gen_loss: 48.328 | mel_loss: 11.173 | 3.373 sec / 10 steps |\n",
      "| step: 416660 | gen_loss: 48.408 | mel_loss: 11.450 | 3.308 sec / 10 steps |\n",
      "| step: 416670 | gen_loss: 44.043 | mel_loss: 10.551 | 3.163 sec / 10 steps |\n",
      "| step: 416680 | gen_loss: 49.800 | mel_loss: 11.700 | 3.158 sec / 10 steps |\n",
      "| step: 416690 | gen_loss: 48.155 | mel_loss: 11.585 | 3.116 sec / 10 steps |\n",
      "| step: 416700 | gen_loss: 44.737 | mel_loss: 11.679 | 2.925 sec / 10 steps |\n",
      "Validation mel_loss: 14.122162818908691\n",
      "| step: 416710 | gen_loss: 43.998 | mel_loss: 10.776 | 3.556 sec / 10 steps |\n",
      "| step: 416720 | gen_loss: 41.771 | mel_loss: 9.279 | 3.572 sec / 10 steps |\n",
      "| step: 416730 | gen_loss: 50.782 | mel_loss: 13.061 | 3.348 sec / 10 steps |\n",
      "| step: 416740 | gen_loss: 43.770 | mel_loss: 9.935 | 3.301 sec / 10 steps |\n",
      "| step: 416750 | gen_loss: 46.090 | mel_loss: 10.862 | 3.237 sec / 10 steps |\n",
      "|| Epoch: 555 ||\n",
      "| step: 416760 | gen_loss: 51.519 | mel_loss: 13.164 | 3.343 sec / 10 steps |\n",
      "| step: 416770 | gen_loss: 44.184 | mel_loss: 11.062 | 3.229 sec / 10 steps |\n",
      "| step: 416780 | gen_loss: 47.402 | mel_loss: 11.946 | 3.412 sec / 10 steps |\n",
      "| step: 416790 | gen_loss: 38.345 | mel_loss: 9.846 | 3.095 sec / 10 steps |\n",
      "| step: 416800 | gen_loss: 46.461 | mel_loss: 12.263 | 3.237 sec / 10 steps |\n",
      "Validation mel_loss: 14.272149085998535\n",
      "| step: 416810 | gen_loss: 38.553 | mel_loss: 9.088 | 3.372 sec / 10 steps |\n",
      "| step: 416820 | gen_loss: 47.437 | mel_loss: 10.863 | 3.114 sec / 10 steps |\n",
      "| step: 416830 | gen_loss: 44.694 | mel_loss: 10.717 | 3.448 sec / 10 steps |\n",
      "| step: 416840 | gen_loss: 46.050 | mel_loss: 10.674 | 3.136 sec / 10 steps |\n",
      "| step: 416850 | gen_loss: 47.447 | mel_loss: 11.371 | 3.400 sec / 10 steps |\n",
      "| step: 416860 | gen_loss: 48.198 | mel_loss: 11.935 | 3.496 sec / 10 steps |\n",
      "| step: 416870 | gen_loss: 49.902 | mel_loss: 11.911 | 2.984 sec / 10 steps |\n",
      "| step: 416880 | gen_loss: 52.920 | mel_loss: 12.958 | 3.184 sec / 10 steps |\n",
      "| step: 416890 | gen_loss: 46.046 | mel_loss: 9.672 | 3.157 sec / 10 steps |\n",
      "| step: 416900 | gen_loss: 51.138 | mel_loss: 12.701 | 3.341 sec / 10 steps |\n",
      "Validation mel_loss: 14.090460777282715\n",
      "| step: 416910 | gen_loss: 39.120 | mel_loss: 9.742 | 3.578 sec / 10 steps |\n",
      "| step: 416920 | gen_loss: 47.303 | mel_loss: 11.923 | 3.622 sec / 10 steps |\n",
      "| step: 416930 | gen_loss: 40.017 | mel_loss: 9.874 | 3.238 sec / 10 steps |\n",
      "| step: 416940 | gen_loss: 49.525 | mel_loss: 11.821 | 3.180 sec / 10 steps |\n",
      "| step: 416950 | gen_loss: 43.222 | mel_loss: 9.907 | 3.361 sec / 10 steps |\n",
      "| step: 416960 | gen_loss: 50.001 | mel_loss: 11.890 | 3.054 sec / 10 steps |\n",
      "| step: 416970 | gen_loss: 46.572 | mel_loss: 10.826 | 3.169 sec / 10 steps |\n",
      "| step: 416980 | gen_loss: 49.086 | mel_loss: 11.344 | 2.853 sec / 10 steps |\n",
      "| step: 416990 | gen_loss: 44.567 | mel_loss: 11.353 | 3.177 sec / 10 steps |\n",
      "| step: 417000 | gen_loss: 46.526 | mel_loss: 10.165 | 3.272 sec / 10 steps |\n",
      "Validation mel_loss: 14.14748477935791\n",
      "| step: 417010 | gen_loss: 50.556 | mel_loss: 12.054 | 3.173 sec / 10 steps |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 417020 | gen_loss: 42.797 | mel_loss: 9.127 | 3.659 sec / 10 steps |\n",
      "| step: 417030 | gen_loss: 47.957 | mel_loss: 12.212 | 2.913 sec / 10 steps |\n",
      "| step: 417040 | gen_loss: 47.161 | mel_loss: 10.866 | 3.167 sec / 10 steps |\n",
      "| step: 417050 | gen_loss: 50.406 | mel_loss: 12.699 | 3.370 sec / 10 steps |\n",
      "| step: 417060 | gen_loss: 51.047 | mel_loss: 13.511 | 3.032 sec / 10 steps |\n",
      "| step: 417070 | gen_loss: 49.552 | mel_loss: 12.855 | 3.321 sec / 10 steps |\n",
      "| step: 417080 | gen_loss: 46.041 | mel_loss: 10.838 | 3.050 sec / 10 steps |\n",
      "| step: 417090 | gen_loss: 41.781 | mel_loss: 9.232 | 3.038 sec / 10 steps |\n",
      "| step: 417100 | gen_loss: 54.658 | mel_loss: 13.361 | 3.301 sec / 10 steps |\n",
      "Validation mel_loss: 14.29780387878418\n",
      "| step: 417110 | gen_loss: 43.688 | mel_loss: 10.095 | 3.385 sec / 10 steps |\n",
      "| step: 417120 | gen_loss: 52.508 | mel_loss: 12.749 | 3.016 sec / 10 steps |\n",
      "| step: 417130 | gen_loss: 46.783 | mel_loss: 11.401 | 3.246 sec / 10 steps |\n",
      "| step: 417140 | gen_loss: 48.353 | mel_loss: 11.642 | 3.515 sec / 10 steps |\n",
      "| step: 417150 | gen_loss: 43.918 | mel_loss: 10.880 | 2.928 sec / 10 steps |\n",
      "| step: 417160 | gen_loss: 47.387 | mel_loss: 11.598 | 3.314 sec / 10 steps |\n",
      "| step: 417170 | gen_loss: 47.949 | mel_loss: 11.398 | 3.394 sec / 10 steps |\n",
      "| step: 417180 | gen_loss: 46.708 | mel_loss: 12.065 | 3.323 sec / 10 steps |\n",
      "| step: 417190 | gen_loss: 42.459 | mel_loss: 9.722 | 3.359 sec / 10 steps |\n",
      "| step: 417200 | gen_loss: 47.450 | mel_loss: 10.351 | 3.276 sec / 10 steps |\n",
      "Validation mel_loss: 14.072737693786621\n",
      "| step: 417210 | gen_loss: 46.086 | mel_loss: 10.239 | 3.254 sec / 10 steps |\n",
      "| step: 417220 | gen_loss: 42.376 | mel_loss: 10.254 | 3.065 sec / 10 steps |\n",
      "| step: 417230 | gen_loss: 40.419 | mel_loss: 8.355 | 3.142 sec / 10 steps |\n",
      "| step: 417240 | gen_loss: 40.712 | mel_loss: 9.117 | 3.280 sec / 10 steps |\n",
      "| step: 417250 | gen_loss: 47.446 | mel_loss: 12.005 | 3.234 sec / 10 steps |\n",
      "| step: 417260 | gen_loss: 50.646 | mel_loss: 12.240 | 3.161 sec / 10 steps |\n",
      "| step: 417270 | gen_loss: 39.953 | mel_loss: 9.289 | 3.581 sec / 10 steps |\n",
      "| step: 417280 | gen_loss: 40.219 | mel_loss: 9.229 | 3.248 sec / 10 steps |\n",
      "| step: 417290 | gen_loss: 46.543 | mel_loss: 11.102 | 3.598 sec / 10 steps |\n",
      "| step: 417300 | gen_loss: 49.196 | mel_loss: 13.292 | 3.026 sec / 10 steps |\n",
      "Validation mel_loss: 14.074565887451172\n",
      "| step: 417310 | gen_loss: 46.665 | mel_loss: 11.199 | 3.683 sec / 10 steps |\n",
      "| step: 417320 | gen_loss: 49.344 | mel_loss: 12.767 | 3.437 sec / 10 steps |\n",
      "| step: 417330 | gen_loss: 42.628 | mel_loss: 10.068 | 3.400 sec / 10 steps |\n",
      "| step: 417340 | gen_loss: 45.116 | mel_loss: 10.362 | 3.150 sec / 10 steps |\n",
      "| step: 417350 | gen_loss: 41.175 | mel_loss: 9.091 | 3.512 sec / 10 steps |\n",
      "| step: 417360 | gen_loss: 45.937 | mel_loss: 11.128 | 3.508 sec / 10 steps |\n",
      "| step: 417370 | gen_loss: 37.554 | mel_loss: 8.799 | 3.066 sec / 10 steps |\n",
      "| step: 417380 | gen_loss: 41.225 | mel_loss: 9.860 | 3.368 sec / 10 steps |\n",
      "| step: 417390 | gen_loss: 43.647 | mel_loss: 9.627 | 3.387 sec / 10 steps |\n",
      "| step: 417400 | gen_loss: 42.010 | mel_loss: 9.779 | 3.694 sec / 10 steps |\n",
      "Validation mel_loss: 14.10700798034668\n",
      "| step: 417410 | gen_loss: 48.931 | mel_loss: 12.251 | 3.486 sec / 10 steps |\n",
      "| step: 417420 | gen_loss: 42.639 | mel_loss: 9.245 | 3.494 sec / 10 steps |\n",
      "| step: 417430 | gen_loss: 43.396 | mel_loss: 9.360 | 3.141 sec / 10 steps |\n",
      "| step: 417440 | gen_loss: 49.379 | mel_loss: 12.246 | 3.300 sec / 10 steps |\n",
      "| step: 417450 | gen_loss: 45.098 | mel_loss: 11.666 | 3.785 sec / 10 steps |\n",
      "| step: 417460 | gen_loss: 48.646 | mel_loss: 11.662 | 3.206 sec / 10 steps |\n",
      "| step: 417470 | gen_loss: 44.940 | mel_loss: 11.081 | 2.928 sec / 10 steps |\n",
      "| step: 417480 | gen_loss: 50.103 | mel_loss: 12.834 | 3.190 sec / 10 steps |\n",
      "| step: 417490 | gen_loss: 46.779 | mel_loss: 11.544 | 2.997 sec / 10 steps |\n",
      "| step: 417500 | gen_loss: 49.574 | mel_loss: 12.158 | 3.229 sec / 10 steps |\n",
      "Validation mel_loss: 14.198225975036621\n",
      "|| Epoch: 556 ||\n",
      "| step: 417510 | gen_loss: 50.576 | mel_loss: 12.419 | 3.582 sec / 10 steps |\n",
      "| step: 417520 | gen_loss: 51.088 | mel_loss: 11.897 | 3.176 sec / 10 steps |\n",
      "| step: 417530 | gen_loss: 51.265 | mel_loss: 12.592 | 3.316 sec / 10 steps |\n",
      "| step: 417540 | gen_loss: 47.969 | mel_loss: 12.320 | 3.105 sec / 10 steps |\n",
      "| step: 417550 | gen_loss: 47.728 | mel_loss: 12.045 | 3.324 sec / 10 steps |\n",
      "| step: 417560 | gen_loss: 42.510 | mel_loss: 10.636 | 3.339 sec / 10 steps |\n",
      "| step: 417570 | gen_loss: 46.205 | mel_loss: 11.057 | 2.974 sec / 10 steps |\n",
      "| step: 417580 | gen_loss: 44.763 | mel_loss: 10.919 | 3.454 sec / 10 steps |\n",
      "| step: 417590 | gen_loss: 45.153 | mel_loss: 11.004 | 2.987 sec / 10 steps |\n",
      "| step: 417600 | gen_loss: 46.417 | mel_loss: 10.610 | 3.270 sec / 10 steps |\n",
      "Validation mel_loss: 14.083471298217773\n",
      "| step: 417610 | gen_loss: 42.699 | mel_loss: 9.551 | 3.286 sec / 10 steps |\n",
      "| step: 417620 | gen_loss: 52.130 | mel_loss: 12.992 | 3.937 sec / 10 steps |\n",
      "| step: 417630 | gen_loss: 46.182 | mel_loss: 10.346 | 3.418 sec / 10 steps |\n",
      "| step: 417640 | gen_loss: 48.014 | mel_loss: 11.442 | 3.204 sec / 10 steps |\n",
      "| step: 417650 | gen_loss: 47.044 | mel_loss: 11.774 | 3.202 sec / 10 steps |\n",
      "| step: 417660 | gen_loss: 49.900 | mel_loss: 11.985 | 3.115 sec / 10 steps |\n",
      "| step: 417670 | gen_loss: 48.131 | mel_loss: 10.878 | 3.549 sec / 10 steps |\n",
      "| step: 417680 | gen_loss: 52.200 | mel_loss: 12.988 | 3.206 sec / 10 steps |\n",
      "| step: 417690 | gen_loss: 43.239 | mel_loss: 9.406 | 3.301 sec / 10 steps |\n",
      "| step: 417700 | gen_loss: 44.887 | mel_loss: 10.699 | 3.686 sec / 10 steps |\n",
      "Validation mel_loss: 14.249129295349121\n",
      "| step: 417710 | gen_loss: 44.422 | mel_loss: 10.135 | 3.505 sec / 10 steps |\n",
      "| step: 417720 | gen_loss: 51.001 | mel_loss: 11.921 | 3.402 sec / 10 steps |\n",
      "| step: 417730 | gen_loss: 42.827 | mel_loss: 10.466 | 3.583 sec / 10 steps |\n",
      "| step: 417740 | gen_loss: 47.135 | mel_loss: 10.879 | 3.407 sec / 10 steps |\n",
      "| step: 417750 | gen_loss: 45.812 | mel_loss: 11.091 | 3.662 sec / 10 steps |\n",
      "| step: 417760 | gen_loss: 45.380 | mel_loss: 10.370 | 3.297 sec / 10 steps |\n",
      "| step: 417770 | gen_loss: 43.996 | mel_loss: 9.995 | 3.328 sec / 10 steps |\n",
      "| step: 417780 | gen_loss: 50.855 | mel_loss: 12.440 | 3.427 sec / 10 steps |\n",
      "| step: 417790 | gen_loss: 46.580 | mel_loss: 11.777 | 3.226 sec / 10 steps |\n",
      "| step: 417800 | gen_loss: 44.511 | mel_loss: 10.585 | 3.425 sec / 10 steps |\n",
      "Validation mel_loss: 14.144949913024902\n",
      "| step: 417810 | gen_loss: 45.238 | mel_loss: 11.403 | 3.395 sec / 10 steps |\n",
      "| step: 417820 | gen_loss: 45.458 | mel_loss: 9.944 | 3.316 sec / 10 steps |\n",
      "| step: 417830 | gen_loss: 42.905 | mel_loss: 9.689 | 3.244 sec / 10 steps |\n",
      "| step: 417840 | gen_loss: 44.963 | mel_loss: 11.571 | 3.102 sec / 10 steps |\n",
      "| step: 417850 | gen_loss: 39.373 | mel_loss: 9.945 | 2.984 sec / 10 steps |\n",
      "| step: 417860 | gen_loss: 48.691 | mel_loss: 12.293 | 3.333 sec / 10 steps |\n",
      "| step: 417870 | gen_loss: 50.461 | mel_loss: 11.797 | 3.488 sec / 10 steps |\n",
      "| step: 417880 | gen_loss: 44.749 | mel_loss: 10.213 | 3.323 sec / 10 steps |\n",
      "| step: 417890 | gen_loss: 47.450 | mel_loss: 12.035 | 3.132 sec / 10 steps |\n",
      "| step: 417900 | gen_loss: 44.384 | mel_loss: 9.686 | 3.598 sec / 10 steps |\n",
      "Validation mel_loss: 14.404812812805176\n",
      "| step: 417910 | gen_loss: 47.886 | mel_loss: 11.088 | 3.471 sec / 10 steps |\n",
      "| step: 417920 | gen_loss: 43.253 | mel_loss: 10.401 | 3.021 sec / 10 steps |\n",
      "| step: 417930 | gen_loss: 50.124 | mel_loss: 13.045 | 3.577 sec / 10 steps |\n",
      "| step: 417940 | gen_loss: 36.877 | mel_loss: 8.979 | 3.236 sec / 10 steps |\n",
      "| step: 417950 | gen_loss: 42.952 | mel_loss: 10.240 | 3.590 sec / 10 steps |\n",
      "| step: 417960 | gen_loss: 49.387 | mel_loss: 12.109 | 2.901 sec / 10 steps |\n",
      "| step: 417970 | gen_loss: 47.636 | mel_loss: 11.670 | 3.296 sec / 10 steps |\n",
      "| step: 417980 | gen_loss: 48.972 | mel_loss: 12.196 | 3.261 sec / 10 steps |\n",
      "| step: 417990 | gen_loss: 47.879 | mel_loss: 10.946 | 3.046 sec / 10 steps |\n",
      "| step: 418000 | gen_loss: 49.553 | mel_loss: 11.714 | 3.073 sec / 10 steps |\n",
      "Validation mel_loss: 14.099806785583496\n",
      "| step: 418010 | gen_loss: 45.435 | mel_loss: 11.044 | 3.090 sec / 10 steps |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 418020 | gen_loss: 40.281 | mel_loss: 9.620 | 3.397 sec / 10 steps |\n",
      "| step: 418030 | gen_loss: 46.907 | mel_loss: 11.997 | 3.032 sec / 10 steps |\n",
      "| step: 418040 | gen_loss: 45.070 | mel_loss: 10.730 | 3.284 sec / 10 steps |\n",
      "| step: 418050 | gen_loss: 48.992 | mel_loss: 12.251 | 3.107 sec / 10 steps |\n",
      "| step: 418060 | gen_loss: 55.540 | mel_loss: 13.265 | 3.426 sec / 10 steps |\n",
      "| step: 418070 | gen_loss: 45.482 | mel_loss: 10.224 | 3.428 sec / 10 steps |\n",
      "| step: 418080 | gen_loss: 44.407 | mel_loss: 10.106 | 3.555 sec / 10 steps |\n",
      "| step: 418090 | gen_loss: 48.305 | mel_loss: 11.653 | 3.455 sec / 10 steps |\n",
      "| step: 418100 | gen_loss: 48.145 | mel_loss: 10.565 | 3.206 sec / 10 steps |\n",
      "Validation mel_loss: 14.19578742980957\n",
      "| step: 418110 | gen_loss: 39.569 | mel_loss: 8.574 | 3.810 sec / 10 steps |\n",
      "| step: 418120 | gen_loss: 43.452 | mel_loss: 10.093 | 3.204 sec / 10 steps |\n",
      "| step: 418130 | gen_loss: 46.382 | mel_loss: 11.486 | 3.470 sec / 10 steps |\n",
      "| step: 418140 | gen_loss: 50.137 | mel_loss: 13.177 | 2.976 sec / 10 steps |\n",
      "| step: 418150 | gen_loss: 47.633 | mel_loss: 11.358 | 3.649 sec / 10 steps |\n",
      "| step: 418160 | gen_loss: 40.028 | mel_loss: 8.623 | 3.264 sec / 10 steps |\n",
      "| step: 418170 | gen_loss: 48.054 | mel_loss: 12.020 | 3.212 sec / 10 steps |\n",
      "| step: 418180 | gen_loss: 47.943 | mel_loss: 11.087 | 3.483 sec / 10 steps |\n",
      "| step: 418190 | gen_loss: 42.650 | mel_loss: 9.690 | 3.411 sec / 10 steps |\n",
      "| step: 418200 | gen_loss: 47.538 | mel_loss: 10.905 | 3.224 sec / 10 steps |\n",
      "Validation mel_loss: 14.15606689453125\n",
      "| step: 418210 | gen_loss: 42.262 | mel_loss: 9.543 | 3.211 sec / 10 steps |\n",
      "| step: 418220 | gen_loss: 44.455 | mel_loss: 10.600 | 3.183 sec / 10 steps |\n",
      "| step: 418230 | gen_loss: 48.349 | mel_loss: 11.395 | 3.527 sec / 10 steps |\n",
      "| step: 418240 | gen_loss: 46.299 | mel_loss: 11.201 | 3.311 sec / 10 steps |\n",
      "| step: 418250 | gen_loss: 43.535 | mel_loss: 10.288 | 3.162 sec / 10 steps |\n",
      "|| Epoch: 557 ||\n",
      "| step: 418260 | gen_loss: 50.706 | mel_loss: 12.470 | 3.126 sec / 10 steps |\n",
      "| step: 418270 | gen_loss: 50.479 | mel_loss: 13.002 | 3.234 sec / 10 steps |\n",
      "| step: 418280 | gen_loss: 47.615 | mel_loss: 11.781 | 3.262 sec / 10 steps |\n",
      "| step: 418290 | gen_loss: 42.105 | mel_loss: 9.661 | 3.419 sec / 10 steps |\n",
      "| step: 418300 | gen_loss: 43.969 | mel_loss: 10.548 | 3.299 sec / 10 steps |\n",
      "Validation mel_loss: 14.154598236083984\n",
      "| step: 418310 | gen_loss: 48.471 | mel_loss: 12.122 | 3.535 sec / 10 steps |\n",
      "| step: 418320 | gen_loss: 45.054 | mel_loss: 10.380 | 3.172 sec / 10 steps |\n",
      "| step: 418330 | gen_loss: 47.677 | mel_loss: 11.603 | 3.352 sec / 10 steps |\n",
      "| step: 418340 | gen_loss: 45.574 | mel_loss: 10.624 | 3.257 sec / 10 steps |\n",
      "| step: 418350 | gen_loss: 44.555 | mel_loss: 10.325 | 3.135 sec / 10 steps |\n",
      "| step: 418360 | gen_loss: 45.025 | mel_loss: 10.315 | 3.028 sec / 10 steps |\n",
      "| step: 418370 | gen_loss: 45.103 | mel_loss: 10.544 | 3.562 sec / 10 steps |\n",
      "| step: 418380 | gen_loss: 45.463 | mel_loss: 10.241 | 3.706 sec / 10 steps |\n",
      "| step: 418390 | gen_loss: 41.397 | mel_loss: 8.965 | 3.551 sec / 10 steps |\n",
      "| step: 418400 | gen_loss: 49.409 | mel_loss: 11.839 | 3.480 sec / 10 steps |\n",
      "Validation mel_loss: 14.157585144042969\n",
      "| step: 418410 | gen_loss: 47.247 | mel_loss: 11.795 | 3.487 sec / 10 steps |\n",
      "| step: 418420 | gen_loss: 46.626 | mel_loss: 10.028 | 3.577 sec / 10 steps |\n",
      "| step: 418430 | gen_loss: 54.086 | mel_loss: 13.011 | 3.378 sec / 10 steps |\n",
      "| step: 418440 | gen_loss: 45.602 | mel_loss: 11.193 | 3.257 sec / 10 steps |\n",
      "| step: 418450 | gen_loss: 47.466 | mel_loss: 11.269 | 3.268 sec / 10 steps |\n",
      "| step: 418460 | gen_loss: 49.974 | mel_loss: 12.904 | 3.630 sec / 10 steps |\n",
      "| step: 418470 | gen_loss: 49.492 | mel_loss: 12.073 | 3.131 sec / 10 steps |\n",
      "| step: 418480 | gen_loss: 41.147 | mel_loss: 9.649 | 3.450 sec / 10 steps |\n",
      "| step: 418490 | gen_loss: 45.558 | mel_loss: 10.453 | 3.621 sec / 10 steps |\n",
      "| step: 418500 | gen_loss: 48.080 | mel_loss: 11.625 | 3.081 sec / 10 steps |\n",
      "Validation mel_loss: 14.24049186706543\n",
      "| step: 418510 | gen_loss: 49.334 | mel_loss: 12.405 | 3.338 sec / 10 steps |\n",
      "| step: 418520 | gen_loss: 44.168 | mel_loss: 9.906 | 2.973 sec / 10 steps |\n",
      "| step: 418530 | gen_loss: 48.506 | mel_loss: 11.350 | 3.400 sec / 10 steps |\n",
      "| step: 418540 | gen_loss: 45.391 | mel_loss: 10.449 | 3.740 sec / 10 steps |\n",
      "| step: 418550 | gen_loss: 49.334 | mel_loss: 11.690 | 3.042 sec / 10 steps |\n",
      "| step: 418560 | gen_loss: 47.305 | mel_loss: 10.815 | 3.219 sec / 10 steps |\n",
      "| step: 418570 | gen_loss: 47.596 | mel_loss: 11.068 | 2.966 sec / 10 steps |\n",
      "| step: 418580 | gen_loss: 44.482 | mel_loss: 10.085 | 3.403 sec / 10 steps |\n",
      "| step: 418590 | gen_loss: 45.559 | mel_loss: 11.974 | 2.961 sec / 10 steps |\n",
      "| step: 418600 | gen_loss: 47.877 | mel_loss: 11.615 | 2.978 sec / 10 steps |\n",
      "Validation mel_loss: 14.141672134399414\n",
      "| step: 418610 | gen_loss: 46.018 | mel_loss: 11.007 | 3.148 sec / 10 steps |\n",
      "| step: 418620 | gen_loss: 45.150 | mel_loss: 11.076 | 3.099 sec / 10 steps |\n",
      "| step: 418630 | gen_loss: 45.804 | mel_loss: 10.416 | 2.997 sec / 10 steps |\n",
      "| step: 418640 | gen_loss: 41.250 | mel_loss: 9.631 | 3.100 sec / 10 steps |\n",
      "| step: 418650 | gen_loss: 49.244 | mel_loss: 12.350 | 3.388 sec / 10 steps |\n",
      "| step: 418660 | gen_loss: 44.192 | mel_loss: 10.469 | 3.291 sec / 10 steps |\n",
      "| step: 418670 | gen_loss: 52.617 | mel_loss: 12.823 | 2.955 sec / 10 steps |\n",
      "| step: 418680 | gen_loss: 44.693 | mel_loss: 10.364 | 3.557 sec / 10 steps |\n",
      "| step: 418690 | gen_loss: 49.324 | mel_loss: 11.141 | 3.169 sec / 10 steps |\n",
      "| step: 418700 | gen_loss: 51.452 | mel_loss: 13.021 | 2.876 sec / 10 steps |\n",
      "Validation mel_loss: 14.178376197814941\n",
      "| step: 418710 | gen_loss: 47.678 | mel_loss: 11.224 | 3.133 sec / 10 steps |\n",
      "| step: 418720 | gen_loss: 48.832 | mel_loss: 11.666 | 3.398 sec / 10 steps |\n",
      "| step: 418730 | gen_loss: 46.339 | mel_loss: 11.093 | 3.217 sec / 10 steps |\n",
      "| step: 418740 | gen_loss: 45.660 | mel_loss: 10.240 | 3.770 sec / 10 steps |\n",
      "| step: 418750 | gen_loss: 45.455 | mel_loss: 10.577 | 3.267 sec / 10 steps |\n",
      "| step: 418760 | gen_loss: 44.739 | mel_loss: 10.260 | 3.320 sec / 10 steps |\n",
      "| step: 418770 | gen_loss: 44.255 | mel_loss: 10.291 | 3.448 sec / 10 steps |\n",
      "| step: 418780 | gen_loss: 47.671 | mel_loss: 11.125 | 3.245 sec / 10 steps |\n",
      "| step: 418790 | gen_loss: 46.602 | mel_loss: 11.404 | 3.217 sec / 10 steps |\n",
      "| step: 418800 | gen_loss: 48.428 | mel_loss: 12.448 | 3.433 sec / 10 steps |\n",
      "Validation mel_loss: 14.07327938079834\n",
      "| step: 418810 | gen_loss: 50.107 | mel_loss: 12.935 | 3.239 sec / 10 steps |\n",
      "| step: 418820 | gen_loss: 49.670 | mel_loss: 11.659 | 3.168 sec / 10 steps |\n",
      "| step: 418830 | gen_loss: 47.464 | mel_loss: 11.694 | 3.130 sec / 10 steps |\n",
      "| step: 418840 | gen_loss: 37.000 | mel_loss: 8.822 | 3.677 sec / 10 steps |\n",
      "| step: 418850 | gen_loss: 44.302 | mel_loss: 10.712 | 3.430 sec / 10 steps |\n",
      "| step: 418860 | gen_loss: 50.292 | mel_loss: 12.906 | 3.316 sec / 10 steps |\n",
      "| step: 418870 | gen_loss: 48.544 | mel_loss: 11.184 | 3.257 sec / 10 steps |\n",
      "| step: 418880 | gen_loss: 49.106 | mel_loss: 12.388 | 3.356 sec / 10 steps |\n",
      "| step: 418890 | gen_loss: 51.480 | mel_loss: 12.612 | 3.340 sec / 10 steps |\n",
      "| step: 418900 | gen_loss: 45.051 | mel_loss: 10.855 | 3.245 sec / 10 steps |\n",
      "Validation mel_loss: 14.122926712036133\n",
      "| step: 418910 | gen_loss: 47.603 | mel_loss: 10.873 | 3.644 sec / 10 steps |\n",
      "| step: 418920 | gen_loss: 46.428 | mel_loss: 10.283 | 3.524 sec / 10 steps |\n",
      "| step: 418930 | gen_loss: 49.812 | mel_loss: 12.435 | 3.871 sec / 10 steps |\n",
      "| step: 418940 | gen_loss: 46.857 | mel_loss: 10.697 | 3.115 sec / 10 steps |\n",
      "| step: 418950 | gen_loss: 48.917 | mel_loss: 12.791 | 2.932 sec / 10 steps |\n",
      "| step: 418960 | gen_loss: 48.153 | mel_loss: 11.345 | 3.331 sec / 10 steps |\n",
      "| step: 418970 | gen_loss: 46.548 | mel_loss: 10.846 | 3.290 sec / 10 steps |\n",
      "| step: 418980 | gen_loss: 48.415 | mel_loss: 12.309 | 3.072 sec / 10 steps |\n",
      "| step: 418990 | gen_loss: 46.259 | mel_loss: 11.040 | 3.176 sec / 10 steps |\n",
      "| step: 419000 | gen_loss: 46.306 | mel_loss: 10.479 | 3.461 sec / 10 steps |\n",
      "Validation mel_loss: 14.206089973449707\n",
      "|| Epoch: 558 ||\n",
      "| step: 419010 | gen_loss: 49.798 | mel_loss: 11.596 | 3.574 sec / 10 steps |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 419020 | gen_loss: 54.358 | mel_loss: 13.464 | 3.029 sec / 10 steps |\n",
      "| step: 419030 | gen_loss: 47.513 | mel_loss: 10.954 | 3.236 sec / 10 steps |\n",
      "| step: 419040 | gen_loss: 51.195 | mel_loss: 13.024 | 3.496 sec / 10 steps |\n",
      "| step: 419050 | gen_loss: 47.939 | mel_loss: 11.104 | 3.150 sec / 10 steps |\n",
      "| step: 419060 | gen_loss: 48.402 | mel_loss: 11.435 | 3.277 sec / 10 steps |\n",
      "| step: 419070 | gen_loss: 44.038 | mel_loss: 9.844 | 3.264 sec / 10 steps |\n",
      "| step: 419080 | gen_loss: 39.262 | mel_loss: 8.537 | 3.006 sec / 10 steps |\n",
      "| step: 419090 | gen_loss: 49.626 | mel_loss: 11.947 | 3.344 sec / 10 steps |\n",
      "| step: 419100 | gen_loss: 47.988 | mel_loss: 10.952 | 3.513 sec / 10 steps |\n",
      "Validation mel_loss: 14.227827072143555\n",
      "| step: 419110 | gen_loss: 43.024 | mel_loss: 9.753 | 3.405 sec / 10 steps |\n",
      "| step: 419120 | gen_loss: 44.436 | mel_loss: 11.675 | 3.082 sec / 10 steps |\n",
      "| step: 419130 | gen_loss: 45.844 | mel_loss: 11.539 | 3.351 sec / 10 steps |\n",
      "| step: 419140 | gen_loss: 42.878 | mel_loss: 9.534 | 3.314 sec / 10 steps |\n",
      "| step: 419150 | gen_loss: 44.818 | mel_loss: 10.516 | 3.449 sec / 10 steps |\n",
      "| step: 419160 | gen_loss: 50.305 | mel_loss: 12.886 | 3.163 sec / 10 steps |\n",
      "| step: 419170 | gen_loss: 46.617 | mel_loss: 11.254 | 3.389 sec / 10 steps |\n",
      "| step: 419180 | gen_loss: 47.595 | mel_loss: 11.671 | 2.981 sec / 10 steps |\n",
      "| step: 419190 | gen_loss: 50.678 | mel_loss: 12.525 | 3.262 sec / 10 steps |\n",
      "| step: 419200 | gen_loss: 45.903 | mel_loss: 10.805 | 3.538 sec / 10 steps |\n",
      "Validation mel_loss: 14.108137130737305\n",
      "| step: 419210 | gen_loss: 47.983 | mel_loss: 11.729 | 3.477 sec / 10 steps |\n",
      "| step: 419220 | gen_loss: 45.549 | mel_loss: 11.511 | 3.548 sec / 10 steps |\n",
      "| step: 419230 | gen_loss: 46.490 | mel_loss: 10.305 | 3.511 sec / 10 steps |\n",
      "| step: 419240 | gen_loss: 46.268 | mel_loss: 9.922 | 3.313 sec / 10 steps |\n",
      "| step: 419250 | gen_loss: 46.343 | mel_loss: 11.144 | 3.339 sec / 10 steps |\n",
      "| step: 419260 | gen_loss: 48.839 | mel_loss: 11.729 | 3.040 sec / 10 steps |\n",
      "| step: 419270 | gen_loss: 44.477 | mel_loss: 10.069 | 3.905 sec / 10 steps |\n",
      "| step: 419280 | gen_loss: 49.582 | mel_loss: 12.235 | 3.175 sec / 10 steps |\n",
      "| step: 419290 | gen_loss: 48.958 | mel_loss: 11.984 | 3.432 sec / 10 steps |\n",
      "| step: 419300 | gen_loss: 47.785 | mel_loss: 11.372 | 3.178 sec / 10 steps |\n",
      "Validation mel_loss: 14.266105651855469\n",
      "| step: 419310 | gen_loss: 46.964 | mel_loss: 10.602 | 3.127 sec / 10 steps |\n",
      "| step: 419320 | gen_loss: 47.347 | mel_loss: 12.363 | 3.497 sec / 10 steps |\n",
      "| step: 419330 | gen_loss: 45.727 | mel_loss: 11.089 | 3.355 sec / 10 steps |\n",
      "| step: 419340 | gen_loss: 44.519 | mel_loss: 10.375 | 3.201 sec / 10 steps |\n",
      "| step: 419350 | gen_loss: 48.214 | mel_loss: 12.308 | 3.298 sec / 10 steps |\n",
      "| step: 419360 | gen_loss: 41.567 | mel_loss: 9.246 | 3.389 sec / 10 steps |\n",
      "| step: 419370 | gen_loss: 53.433 | mel_loss: 13.824 | 3.232 sec / 10 steps |\n",
      "| step: 419380 | gen_loss: 45.308 | mel_loss: 10.212 | 3.378 sec / 10 steps |\n",
      "| step: 419390 | gen_loss: 46.291 | mel_loss: 11.037 | 2.953 sec / 10 steps |\n",
      "| step: 419400 | gen_loss: 45.808 | mel_loss: 10.293 | 3.371 sec / 10 steps |\n",
      "Validation mel_loss: 14.091520309448242\n",
      "| step: 419410 | gen_loss: 39.611 | mel_loss: 8.877 | 3.459 sec / 10 steps |\n",
      "| step: 419420 | gen_loss: 44.605 | mel_loss: 10.232 | 3.283 sec / 10 steps |\n",
      "| step: 419430 | gen_loss: 45.838 | mel_loss: 11.846 | 3.422 sec / 10 steps |\n",
      "| step: 419440 | gen_loss: 48.639 | mel_loss: 11.069 | 3.772 sec / 10 steps |\n",
      "| step: 419450 | gen_loss: 50.587 | mel_loss: 13.064 | 3.358 sec / 10 steps |\n",
      "| step: 419460 | gen_loss: 46.951 | mel_loss: 11.063 | 3.417 sec / 10 steps |\n",
      "| step: 419470 | gen_loss: 47.379 | mel_loss: 12.060 | 3.275 sec / 10 steps |\n",
      "| step: 419480 | gen_loss: 45.191 | mel_loss: 10.293 | 3.361 sec / 10 steps |\n",
      "| step: 419490 | gen_loss: 46.172 | mel_loss: 10.412 | 3.389 sec / 10 steps |\n",
      "| step: 419500 | gen_loss: 46.383 | mel_loss: 11.271 | 3.241 sec / 10 steps |\n",
      "Validation mel_loss: 14.207563400268555\n",
      "| step: 419510 | gen_loss: 41.384 | mel_loss: 9.376 | 3.322 sec / 10 steps |\n",
      "| step: 419520 | gen_loss: 50.938 | mel_loss: 12.373 | 2.851 sec / 10 steps |\n",
      "| step: 419530 | gen_loss: 47.332 | mel_loss: 11.641 | 3.491 sec / 10 steps |\n",
      "| step: 419540 | gen_loss: 47.388 | mel_loss: 11.934 | 3.278 sec / 10 steps |\n",
      "| step: 419550 | gen_loss: 43.630 | mel_loss: 10.483 | 3.148 sec / 10 steps |\n",
      "| step: 419560 | gen_loss: 44.338 | mel_loss: 10.834 | 3.683 sec / 10 steps |\n",
      "| step: 419570 | gen_loss: 48.575 | mel_loss: 11.770 | 3.252 sec / 10 steps |\n",
      "| step: 419580 | gen_loss: 47.888 | mel_loss: 11.381 | 3.259 sec / 10 steps |\n",
      "| step: 419590 | gen_loss: 43.549 | mel_loss: 10.383 | 3.512 sec / 10 steps |\n",
      "| step: 419600 | gen_loss: 50.407 | mel_loss: 12.813 | 3.053 sec / 10 steps |\n",
      "Validation mel_loss: 14.104893684387207\n",
      "| step: 419610 | gen_loss: 45.905 | mel_loss: 10.937 | 3.685 sec / 10 steps |\n",
      "| step: 419620 | gen_loss: 41.198 | mel_loss: 8.871 | 3.549 sec / 10 steps |\n",
      "| step: 419630 | gen_loss: 46.213 | mel_loss: 10.626 | 3.307 sec / 10 steps |\n",
      "| step: 419640 | gen_loss: 44.651 | mel_loss: 11.029 | 3.474 sec / 10 steps |\n",
      "| step: 419650 | gen_loss: 43.504 | mel_loss: 9.301 | 3.050 sec / 10 steps |\n",
      "| step: 419660 | gen_loss: 46.784 | mel_loss: 10.914 | 3.303 sec / 10 steps |\n",
      "| step: 419670 | gen_loss: 42.986 | mel_loss: 9.124 | 3.654 sec / 10 steps |\n",
      "| step: 419680 | gen_loss: 53.115 | mel_loss: 13.550 | 3.236 sec / 10 steps |\n",
      "| step: 419690 | gen_loss: 48.912 | mel_loss: 13.174 | 3.192 sec / 10 steps |\n",
      "| step: 419700 | gen_loss: 49.451 | mel_loss: 13.285 | 2.997 sec / 10 steps |\n",
      "Validation mel_loss: 14.211126327514648\n",
      "| step: 419710 | gen_loss: 49.236 | mel_loss: 12.261 | 3.463 sec / 10 steps |\n",
      "| step: 419720 | gen_loss: 50.827 | mel_loss: 12.945 | 3.441 sec / 10 steps |\n",
      "| step: 419730 | gen_loss: 42.279 | mel_loss: 9.988 | 3.583 sec / 10 steps |\n",
      "| step: 419740 | gen_loss: 42.969 | mel_loss: 10.070 | 3.403 sec / 10 steps |\n",
      "| step: 419750 | gen_loss: 52.213 | mel_loss: 12.440 | 3.613 sec / 10 steps |\n",
      "|| Epoch: 559 ||\n",
      "| step: 419760 | gen_loss: 51.026 | mel_loss: 13.049 | 3.361 sec / 10 steps |\n",
      "| step: 419770 | gen_loss: 43.074 | mel_loss: 10.495 | 3.421 sec / 10 steps |\n",
      "| step: 419780 | gen_loss: 47.807 | mel_loss: 11.171 | 2.861 sec / 10 steps |\n",
      "| step: 419790 | gen_loss: 52.092 | mel_loss: 13.131 | 3.156 sec / 10 steps |\n",
      "| step: 419800 | gen_loss: 43.692 | mel_loss: 9.530 | 3.277 sec / 10 steps |\n",
      "Validation mel_loss: 14.117485046386719\n",
      "| step: 419810 | gen_loss: 44.536 | mel_loss: 10.677 | 3.539 sec / 10 steps |\n",
      "| step: 419820 | gen_loss: 44.705 | mel_loss: 10.559 | 3.471 sec / 10 steps |\n",
      "| step: 419830 | gen_loss: 46.377 | mel_loss: 10.555 | 3.342 sec / 10 steps |\n",
      "| step: 419840 | gen_loss: 40.302 | mel_loss: 8.379 | 3.363 sec / 10 steps |\n",
      "| step: 419850 | gen_loss: 48.123 | mel_loss: 11.964 | 2.986 sec / 10 steps |\n",
      "| step: 419860 | gen_loss: 44.597 | mel_loss: 10.488 | 3.472 sec / 10 steps |\n",
      "| step: 419870 | gen_loss: 46.684 | mel_loss: 11.154 | 3.314 sec / 10 steps |\n",
      "| step: 419880 | gen_loss: 43.784 | mel_loss: 10.843 | 3.538 sec / 10 steps |\n",
      "| step: 419890 | gen_loss: 41.851 | mel_loss: 9.702 | 3.249 sec / 10 steps |\n",
      "| step: 419900 | gen_loss: 46.662 | mel_loss: 10.911 | 3.424 sec / 10 steps |\n",
      "Validation mel_loss: 14.09214973449707\n",
      "| step: 419910 | gen_loss: 51.705 | mel_loss: 12.291 | 3.151 sec / 10 steps |\n",
      "| step: 419920 | gen_loss: 43.799 | mel_loss: 9.025 | 3.497 sec / 10 steps |\n",
      "| step: 419930 | gen_loss: 48.633 | mel_loss: 12.661 | 3.377 sec / 10 steps |\n",
      "| step: 419940 | gen_loss: 53.787 | mel_loss: 13.030 | 3.221 sec / 10 steps |\n",
      "| step: 419950 | gen_loss: 50.310 | mel_loss: 11.762 | 3.554 sec / 10 steps |\n",
      "| step: 419960 | gen_loss: 50.751 | mel_loss: 12.490 | 3.239 sec / 10 steps |\n",
      "| step: 419970 | gen_loss: 42.745 | mel_loss: 9.576 | 3.475 sec / 10 steps |\n",
      "| step: 419980 | gen_loss: 46.052 | mel_loss: 10.231 | 3.477 sec / 10 steps |\n",
      "| step: 419990 | gen_loss: 46.275 | mel_loss: 10.479 | 3.433 sec / 10 steps |\n",
      "| step: 420000 | gen_loss: 49.793 | mel_loss: 11.241 | 3.177 sec / 10 steps |\n",
      "Validation mel_loss: 14.296388626098633\n",
      "| step: 420010 | gen_loss: 45.942 | mel_loss: 10.445 | 3.180 sec / 10 steps |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 420020 | gen_loss: 47.745 | mel_loss: 11.058 | 3.207 sec / 10 steps |\n",
      "| step: 420030 | gen_loss: 48.074 | mel_loss: 12.349 | 3.028 sec / 10 steps |\n",
      "| step: 420040 | gen_loss: 47.637 | mel_loss: 11.861 | 2.967 sec / 10 steps |\n",
      "| step: 420050 | gen_loss: 44.737 | mel_loss: 10.125 | 3.133 sec / 10 steps |\n",
      "| step: 420060 | gen_loss: 46.810 | mel_loss: 11.218 | 3.326 sec / 10 steps |\n",
      "| step: 420070 | gen_loss: 47.052 | mel_loss: 10.856 | 3.013 sec / 10 steps |\n",
      "| step: 420080 | gen_loss: 49.224 | mel_loss: 12.307 | 2.963 sec / 10 steps |\n",
      "| step: 420090 | gen_loss: 50.954 | mel_loss: 13.239 | 3.076 sec / 10 steps |\n",
      "| step: 420100 | gen_loss: 46.706 | mel_loss: 10.797 | 3.058 sec / 10 steps |\n",
      "Validation mel_loss: 14.263126373291016\n",
      "| step: 420110 | gen_loss: 50.916 | mel_loss: 11.757 | 3.146 sec / 10 steps |\n",
      "| step: 420120 | gen_loss: 51.354 | mel_loss: 12.276 | 3.429 sec / 10 steps |\n",
      "| step: 420130 | gen_loss: 44.110 | mel_loss: 10.212 | 3.431 sec / 10 steps |\n",
      "| step: 420140 | gen_loss: 52.290 | mel_loss: 12.762 | 3.114 sec / 10 steps |\n",
      "| step: 420150 | gen_loss: 41.573 | mel_loss: 9.332 | 3.513 sec / 10 steps |\n",
      "| step: 420160 | gen_loss: 49.696 | mel_loss: 11.806 | 3.330 sec / 10 steps |\n",
      "| step: 420170 | gen_loss: 45.716 | mel_loss: 10.523 | 3.142 sec / 10 steps |\n",
      "| step: 420180 | gen_loss: 51.213 | mel_loss: 11.948 | 3.411 sec / 10 steps |\n",
      "| step: 420190 | gen_loss: 48.631 | mel_loss: 12.156 | 3.278 sec / 10 steps |\n",
      "| step: 420200 | gen_loss: 46.403 | mel_loss: 10.862 | 3.211 sec / 10 steps |\n",
      "Validation mel_loss: 14.256937026977539\n",
      "| step: 420210 | gen_loss: 40.203 | mel_loss: 9.835 | 3.682 sec / 10 steps |\n",
      "| step: 420220 | gen_loss: 47.361 | mel_loss: 11.333 | 3.508 sec / 10 steps |\n",
      "| step: 420230 | gen_loss: 46.600 | mel_loss: 11.523 | 3.119 sec / 10 steps |\n",
      "| step: 420240 | gen_loss: 42.972 | mel_loss: 10.410 | 3.447 sec / 10 steps |\n",
      "| step: 420250 | gen_loss: 47.290 | mel_loss: 10.524 | 3.349 sec / 10 steps |\n",
      "| step: 420260 | gen_loss: 33.818 | mel_loss: 8.731 | 3.450 sec / 10 steps |\n",
      "| step: 420270 | gen_loss: 50.893 | mel_loss: 12.465 | 3.273 sec / 10 steps |\n",
      "| step: 420280 | gen_loss: 49.999 | mel_loss: 12.963 | 3.268 sec / 10 steps |\n",
      "| step: 420290 | gen_loss: 48.167 | mel_loss: 11.878 | 3.004 sec / 10 steps |\n",
      "| step: 420300 | gen_loss: 44.458 | mel_loss: 10.299 | 3.309 sec / 10 steps |\n",
      "Validation mel_loss: 14.177591323852539\n",
      "| step: 420310 | gen_loss: 50.862 | mel_loss: 12.916 | 3.622 sec / 10 steps |\n",
      "| step: 420320 | gen_loss: 46.745 | mel_loss: 11.148 | 3.449 sec / 10 steps |\n",
      "| step: 420330 | gen_loss: 47.095 | mel_loss: 11.259 | 3.309 sec / 10 steps |\n",
      "| step: 420340 | gen_loss: 43.911 | mel_loss: 10.516 | 3.374 sec / 10 steps |\n",
      "| step: 420350 | gen_loss: 43.191 | mel_loss: 9.336 | 2.961 sec / 10 steps |\n",
      "| step: 420360 | gen_loss: 41.836 | mel_loss: 9.542 | 3.632 sec / 10 steps |\n",
      "| step: 420370 | gen_loss: 44.922 | mel_loss: 9.429 | 3.268 sec / 10 steps |\n",
      "| step: 420380 | gen_loss: 50.889 | mel_loss: 13.051 | 3.824 sec / 10 steps |\n",
      "| step: 420390 | gen_loss: 47.889 | mel_loss: 12.154 | 3.367 sec / 10 steps |\n",
      "| step: 420400 | gen_loss: 51.882 | mel_loss: 13.358 | 3.045 sec / 10 steps |\n",
      "Validation mel_loss: 14.150717735290527\n",
      "| step: 420410 | gen_loss: 49.437 | mel_loss: 12.967 | 3.311 sec / 10 steps |\n",
      "| step: 420420 | gen_loss: 50.486 | mel_loss: 12.804 | 3.062 sec / 10 steps |\n",
      "| step: 420430 | gen_loss: 50.074 | mel_loss: 12.682 | 3.676 sec / 10 steps |\n",
      "| step: 420440 | gen_loss: 48.244 | mel_loss: 11.497 | 3.356 sec / 10 steps |\n",
      "| step: 420450 | gen_loss: 46.060 | mel_loss: 9.730 | 3.391 sec / 10 steps |\n",
      "| step: 420460 | gen_loss: 49.740 | mel_loss: 12.295 | 3.523 sec / 10 steps |\n",
      "| step: 420470 | gen_loss: 45.735 | mel_loss: 10.340 | 3.353 sec / 10 steps |\n",
      "| step: 420480 | gen_loss: 45.123 | mel_loss: 11.422 | 3.245 sec / 10 steps |\n",
      "| step: 420490 | gen_loss: 48.607 | mel_loss: 12.896 | 3.686 sec / 10 steps |\n",
      "| step: 420500 | gen_loss: 42.336 | mel_loss: 11.126 | 3.366 sec / 10 steps |\n",
      "Validation mel_loss: 14.103324890136719\n",
      "|| Epoch: 560 ||\n",
      "| step: 420510 | gen_loss: 44.993 | mel_loss: 10.775 | 3.127 sec / 10 steps |\n",
      "| step: 420520 | gen_loss: 47.372 | mel_loss: 11.004 | 3.523 sec / 10 steps |\n",
      "| step: 420530 | gen_loss: 46.971 | mel_loss: 11.613 | 3.342 sec / 10 steps |\n",
      "| step: 420540 | gen_loss: 47.948 | mel_loss: 11.601 | 3.464 sec / 10 steps |\n",
      "| step: 420550 | gen_loss: 45.874 | mel_loss: 10.587 | 3.107 sec / 10 steps |\n",
      "| step: 420560 | gen_loss: 50.422 | mel_loss: 12.652 | 3.437 sec / 10 steps |\n",
      "| step: 420570 | gen_loss: 42.776 | mel_loss: 9.415 | 3.572 sec / 10 steps |\n",
      "| step: 420580 | gen_loss: 48.386 | mel_loss: 11.873 | 3.232 sec / 10 steps |\n",
      "| step: 420590 | gen_loss: 46.564 | mel_loss: 11.441 | 3.257 sec / 10 steps |\n",
      "| step: 420600 | gen_loss: 47.353 | mel_loss: 10.997 | 2.768 sec / 10 steps |\n",
      "Validation mel_loss: 14.119524002075195\n",
      "| step: 420610 | gen_loss: 46.086 | mel_loss: 10.369 | 3.319 sec / 10 steps |\n",
      "| step: 420620 | gen_loss: 48.878 | mel_loss: 11.619 | 3.282 sec / 10 steps |\n",
      "| step: 420630 | gen_loss: 45.031 | mel_loss: 10.536 | 3.064 sec / 10 steps |\n",
      "| step: 420640 | gen_loss: 42.650 | mel_loss: 10.046 | 3.069 sec / 10 steps |\n",
      "| step: 420650 | gen_loss: 48.194 | mel_loss: 11.834 | 3.381 sec / 10 steps |\n",
      "| step: 420660 | gen_loss: 51.806 | mel_loss: 13.265 | 3.399 sec / 10 steps |\n",
      "| step: 420670 | gen_loss: 49.169 | mel_loss: 12.128 | 3.187 sec / 10 steps |\n",
      "| step: 420680 | gen_loss: 49.717 | mel_loss: 11.757 | 3.166 sec / 10 steps |\n",
      "| step: 420690 | gen_loss: 45.764 | mel_loss: 10.996 | 3.181 sec / 10 steps |\n",
      "| step: 420700 | gen_loss: 42.285 | mel_loss: 10.623 | 3.256 sec / 10 steps |\n",
      "Validation mel_loss: 14.055217742919922\n",
      "| step: 420710 | gen_loss: 48.402 | mel_loss: 12.660 | 3.234 sec / 10 steps |\n",
      "| step: 420720 | gen_loss: 45.140 | mel_loss: 11.437 | 3.232 sec / 10 steps |\n",
      "| step: 420730 | gen_loss: 48.034 | mel_loss: 11.402 | 3.355 sec / 10 steps |\n",
      "| step: 420740 | gen_loss: 50.698 | mel_loss: 11.894 | 3.268 sec / 10 steps |\n",
      "| step: 420750 | gen_loss: 46.875 | mel_loss: 10.271 | 3.500 sec / 10 steps |\n",
      "| step: 420760 | gen_loss: 47.067 | mel_loss: 10.911 | 3.438 sec / 10 steps |\n",
      "| step: 420770 | gen_loss: 44.555 | mel_loss: 10.224 | 3.098 sec / 10 steps |\n",
      "| step: 420780 | gen_loss: 47.571 | mel_loss: 11.970 | 3.186 sec / 10 steps |\n",
      "| step: 420790 | gen_loss: 50.659 | mel_loss: 11.577 | 3.071 sec / 10 steps |\n",
      "| step: 420800 | gen_loss: 46.163 | mel_loss: 10.342 | 2.951 sec / 10 steps |\n",
      "Validation mel_loss: 14.10097885131836\n",
      "| step: 420810 | gen_loss: 44.487 | mel_loss: 10.367 | 3.239 sec / 10 steps |\n",
      "| step: 420820 | gen_loss: 50.615 | mel_loss: 12.335 | 3.098 sec / 10 steps |\n",
      "| step: 420830 | gen_loss: 47.491 | mel_loss: 10.753 | 3.225 sec / 10 steps |\n",
      "| step: 420840 | gen_loss: 47.670 | mel_loss: 11.427 | 3.219 sec / 10 steps |\n",
      "| step: 420850 | gen_loss: 40.927 | mel_loss: 9.579 | 3.225 sec / 10 steps |\n",
      "| step: 420860 | gen_loss: 47.656 | mel_loss: 11.400 | 3.680 sec / 10 steps |\n",
      "| step: 420870 | gen_loss: 45.839 | mel_loss: 10.754 | 3.631 sec / 10 steps |\n",
      "| step: 420880 | gen_loss: 50.908 | mel_loss: 11.891 | 3.145 sec / 10 steps |\n",
      "| step: 420890 | gen_loss: 47.902 | mel_loss: 10.600 | 3.363 sec / 10 steps |\n",
      "| step: 420900 | gen_loss: 45.992 | mel_loss: 10.316 | 3.570 sec / 10 steps |\n",
      "Validation mel_loss: 13.970540046691895\n",
      "| step: 420910 | gen_loss: 43.919 | mel_loss: 10.123 | 3.550 sec / 10 steps |\n",
      "| step: 420920 | gen_loss: 45.731 | mel_loss: 11.066 | 3.279 sec / 10 steps |\n",
      "| step: 420930 | gen_loss: 45.601 | mel_loss: 11.096 | 3.470 sec / 10 steps |\n",
      "| step: 420940 | gen_loss: 43.846 | mel_loss: 9.958 | 3.021 sec / 10 steps |\n",
      "| step: 420950 | gen_loss: 45.115 | mel_loss: 10.399 | 3.431 sec / 10 steps |\n",
      "| step: 420960 | gen_loss: 45.748 | mel_loss: 10.882 | 3.196 sec / 10 steps |\n",
      "| step: 420970 | gen_loss: 45.534 | mel_loss: 11.834 | 3.126 sec / 10 steps |\n",
      "| step: 420980 | gen_loss: 41.412 | mel_loss: 9.247 | 3.041 sec / 10 steps |\n",
      "| step: 420990 | gen_loss: 51.230 | mel_loss: 13.321 | 3.399 sec / 10 steps |\n",
      "| step: 421000 | gen_loss: 47.225 | mel_loss: 11.446 | 3.102 sec / 10 steps |\n",
      "Validation mel_loss: 14.029647827148438\n",
      "| step: 421010 | gen_loss: 48.065 | mel_loss: 12.062 | 3.329 sec / 10 steps |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 421020 | gen_loss: 46.696 | mel_loss: 10.853 | 3.280 sec / 10 steps |\n",
      "| step: 421030 | gen_loss: 49.247 | mel_loss: 12.041 | 3.466 sec / 10 steps |\n",
      "| step: 421040 | gen_loss: 47.282 | mel_loss: 11.024 | 3.453 sec / 10 steps |\n",
      "| step: 421050 | gen_loss: 47.716 | mel_loss: 10.817 | 3.521 sec / 10 steps |\n",
      "| step: 421060 | gen_loss: 50.696 | mel_loss: 12.045 | 3.273 sec / 10 steps |\n",
      "| step: 421070 | gen_loss: 49.047 | mel_loss: 11.619 | 3.102 sec / 10 steps |\n",
      "| step: 421080 | gen_loss: 44.975 | mel_loss: 9.904 | 3.414 sec / 10 steps |\n",
      "| step: 421090 | gen_loss: 43.029 | mel_loss: 9.669 | 2.874 sec / 10 steps |\n",
      "| step: 421100 | gen_loss: 46.331 | mel_loss: 10.681 | 3.821 sec / 10 steps |\n",
      "Validation mel_loss: 13.958352088928223\n",
      "| step: 421110 | gen_loss: 50.874 | mel_loss: 12.972 | 2.977 sec / 10 steps |\n",
      "| step: 421120 | gen_loss: 47.458 | mel_loss: 10.706 | 3.417 sec / 10 steps |\n",
      "| step: 421130 | gen_loss: 45.003 | mel_loss: 10.863 | 3.412 sec / 10 steps |\n",
      "| step: 421140 | gen_loss: 50.291 | mel_loss: 11.964 | 2.905 sec / 10 steps |\n",
      "| step: 421150 | gen_loss: 44.147 | mel_loss: 10.152 | 3.189 sec / 10 steps |\n",
      "| step: 421160 | gen_loss: 43.411 | mel_loss: 10.229 | 2.911 sec / 10 steps |\n",
      "| step: 421170 | gen_loss: 45.958 | mel_loss: 10.561 | 3.131 sec / 10 steps |\n",
      "| step: 421180 | gen_loss: 46.657 | mel_loss: 10.776 | 3.157 sec / 10 steps |\n",
      "| step: 421190 | gen_loss: 47.197 | mel_loss: 10.563 | 3.279 sec / 10 steps |\n",
      "| step: 421200 | gen_loss: 44.477 | mel_loss: 10.567 | 3.243 sec / 10 steps |\n",
      "Validation mel_loss: 14.00684642791748\n",
      "| step: 421210 | gen_loss: 45.797 | mel_loss: 10.616 | 3.292 sec / 10 steps |\n",
      "| step: 421220 | gen_loss: 46.637 | mel_loss: 11.434 | 3.322 sec / 10 steps |\n",
      "| step: 421230 | gen_loss: 48.579 | mel_loss: 12.120 | 3.511 sec / 10 steps |\n",
      "| step: 421240 | gen_loss: 46.868 | mel_loss: 11.687 | 3.381 sec / 10 steps |\n",
      "| step: 421250 | gen_loss: 42.943 | mel_loss: 11.249 | 3.295 sec / 10 steps |\n",
      "|| Epoch: 561 ||\n",
      "| step: 421260 | gen_loss: 44.578 | mel_loss: 11.101 | 3.484 sec / 10 steps |\n",
      "| step: 421270 | gen_loss: 50.740 | mel_loss: 12.987 | 3.389 sec / 10 steps |\n",
      "| step: 421280 | gen_loss: 46.164 | mel_loss: 10.669 | 3.085 sec / 10 steps |\n",
      "| step: 421290 | gen_loss: 43.034 | mel_loss: 8.888 | 3.002 sec / 10 steps |\n",
      "| step: 421300 | gen_loss: 48.914 | mel_loss: 12.370 | 3.053 sec / 10 steps |\n",
      "Validation mel_loss: 14.031403541564941\n",
      "| step: 421310 | gen_loss: 43.173 | mel_loss: 9.771 | 3.295 sec / 10 steps |\n",
      "| step: 421320 | gen_loss: 46.955 | mel_loss: 10.604 | 2.796 sec / 10 steps |\n",
      "| step: 421330 | gen_loss: 46.566 | mel_loss: 11.381 | 3.367 sec / 10 steps |\n",
      "| step: 421340 | gen_loss: 44.761 | mel_loss: 10.576 | 3.231 sec / 10 steps |\n",
      "| step: 421350 | gen_loss: 44.239 | mel_loss: 9.735 | 3.698 sec / 10 steps |\n",
      "| step: 421360 | gen_loss: 47.119 | mel_loss: 11.053 | 3.192 sec / 10 steps |\n",
      "| step: 421370 | gen_loss: 40.500 | mel_loss: 8.711 | 3.428 sec / 10 steps |\n",
      "| step: 421380 | gen_loss: 52.174 | mel_loss: 13.058 | 3.375 sec / 10 steps |\n",
      "| step: 421390 | gen_loss: 44.707 | mel_loss: 10.432 | 3.236 sec / 10 steps |\n",
      "| step: 421400 | gen_loss: 50.848 | mel_loss: 12.501 | 3.327 sec / 10 steps |\n",
      "Validation mel_loss: 14.193747520446777\n",
      "| step: 421410 | gen_loss: 50.179 | mel_loss: 12.348 | 3.346 sec / 10 steps |\n",
      "| step: 421420 | gen_loss: 50.031 | mel_loss: 12.005 | 3.299 sec / 10 steps |\n",
      "| step: 421430 | gen_loss: 47.107 | mel_loss: 10.928 | 3.147 sec / 10 steps |\n",
      "| step: 421440 | gen_loss: 46.763 | mel_loss: 11.654 | 3.606 sec / 10 steps |\n",
      "| step: 421450 | gen_loss: 46.924 | mel_loss: 11.368 | 3.289 sec / 10 steps |\n",
      "| step: 421460 | gen_loss: 48.115 | mel_loss: 11.946 | 3.261 sec / 10 steps |\n",
      "| step: 421470 | gen_loss: 46.979 | mel_loss: 11.767 | 3.248 sec / 10 steps |\n",
      "| step: 421480 | gen_loss: 50.176 | mel_loss: 11.492 | 3.345 sec / 10 steps |\n",
      "| step: 421490 | gen_loss: 40.836 | mel_loss: 9.263 | 3.370 sec / 10 steps |\n",
      "| step: 421500 | gen_loss: 48.765 | mel_loss: 11.937 | 2.923 sec / 10 steps |\n",
      "Validation mel_loss: 14.233160018920898\n",
      "| step: 421510 | gen_loss: 44.611 | mel_loss: 10.636 | 3.449 sec / 10 steps |\n",
      "| step: 421520 | gen_loss: 48.412 | mel_loss: 11.773 | 3.165 sec / 10 steps |\n",
      "| step: 421530 | gen_loss: 44.163 | mel_loss: 10.895 | 3.222 sec / 10 steps |\n",
      "| step: 421540 | gen_loss: 47.038 | mel_loss: 10.568 | 2.913 sec / 10 steps |\n",
      "| step: 421550 | gen_loss: 51.160 | mel_loss: 12.574 | 3.089 sec / 10 steps |\n",
      "| step: 421560 | gen_loss: 45.577 | mel_loss: 10.882 | 3.267 sec / 10 steps |\n",
      "| step: 421570 | gen_loss: 44.100 | mel_loss: 9.248 | 3.287 sec / 10 steps |\n",
      "| step: 421580 | gen_loss: 50.994 | mel_loss: 12.097 | 3.716 sec / 10 steps |\n",
      "| step: 421590 | gen_loss: 50.330 | mel_loss: 11.671 | 3.336 sec / 10 steps |\n",
      "| step: 421600 | gen_loss: 52.191 | mel_loss: 12.475 | 3.321 sec / 10 steps |\n",
      "Validation mel_loss: 14.100090026855469\n",
      "| step: 421610 | gen_loss: 49.071 | mel_loss: 11.730 | 3.455 sec / 10 steps |\n",
      "| step: 421620 | gen_loss: 51.005 | mel_loss: 12.284 | 3.058 sec / 10 steps |\n",
      "| step: 421630 | gen_loss: 44.066 | mel_loss: 9.983 | 3.115 sec / 10 steps |\n",
      "| step: 421640 | gen_loss: 40.088 | mel_loss: 8.384 | 3.302 sec / 10 steps |\n",
      "| step: 421650 | gen_loss: 49.937 | mel_loss: 11.397 | 3.236 sec / 10 steps |\n",
      "| step: 421660 | gen_loss: 46.069 | mel_loss: 10.608 | 3.192 sec / 10 steps |\n",
      "| step: 421670 | gen_loss: 53.194 | mel_loss: 12.812 | 3.238 sec / 10 steps |\n",
      "| step: 421680 | gen_loss: 51.019 | mel_loss: 12.166 | 3.127 sec / 10 steps |\n",
      "| step: 421690 | gen_loss: 48.298 | mel_loss: 11.463 | 3.031 sec / 10 steps |\n",
      "| step: 421700 | gen_loss: 48.212 | mel_loss: 10.775 | 3.072 sec / 10 steps |\n",
      "Validation mel_loss: 13.947579383850098\n",
      "| step: 421710 | gen_loss: 47.325 | mel_loss: 10.593 | 3.252 sec / 10 steps |\n",
      "| step: 421720 | gen_loss: 46.841 | mel_loss: 12.675 | 2.936 sec / 10 steps |\n",
      "| step: 421730 | gen_loss: 48.244 | mel_loss: 12.177 | 3.325 sec / 10 steps |\n",
      "| step: 421740 | gen_loss: 38.849 | mel_loss: 8.847 | 3.459 sec / 10 steps |\n",
      "| step: 421750 | gen_loss: 41.535 | mel_loss: 9.547 | 3.211 sec / 10 steps |\n",
      "| step: 421760 | gen_loss: 49.200 | mel_loss: 11.052 | 3.488 sec / 10 steps |\n",
      "| step: 421770 | gen_loss: 42.872 | mel_loss: 9.766 | 3.016 sec / 10 steps |\n",
      "| step: 421780 | gen_loss: 44.774 | mel_loss: 10.492 | 3.151 sec / 10 steps |\n",
      "| step: 421790 | gen_loss: 43.929 | mel_loss: 10.659 | 3.489 sec / 10 steps |\n",
      "| step: 421800 | gen_loss: 48.747 | mel_loss: 11.969 | 3.248 sec / 10 steps |\n",
      "Validation mel_loss: 13.976394653320312\n",
      "| step: 421810 | gen_loss: 53.026 | mel_loss: 12.461 | 3.333 sec / 10 steps |\n",
      "| step: 421820 | gen_loss: 43.812 | mel_loss: 9.016 | 3.417 sec / 10 steps |\n",
      "| step: 421830 | gen_loss: 45.564 | mel_loss: 11.281 | 3.517 sec / 10 steps |\n",
      "| step: 421840 | gen_loss: 43.156 | mel_loss: 9.269 | 3.204 sec / 10 steps |\n",
      "| step: 421850 | gen_loss: 46.351 | mel_loss: 10.820 | 3.315 sec / 10 steps |\n",
      "| step: 421860 | gen_loss: 48.822 | mel_loss: 12.205 | 3.380 sec / 10 steps |\n",
      "| step: 421870 | gen_loss: 52.731 | mel_loss: 12.544 | 3.457 sec / 10 steps |\n",
      "| step: 421880 | gen_loss: 48.846 | mel_loss: 12.161 | 3.174 sec / 10 steps |\n",
      "| step: 421890 | gen_loss: 45.056 | mel_loss: 11.267 | 3.104 sec / 10 steps |\n",
      "| step: 421900 | gen_loss: 41.161 | mel_loss: 8.602 | 3.268 sec / 10 steps |\n",
      "Validation mel_loss: 13.92718505859375\n",
      "| step: 421910 | gen_loss: 48.671 | mel_loss: 12.813 | 3.313 sec / 10 steps |\n",
      "| step: 421920 | gen_loss: 42.977 | mel_loss: 9.463 | 3.366 sec / 10 steps |\n",
      "| step: 421930 | gen_loss: 45.875 | mel_loss: 10.051 | 2.946 sec / 10 steps |\n",
      "| step: 421940 | gen_loss: 52.070 | mel_loss: 11.987 | 3.121 sec / 10 steps |\n",
      "| step: 421950 | gen_loss: 52.307 | mel_loss: 13.145 | 3.441 sec / 10 steps |\n",
      "| step: 421960 | gen_loss: 46.163 | mel_loss: 10.333 | 3.232 sec / 10 steps |\n",
      "| step: 421970 | gen_loss: 40.356 | mel_loss: 9.296 | 3.479 sec / 10 steps |\n",
      "| step: 421980 | gen_loss: 43.368 | mel_loss: 9.587 | 3.270 sec / 10 steps |\n",
      "| step: 421990 | gen_loss: 47.101 | mel_loss: 11.331 | 3.810 sec / 10 steps |\n",
      "| step: 422000 | gen_loss: 49.732 | mel_loss: 11.815 | 3.496 sec / 10 steps |\n",
      "Validation mel_loss: 13.936372756958008\n",
      "|| Epoch: 562 ||\n",
      "| step: 422010 | gen_loss: 50.967 | mel_loss: 12.155 | 3.548 sec / 10 steps |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 422020 | gen_loss: 43.641 | mel_loss: 9.681 | 3.366 sec / 10 steps |\n",
      "| step: 422030 | gen_loss: 52.838 | mel_loss: 13.405 | 3.130 sec / 10 steps |\n",
      "| step: 422040 | gen_loss: 51.956 | mel_loss: 12.050 | 3.256 sec / 10 steps |\n",
      "| step: 422050 | gen_loss: 48.957 | mel_loss: 11.740 | 3.191 sec / 10 steps |\n",
      "| step: 422060 | gen_loss: 48.254 | mel_loss: 11.440 | 3.055 sec / 10 steps |\n",
      "| step: 422070 | gen_loss: 47.263 | mel_loss: 11.171 | 3.291 sec / 10 steps |\n",
      "| step: 422080 | gen_loss: 41.896 | mel_loss: 10.226 | 3.133 sec / 10 steps |\n",
      "| step: 422090 | gen_loss: 46.074 | mel_loss: 10.678 | 3.552 sec / 10 steps |\n",
      "| step: 422100 | gen_loss: 49.725 | mel_loss: 12.910 | 2.992 sec / 10 steps |\n",
      "Validation mel_loss: 13.889704704284668\n",
      "| step: 422110 | gen_loss: 49.685 | mel_loss: 12.401 | 3.618 sec / 10 steps |\n",
      "| step: 422120 | gen_loss: 40.071 | mel_loss: 8.934 | 3.401 sec / 10 steps |\n",
      "| step: 422130 | gen_loss: 47.874 | mel_loss: 11.315 | 3.373 sec / 10 steps |\n",
      "| step: 422140 | gen_loss: 44.496 | mel_loss: 9.670 | 3.054 sec / 10 steps |\n",
      "| step: 422150 | gen_loss: 47.149 | mel_loss: 11.318 | 3.276 sec / 10 steps |\n",
      "| step: 422160 | gen_loss: 48.568 | mel_loss: 10.763 | 2.950 sec / 10 steps |\n",
      "| step: 422170 | gen_loss: 51.967 | mel_loss: 12.044 | 2.894 sec / 10 steps |\n",
      "| step: 422180 | gen_loss: 44.745 | mel_loss: 9.728 | 3.456 sec / 10 steps |\n",
      "| step: 422190 | gen_loss: 38.036 | mel_loss: 8.782 | 3.180 sec / 10 steps |\n",
      "| step: 422200 | gen_loss: 41.865 | mel_loss: 9.723 | 3.215 sec / 10 steps |\n",
      "Validation mel_loss: 14.247118949890137\n",
      "| step: 422210 | gen_loss: 44.189 | mel_loss: 11.255 | 3.209 sec / 10 steps |\n",
      "| step: 422220 | gen_loss: 39.221 | mel_loss: 8.775 | 3.256 sec / 10 steps |\n",
      "| step: 422230 | gen_loss: 48.950 | mel_loss: 11.636 | 3.508 sec / 10 steps |\n",
      "| step: 422240 | gen_loss: 43.167 | mel_loss: 9.866 | 3.469 sec / 10 steps |\n",
      "| step: 422250 | gen_loss: 52.496 | mel_loss: 12.667 | 3.369 sec / 10 steps |\n",
      "| step: 422260 | gen_loss: 40.352 | mel_loss: 9.916 | 3.051 sec / 10 steps |\n",
      "| step: 422270 | gen_loss: 49.402 | mel_loss: 11.444 | 3.223 sec / 10 steps |\n",
      "| step: 422280 | gen_loss: 42.912 | mel_loss: 10.697 | 3.032 sec / 10 steps |\n",
      "| step: 422290 | gen_loss: 49.077 | mel_loss: 12.292 | 3.409 sec / 10 steps |\n",
      "| step: 422300 | gen_loss: 51.240 | mel_loss: 12.472 | 2.838 sec / 10 steps |\n",
      "Validation mel_loss: 14.079853057861328\n",
      "| step: 422310 | gen_loss: 44.441 | mel_loss: 10.554 | 3.455 sec / 10 steps |\n",
      "| step: 422320 | gen_loss: 50.098 | mel_loss: 12.020 | 3.237 sec / 10 steps |\n",
      "| step: 422330 | gen_loss: 45.997 | mel_loss: 10.592 | 3.582 sec / 10 steps |\n",
      "| step: 422340 | gen_loss: 46.514 | mel_loss: 11.204 | 3.264 sec / 10 steps |\n",
      "| step: 422350 | gen_loss: 44.759 | mel_loss: 10.906 | 3.395 sec / 10 steps |\n",
      "| step: 422360 | gen_loss: 53.168 | mel_loss: 13.234 | 3.294 sec / 10 steps |\n",
      "| step: 422370 | gen_loss: 44.508 | mel_loss: 10.605 | 3.175 sec / 10 steps |\n",
      "| step: 422380 | gen_loss: 48.323 | mel_loss: 12.013 | 3.257 sec / 10 steps |\n",
      "| step: 422390 | gen_loss: 51.538 | mel_loss: 12.037 | 3.589 sec / 10 steps |\n",
      "| step: 422400 | gen_loss: 47.409 | mel_loss: 11.544 | 3.447 sec / 10 steps |\n",
      "Validation mel_loss: 14.033926963806152\n",
      "| step: 422410 | gen_loss: 47.722 | mel_loss: 11.800 | 3.444 sec / 10 steps |\n",
      "| step: 422420 | gen_loss: 44.851 | mel_loss: 10.699 | 3.631 sec / 10 steps |\n",
      "| step: 422430 | gen_loss: 39.024 | mel_loss: 8.645 | 3.119 sec / 10 steps |\n",
      "| step: 422440 | gen_loss: 49.613 | mel_loss: 11.859 | 3.574 sec / 10 steps |\n",
      "| step: 422450 | gen_loss: 52.622 | mel_loss: 12.357 | 2.887 sec / 10 steps |\n",
      "| step: 422460 | gen_loss: 49.536 | mel_loss: 12.844 | 3.156 sec / 10 steps |\n",
      "| step: 422470 | gen_loss: 48.456 | mel_loss: 11.423 | 3.486 sec / 10 steps |\n",
      "| step: 422480 | gen_loss: 48.104 | mel_loss: 12.022 | 3.330 sec / 10 steps |\n",
      "| step: 422490 | gen_loss: 43.975 | mel_loss: 9.052 | 3.353 sec / 10 steps |\n",
      "| step: 422500 | gen_loss: 43.261 | mel_loss: 9.877 | 3.207 sec / 10 steps |\n",
      "Validation mel_loss: 14.125006675720215\n",
      "| step: 422510 | gen_loss: 46.525 | mel_loss: 11.717 | 3.390 sec / 10 steps |\n",
      "| step: 422520 | gen_loss: 38.351 | mel_loss: 8.137 | 3.328 sec / 10 steps |\n",
      "| step: 422530 | gen_loss: 46.837 | mel_loss: 11.779 | 3.420 sec / 10 steps |\n",
      "| step: 422540 | gen_loss: 44.088 | mel_loss: 9.900 | 3.428 sec / 10 steps |\n",
      "| step: 422550 | gen_loss: 37.370 | mel_loss: 7.971 | 3.274 sec / 10 steps |\n",
      "| step: 422560 | gen_loss: 46.591 | mel_loss: 10.542 | 3.388 sec / 10 steps |\n",
      "| step: 422570 | gen_loss: 47.017 | mel_loss: 11.671 | 3.377 sec / 10 steps |\n",
      "| step: 422580 | gen_loss: 50.845 | mel_loss: 12.214 | 3.506 sec / 10 steps |\n",
      "| step: 422590 | gen_loss: 44.712 | mel_loss: 10.413 | 3.068 sec / 10 steps |\n",
      "| step: 422600 | gen_loss: 47.362 | mel_loss: 10.916 | 2.982 sec / 10 steps |\n",
      "Validation mel_loss: 14.18829345703125\n",
      "| step: 422610 | gen_loss: 45.232 | mel_loss: 10.178 | 3.441 sec / 10 steps |\n",
      "| step: 422620 | gen_loss: 46.691 | mel_loss: 11.222 | 3.770 sec / 10 steps |\n",
      "| step: 422630 | gen_loss: 50.200 | mel_loss: 12.551 | 3.198 sec / 10 steps |\n",
      "| step: 422640 | gen_loss: 50.777 | mel_loss: 12.099 | 2.971 sec / 10 steps |\n",
      "| step: 422650 | gen_loss: 44.936 | mel_loss: 10.323 | 3.256 sec / 10 steps |\n",
      "| step: 422660 | gen_loss: 44.758 | mel_loss: 9.980 | 3.557 sec / 10 steps |\n",
      "| step: 422670 | gen_loss: 50.225 | mel_loss: 13.031 | 3.363 sec / 10 steps |\n",
      "| step: 422680 | gen_loss: 40.210 | mel_loss: 10.071 | 3.501 sec / 10 steps |\n",
      "| step: 422690 | gen_loss: 49.425 | mel_loss: 12.505 | 2.827 sec / 10 steps |\n",
      "| step: 422700 | gen_loss: 45.009 | mel_loss: 10.393 | 3.366 sec / 10 steps |\n",
      "Validation mel_loss: 13.947237014770508\n",
      "| step: 422710 | gen_loss: 44.545 | mel_loss: 11.653 | 3.290 sec / 10 steps |\n",
      "| step: 422720 | gen_loss: 50.445 | mel_loss: 12.625 | 3.284 sec / 10 steps |\n",
      "| step: 422730 | gen_loss: 44.540 | mel_loss: 10.659 | 3.267 sec / 10 steps |\n",
      "| step: 422740 | gen_loss: 39.853 | mel_loss: 8.851 | 3.518 sec / 10 steps |\n",
      "| step: 422750 | gen_loss: 46.319 | mel_loss: 10.724 | 3.563 sec / 10 steps |\n",
      "|| Epoch: 563 ||\n",
      "| step: 422760 | gen_loss: 40.077 | mel_loss: 9.342 | 3.113 sec / 10 steps |\n",
      "| step: 422770 | gen_loss: 45.173 | mel_loss: 10.723 | 2.970 sec / 10 steps |\n",
      "| step: 422780 | gen_loss: 45.437 | mel_loss: 10.044 | 3.648 sec / 10 steps |\n",
      "| step: 422790 | gen_loss: 47.070 | mel_loss: 10.350 | 3.275 sec / 10 steps |\n",
      "| step: 422800 | gen_loss: 47.029 | mel_loss: 11.010 | 3.192 sec / 10 steps |\n",
      "Validation mel_loss: 14.270320892333984\n",
      "| step: 422810 | gen_loss: 46.741 | mel_loss: 10.900 | 3.297 sec / 10 steps |\n",
      "| step: 422820 | gen_loss: 49.444 | mel_loss: 11.840 | 3.157 sec / 10 steps |\n",
      "| step: 422830 | gen_loss: 49.768 | mel_loss: 11.386 | 3.724 sec / 10 steps |\n",
      "| step: 422840 | gen_loss: 47.036 | mel_loss: 10.699 | 3.205 sec / 10 steps |\n",
      "| step: 422850 | gen_loss: 42.216 | mel_loss: 9.653 | 3.596 sec / 10 steps |\n",
      "| step: 422860 | gen_loss: 46.554 | mel_loss: 11.246 | 3.267 sec / 10 steps |\n",
      "| step: 422870 | gen_loss: 45.974 | mel_loss: 10.962 | 3.086 sec / 10 steps |\n",
      "| step: 422880 | gen_loss: 47.828 | mel_loss: 11.964 | 3.167 sec / 10 steps |\n",
      "| step: 422890 | gen_loss: 48.209 | mel_loss: 11.490 | 3.501 sec / 10 steps |\n",
      "| step: 422900 | gen_loss: 54.239 | mel_loss: 13.313 | 3.416 sec / 10 steps |\n",
      "Validation mel_loss: 14.508495330810547\n",
      "| step: 422910 | gen_loss: 45.101 | mel_loss: 9.917 | 3.518 sec / 10 steps |\n",
      "| step: 422920 | gen_loss: 51.198 | mel_loss: 12.606 | 3.502 sec / 10 steps |\n",
      "| step: 422930 | gen_loss: 48.543 | mel_loss: 12.430 | 3.618 sec / 10 steps |\n",
      "| step: 422940 | gen_loss: 41.149 | mel_loss: 8.957 | 3.485 sec / 10 steps |\n",
      "| step: 422950 | gen_loss: 42.898 | mel_loss: 8.912 | 3.207 sec / 10 steps |\n",
      "| step: 422960 | gen_loss: 45.494 | mel_loss: 10.152 | 3.002 sec / 10 steps |\n",
      "| step: 422970 | gen_loss: 49.903 | mel_loss: 11.782 | 3.378 sec / 10 steps |\n",
      "| step: 422980 | gen_loss: 52.444 | mel_loss: 12.170 | 3.368 sec / 10 steps |\n",
      "| step: 422990 | gen_loss: 43.772 | mel_loss: 9.919 | 3.482 sec / 10 steps |\n",
      "| step: 423000 | gen_loss: 46.287 | mel_loss: 10.722 | 3.196 sec / 10 steps |\n",
      "Validation mel_loss: 14.144149780273438\n",
      "| step: 423010 | gen_loss: 50.196 | mel_loss: 12.419 | 3.363 sec / 10 steps |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 423020 | gen_loss: 48.568 | mel_loss: 11.440 | 3.244 sec / 10 steps |\n",
      "| step: 423030 | gen_loss: 47.707 | mel_loss: 10.890 | 3.284 sec / 10 steps |\n",
      "| step: 423040 | gen_loss: 46.617 | mel_loss: 12.276 | 2.897 sec / 10 steps |\n",
      "| step: 423050 | gen_loss: 39.885 | mel_loss: 8.443 | 3.436 sec / 10 steps |\n",
      "| step: 423060 | gen_loss: 46.764 | mel_loss: 11.176 | 3.410 sec / 10 steps |\n",
      "| step: 423070 | gen_loss: 43.115 | mel_loss: 9.642 | 3.358 sec / 10 steps |\n",
      "| step: 423080 | gen_loss: 45.130 | mel_loss: 11.137 | 3.561 sec / 10 steps |\n",
      "| step: 423090 | gen_loss: 49.077 | mel_loss: 12.211 | 3.516 sec / 10 steps |\n",
      "| step: 423100 | gen_loss: 47.653 | mel_loss: 11.374 | 2.973 sec / 10 steps |\n",
      "Validation mel_loss: 14.15711784362793\n",
      "| step: 423110 | gen_loss: 49.030 | mel_loss: 11.819 | 3.111 sec / 10 steps |\n",
      "| step: 423120 | gen_loss: 50.123 | mel_loss: 12.193 | 3.021 sec / 10 steps |\n",
      "| step: 423130 | gen_loss: 48.299 | mel_loss: 11.559 | 3.151 sec / 10 steps |\n",
      "| step: 423140 | gen_loss: 44.939 | mel_loss: 9.949 | 3.154 sec / 10 steps |\n",
      "| step: 423150 | gen_loss: 48.009 | mel_loss: 10.977 | 3.576 sec / 10 steps |\n",
      "| step: 423160 | gen_loss: 46.325 | mel_loss: 10.946 | 3.179 sec / 10 steps |\n",
      "| step: 423170 | gen_loss: 46.044 | mel_loss: 10.390 | 3.407 sec / 10 steps |\n",
      "| step: 423180 | gen_loss: 50.012 | mel_loss: 11.346 | 3.180 sec / 10 steps |\n",
      "| step: 423190 | gen_loss: 48.535 | mel_loss: 11.213 | 3.593 sec / 10 steps |\n",
      "| step: 423200 | gen_loss: 50.146 | mel_loss: 12.306 | 3.088 sec / 10 steps |\n",
      "Validation mel_loss: 14.234517097473145\n",
      "| step: 423210 | gen_loss: 47.499 | mel_loss: 12.254 | 3.422 sec / 10 steps |\n",
      "| step: 423220 | gen_loss: 35.683 | mel_loss: 8.136 | 3.699 sec / 10 steps |\n",
      "| step: 423230 | gen_loss: 46.097 | mel_loss: 10.662 | 3.497 sec / 10 steps |\n",
      "| step: 423240 | gen_loss: 47.638 | mel_loss: 10.813 | 3.171 sec / 10 steps |\n",
      "| step: 423250 | gen_loss: 46.008 | mel_loss: 10.160 | 3.242 sec / 10 steps |\n",
      "| step: 423260 | gen_loss: 48.919 | mel_loss: 11.177 | 3.221 sec / 10 steps |\n",
      "| step: 423270 | gen_loss: 51.461 | mel_loss: 12.636 | 3.052 sec / 10 steps |\n",
      "| step: 423280 | gen_loss: 50.049 | mel_loss: 12.012 | 3.340 sec / 10 steps |\n",
      "| step: 423290 | gen_loss: 47.564 | mel_loss: 11.193 | 2.915 sec / 10 steps |\n",
      "| step: 423300 | gen_loss: 47.241 | mel_loss: 11.709 | 3.173 sec / 10 steps |\n",
      "Validation mel_loss: 14.074739456176758\n",
      "| step: 423310 | gen_loss: 45.506 | mel_loss: 10.466 | 3.351 sec / 10 steps |\n",
      "| step: 423320 | gen_loss: 51.114 | mel_loss: 13.292 | 3.519 sec / 10 steps |\n",
      "| step: 423330 | gen_loss: 41.272 | mel_loss: 9.459 | 3.606 sec / 10 steps |\n",
      "| step: 423340 | gen_loss: 48.112 | mel_loss: 12.206 | 3.469 sec / 10 steps |\n",
      "| step: 423350 | gen_loss: 43.545 | mel_loss: 10.511 | 3.540 sec / 10 steps |\n",
      "| step: 423360 | gen_loss: 49.986 | mel_loss: 11.791 | 3.246 sec / 10 steps |\n",
      "| step: 423370 | gen_loss: 42.226 | mel_loss: 9.731 | 3.156 sec / 10 steps |\n",
      "| step: 423380 | gen_loss: 50.309 | mel_loss: 11.174 | 3.562 sec / 10 steps |\n",
      "| step: 423390 | gen_loss: 50.083 | mel_loss: 11.945 | 3.282 sec / 10 steps |\n",
      "| step: 423400 | gen_loss: 48.715 | mel_loss: 11.559 | 3.478 sec / 10 steps |\n",
      "Validation mel_loss: 14.27092456817627\n",
      "| step: 423410 | gen_loss: 43.422 | mel_loss: 10.276 | 3.212 sec / 10 steps |\n",
      "| step: 423420 | gen_loss: 47.399 | mel_loss: 11.889 | 3.140 sec / 10 steps |\n",
      "| step: 423430 | gen_loss: 52.973 | mel_loss: 12.460 | 3.192 sec / 10 steps |\n",
      "| step: 423440 | gen_loss: 48.949 | mel_loss: 11.710 | 3.239 sec / 10 steps |\n",
      "| step: 423450 | gen_loss: 44.061 | mel_loss: 10.244 | 3.349 sec / 10 steps |\n",
      "| step: 423460 | gen_loss: 48.787 | mel_loss: 11.998 | 3.266 sec / 10 steps |\n",
      "| step: 423470 | gen_loss: 44.258 | mel_loss: 10.394 | 2.878 sec / 10 steps |\n",
      "| step: 423480 | gen_loss: 46.096 | mel_loss: 10.411 | 2.876 sec / 10 steps |\n",
      "| step: 423490 | gen_loss: 46.139 | mel_loss: 10.667 | 2.930 sec / 10 steps |\n",
      "| step: 423500 | gen_loss: 45.412 | mel_loss: 10.437 | 3.072 sec / 10 steps |\n",
      "Validation mel_loss: 14.083487510681152\n",
      "|| Epoch: 564 ||\n",
      "| step: 423510 | gen_loss: 50.616 | mel_loss: 12.413 | 3.477 sec / 10 steps |\n",
      "| step: 423520 | gen_loss: 43.891 | mel_loss: 10.249 | 3.251 sec / 10 steps |\n",
      "| step: 423530 | gen_loss: 44.287 | mel_loss: 10.209 | 3.112 sec / 10 steps |\n",
      "| step: 423540 | gen_loss: 44.919 | mel_loss: 10.076 | 3.128 sec / 10 steps |\n",
      "| step: 423550 | gen_loss: 42.976 | mel_loss: 10.056 | 3.194 sec / 10 steps |\n",
      "| step: 423560 | gen_loss: 40.718 | mel_loss: 8.337 | 3.555 sec / 10 steps |\n",
      "| step: 423570 | gen_loss: 45.997 | mel_loss: 10.404 | 3.229 sec / 10 steps |\n",
      "| step: 423580 | gen_loss: 47.662 | mel_loss: 11.046 | 3.234 sec / 10 steps |\n",
      "| step: 423590 | gen_loss: 51.501 | mel_loss: 13.067 | 3.289 sec / 10 steps |\n",
      "| step: 423600 | gen_loss: 48.093 | mel_loss: 12.126 | 3.315 sec / 10 steps |\n",
      "Validation mel_loss: 14.05080509185791\n",
      "| step: 423610 | gen_loss: 48.282 | mel_loss: 11.655 | 3.411 sec / 10 steps |\n",
      "| step: 423620 | gen_loss: 45.660 | mel_loss: 10.468 | 3.199 sec / 10 steps |\n",
      "| step: 423630 | gen_loss: 49.845 | mel_loss: 12.049 | 3.471 sec / 10 steps |\n",
      "| step: 423640 | gen_loss: 44.915 | mel_loss: 10.460 | 3.139 sec / 10 steps |\n",
      "| step: 423650 | gen_loss: 53.313 | mel_loss: 12.605 | 2.820 sec / 10 steps |\n",
      "| step: 423660 | gen_loss: 42.861 | mel_loss: 9.991 | 3.800 sec / 10 steps |\n",
      "| step: 423670 | gen_loss: 47.640 | mel_loss: 11.809 | 3.294 sec / 10 steps |\n",
      "| step: 423680 | gen_loss: 46.896 | mel_loss: 11.138 | 3.192 sec / 10 steps |\n",
      "| step: 423690 | gen_loss: 42.227 | mel_loss: 9.050 | 3.538 sec / 10 steps |\n",
      "| step: 423700 | gen_loss: 44.402 | mel_loss: 10.494 | 3.435 sec / 10 steps |\n",
      "Validation mel_loss: 14.29198932647705\n",
      "| step: 423710 | gen_loss: 43.548 | mel_loss: 9.799 | 3.346 sec / 10 steps |\n",
      "| step: 423720 | gen_loss: 48.972 | mel_loss: 12.460 | 3.435 sec / 10 steps |\n",
      "| step: 423730 | gen_loss: 47.566 | mel_loss: 10.629 | 3.321 sec / 10 steps |\n",
      "| step: 423740 | gen_loss: 48.337 | mel_loss: 10.823 | 3.466 sec / 10 steps |\n",
      "| step: 423750 | gen_loss: 50.965 | mel_loss: 12.779 | 3.106 sec / 10 steps |\n",
      "| step: 423760 | gen_loss: 46.505 | mel_loss: 9.983 | 3.282 sec / 10 steps |\n",
      "| step: 423770 | gen_loss: 43.133 | mel_loss: 9.974 | 3.518 sec / 10 steps |\n",
      "| step: 423780 | gen_loss: 42.841 | mel_loss: 10.113 | 3.242 sec / 10 steps |\n",
      "| step: 423790 | gen_loss: 47.485 | mel_loss: 12.408 | 2.991 sec / 10 steps |\n",
      "| step: 423800 | gen_loss: 50.868 | mel_loss: 12.749 | 2.879 sec / 10 steps |\n",
      "Validation mel_loss: 14.132840156555176\n",
      "| step: 423810 | gen_loss: 52.042 | mel_loss: 13.311 | 3.464 sec / 10 steps |\n",
      "| step: 423820 | gen_loss: 45.633 | mel_loss: 11.024 | 3.150 sec / 10 steps |\n",
      "| step: 423830 | gen_loss: 40.934 | mel_loss: 10.637 | 3.223 sec / 10 steps |\n",
      "| step: 423840 | gen_loss: 41.608 | mel_loss: 9.708 | 3.690 sec / 10 steps |\n",
      "| step: 423850 | gen_loss: 46.916 | mel_loss: 11.027 | 3.143 sec / 10 steps |\n",
      "| step: 423860 | gen_loss: 43.831 | mel_loss: 9.519 | 3.313 sec / 10 steps |\n",
      "| step: 423870 | gen_loss: 50.129 | mel_loss: 12.691 | 3.377 sec / 10 steps |\n",
      "| step: 423880 | gen_loss: 50.114 | mel_loss: 12.394 | 3.122 sec / 10 steps |\n",
      "| step: 423890 | gen_loss: 42.714 | mel_loss: 11.075 | 3.392 sec / 10 steps |\n",
      "| step: 423900 | gen_loss: 45.439 | mel_loss: 10.670 | 3.284 sec / 10 steps |\n",
      "Validation mel_loss: 14.10299015045166\n",
      "| step: 423910 | gen_loss: 48.613 | mel_loss: 11.697 | 3.147 sec / 10 steps |\n",
      "| step: 423920 | gen_loss: 47.317 | mel_loss: 10.169 | 3.081 sec / 10 steps |\n",
      "| step: 423930 | gen_loss: 43.526 | mel_loss: 11.379 | 3.336 sec / 10 steps |\n",
      "| step: 423940 | gen_loss: 46.296 | mel_loss: 12.167 | 3.212 sec / 10 steps |\n",
      "| step: 423950 | gen_loss: 43.926 | mel_loss: 9.482 | 3.469 sec / 10 steps |\n",
      "| step: 423960 | gen_loss: 42.433 | mel_loss: 9.425 | 3.270 sec / 10 steps |\n",
      "| step: 423970 | gen_loss: 35.296 | mel_loss: 8.428 | 2.989 sec / 10 steps |\n",
      "| step: 423980 | gen_loss: 39.169 | mel_loss: 9.565 | 3.424 sec / 10 steps |\n",
      "| step: 423990 | gen_loss: 44.642 | mel_loss: 10.536 | 3.264 sec / 10 steps |\n",
      "| step: 424000 | gen_loss: 53.593 | mel_loss: 13.390 | 3.323 sec / 10 steps |\n",
      "Validation mel_loss: 14.188346862792969\n",
      "| step: 424010 | gen_loss: 42.707 | mel_loss: 9.806 | 3.101 sec / 10 steps |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 424020 | gen_loss: 52.621 | mel_loss: 12.050 | 2.911 sec / 10 steps |\n",
      "| step: 424030 | gen_loss: 48.753 | mel_loss: 11.986 | 2.985 sec / 10 steps |\n",
      "| step: 424040 | gen_loss: 44.360 | mel_loss: 10.587 | 2.985 sec / 10 steps |\n",
      "| step: 424050 | gen_loss: 44.498 | mel_loss: 10.012 | 3.322 sec / 10 steps |\n",
      "| step: 424060 | gen_loss: 47.380 | mel_loss: 11.271 | 3.165 sec / 10 steps |\n",
      "| step: 424070 | gen_loss: 40.721 | mel_loss: 9.005 | 3.178 sec / 10 steps |\n",
      "| step: 424080 | gen_loss: 44.518 | mel_loss: 10.127 | 3.444 sec / 10 steps |\n",
      "| step: 424090 | gen_loss: 46.581 | mel_loss: 11.325 | 3.184 sec / 10 steps |\n",
      "| step: 424100 | gen_loss: 45.321 | mel_loss: 10.247 | 3.086 sec / 10 steps |\n",
      "Validation mel_loss: 13.9893798828125\n",
      "| step: 424110 | gen_loss: 47.959 | mel_loss: 11.813 | 3.503 sec / 10 steps |\n",
      "| step: 424120 | gen_loss: 46.586 | mel_loss: 10.246 | 3.250 sec / 10 steps |\n",
      "| step: 424130 | gen_loss: 46.865 | mel_loss: 11.577 | 2.917 sec / 10 steps |\n",
      "| step: 424140 | gen_loss: 47.682 | mel_loss: 11.346 | 3.137 sec / 10 steps |\n",
      "| step: 424150 | gen_loss: 49.998 | mel_loss: 11.983 | 2.931 sec / 10 steps |\n",
      "| step: 424160 | gen_loss: 48.468 | mel_loss: 11.724 | 3.263 sec / 10 steps |\n",
      "| step: 424170 | gen_loss: 43.773 | mel_loss: 10.298 | 2.957 sec / 10 steps |\n",
      "| step: 424180 | gen_loss: 45.245 | mel_loss: 10.804 | 2.965 sec / 10 steps |\n",
      "| step: 424190 | gen_loss: 40.659 | mel_loss: 9.007 | 3.576 sec / 10 steps |\n",
      "| step: 424200 | gen_loss: 51.625 | mel_loss: 12.907 | 3.488 sec / 10 steps |\n",
      "Validation mel_loss: 14.14520263671875\n",
      "| step: 424210 | gen_loss: 49.233 | mel_loss: 12.639 | 3.191 sec / 10 steps |\n",
      "| step: 424220 | gen_loss: 45.442 | mel_loss: 10.550 | 3.283 sec / 10 steps |\n",
      "| step: 424230 | gen_loss: 44.803 | mel_loss: 11.150 | 2.919 sec / 10 steps |\n",
      "| step: 424240 | gen_loss: 49.254 | mel_loss: 13.133 | 3.015 sec / 10 steps |\n",
      "| step: 424250 | gen_loss: 46.494 | mel_loss: 10.173 | 3.108 sec / 10 steps |\n",
      "|| Epoch: 565 ||\n",
      "| step: 424260 | gen_loss: 45.168 | mel_loss: 9.617 | 3.232 sec / 10 steps |\n",
      "| step: 424270 | gen_loss: 46.864 | mel_loss: 10.787 | 3.528 sec / 10 steps |\n",
      "| step: 424280 | gen_loss: 46.079 | mel_loss: 10.443 | 2.917 sec / 10 steps |\n",
      "| step: 424290 | gen_loss: 45.861 | mel_loss: 10.063 | 2.980 sec / 10 steps |\n",
      "| step: 424300 | gen_loss: 43.484 | mel_loss: 10.643 | 3.088 sec / 10 steps |\n",
      "Validation mel_loss: 14.069048881530762\n",
      "| step: 424310 | gen_loss: 51.388 | mel_loss: 12.809 | 3.312 sec / 10 steps |\n",
      "| step: 424320 | gen_loss: 46.492 | mel_loss: 10.025 | 3.222 sec / 10 steps |\n",
      "| step: 424330 | gen_loss: 45.385 | mel_loss: 10.439 | 3.350 sec / 10 steps |\n",
      "| step: 424340 | gen_loss: 48.161 | mel_loss: 11.605 | 3.011 sec / 10 steps |\n",
      "| step: 424350 | gen_loss: 41.587 | mel_loss: 8.776 | 3.303 sec / 10 steps |\n",
      "| step: 424360 | gen_loss: 52.525 | mel_loss: 12.265 | 2.941 sec / 10 steps |\n",
      "| step: 424370 | gen_loss: 46.890 | mel_loss: 10.953 | 3.163 sec / 10 steps |\n",
      "| step: 424380 | gen_loss: 48.862 | mel_loss: 12.831 | 3.045 sec / 10 steps |\n",
      "| step: 424390 | gen_loss: 44.662 | mel_loss: 10.310 | 3.200 sec / 10 steps |\n",
      "| step: 424400 | gen_loss: 46.891 | mel_loss: 11.253 | 3.353 sec / 10 steps |\n",
      "Validation mel_loss: 14.096162796020508\n",
      "| step: 424410 | gen_loss: 46.024 | mel_loss: 10.297 | 3.309 sec / 10 steps |\n",
      "| step: 424420 | gen_loss: 46.653 | mel_loss: 10.952 | 3.323 sec / 10 steps |\n",
      "| step: 424430 | gen_loss: 50.062 | mel_loss: 12.099 | 3.273 sec / 10 steps |\n",
      "| step: 424440 | gen_loss: 47.657 | mel_loss: 11.799 | 3.309 sec / 10 steps |\n",
      "| step: 424450 | gen_loss: 45.917 | mel_loss: 10.299 | 3.329 sec / 10 steps |\n",
      "| step: 424460 | gen_loss: 45.846 | mel_loss: 10.375 | 3.097 sec / 10 steps |\n",
      "| step: 424470 | gen_loss: 46.666 | mel_loss: 11.234 | 3.014 sec / 10 steps |\n",
      "| step: 424480 | gen_loss: 46.291 | mel_loss: 11.443 | 3.252 sec / 10 steps |\n",
      "| step: 424490 | gen_loss: 50.404 | mel_loss: 11.671 | 3.307 sec / 10 steps |\n",
      "| step: 424500 | gen_loss: 45.642 | mel_loss: 10.631 | 3.226 sec / 10 steps |\n",
      "Validation mel_loss: 14.02874755859375\n",
      "| step: 424510 | gen_loss: 51.858 | mel_loss: 12.291 | 3.035 sec / 10 steps |\n",
      "| step: 424520 | gen_loss: 50.977 | mel_loss: 12.396 | 2.901 sec / 10 steps |\n",
      "| step: 424530 | gen_loss: 47.442 | mel_loss: 11.855 | 3.383 sec / 10 steps |\n",
      "| step: 424540 | gen_loss: 50.911 | mel_loss: 12.374 | 3.466 sec / 10 steps |\n",
      "| step: 424550 | gen_loss: 48.518 | mel_loss: 12.034 | 3.680 sec / 10 steps |\n",
      "| step: 424560 | gen_loss: 47.477 | mel_loss: 11.997 | 3.219 sec / 10 steps |\n",
      "| step: 424570 | gen_loss: 48.906 | mel_loss: 12.319 | 3.318 sec / 10 steps |\n",
      "| step: 424580 | gen_loss: 48.044 | mel_loss: 12.424 | 3.031 sec / 10 steps |\n",
      "| step: 424590 | gen_loss: 45.768 | mel_loss: 10.175 | 3.566 sec / 10 steps |\n",
      "| step: 424600 | gen_loss: 48.414 | mel_loss: 11.389 | 3.476 sec / 10 steps |\n",
      "Validation mel_loss: 14.167311668395996\n",
      "| step: 424610 | gen_loss: 46.251 | mel_loss: 11.260 | 3.405 sec / 10 steps |\n",
      "| step: 424620 | gen_loss: 41.442 | mel_loss: 10.755 | 2.922 sec / 10 steps |\n",
      "| step: 424630 | gen_loss: 50.078 | mel_loss: 12.529 | 3.182 sec / 10 steps |\n",
      "| step: 424640 | gen_loss: 46.783 | mel_loss: 11.315 | 3.190 sec / 10 steps |\n",
      "| step: 424650 | gen_loss: 47.127 | mel_loss: 11.651 | 3.345 sec / 10 steps |\n",
      "| step: 424660 | gen_loss: 43.698 | mel_loss: 10.189 | 3.294 sec / 10 steps |\n",
      "| step: 424670 | gen_loss: 41.182 | mel_loss: 8.735 | 3.561 sec / 10 steps |\n",
      "| step: 424680 | gen_loss: 45.196 | mel_loss: 9.925 | 3.248 sec / 10 steps |\n",
      "| step: 424690 | gen_loss: 49.065 | mel_loss: 11.803 | 3.138 sec / 10 steps |\n",
      "| step: 424700 | gen_loss: 49.564 | mel_loss: 12.018 | 3.066 sec / 10 steps |\n",
      "Validation mel_loss: 14.161075592041016\n",
      "| step: 424710 | gen_loss: 41.411 | mel_loss: 9.927 | 3.568 sec / 10 steps |\n",
      "| step: 424720 | gen_loss: 47.958 | mel_loss: 11.137 | 3.363 sec / 10 steps |\n",
      "| step: 424730 | gen_loss: 44.653 | mel_loss: 10.002 | 3.271 sec / 10 steps |\n",
      "| step: 424740 | gen_loss: 50.514 | mel_loss: 11.912 | 3.357 sec / 10 steps |\n",
      "| step: 424750 | gen_loss: 51.262 | mel_loss: 11.954 | 3.309 sec / 10 steps |\n",
      "| step: 424760 | gen_loss: 48.882 | mel_loss: 11.808 | 3.706 sec / 10 steps |\n",
      "| step: 424770 | gen_loss: 41.279 | mel_loss: 10.251 | 3.325 sec / 10 steps |\n",
      "| step: 424780 | gen_loss: 47.226 | mel_loss: 11.526 | 3.061 sec / 10 steps |\n",
      "| step: 424790 | gen_loss: 46.909 | mel_loss: 10.726 | 3.194 sec / 10 steps |\n",
      "| step: 424800 | gen_loss: 44.830 | mel_loss: 11.594 | 3.272 sec / 10 steps |\n",
      "Validation mel_loss: 14.069052696228027\n",
      "| step: 424810 | gen_loss: 41.252 | mel_loss: 9.760 | 3.095 sec / 10 steps |\n",
      "| step: 424820 | gen_loss: 49.100 | mel_loss: 12.240 | 3.242 sec / 10 steps |\n",
      "| step: 424830 | gen_loss: 49.224 | mel_loss: 12.067 | 3.309 sec / 10 steps |\n",
      "| step: 424840 | gen_loss: 49.486 | mel_loss: 11.707 | 2.770 sec / 10 steps |\n",
      "| step: 424850 | gen_loss: 51.227 | mel_loss: 12.924 | 3.163 sec / 10 steps |\n",
      "| step: 424860 | gen_loss: 48.558 | mel_loss: 11.526 | 3.207 sec / 10 steps |\n",
      "| step: 424870 | gen_loss: 45.196 | mel_loss: 9.527 | 3.171 sec / 10 steps |\n",
      "| step: 424880 | gen_loss: 46.178 | mel_loss: 11.387 | 3.158 sec / 10 steps |\n",
      "| step: 424890 | gen_loss: 48.230 | mel_loss: 11.522 | 3.130 sec / 10 steps |\n",
      "| step: 424900 | gen_loss: 49.913 | mel_loss: 12.756 | 3.225 sec / 10 steps |\n",
      "Validation mel_loss: 14.056333541870117\n",
      "| step: 424910 | gen_loss: 51.545 | mel_loss: 13.481 | 3.337 sec / 10 steps |\n",
      "| step: 424920 | gen_loss: 43.810 | mel_loss: 10.340 | 3.358 sec / 10 steps |\n",
      "| step: 424930 | gen_loss: 43.492 | mel_loss: 10.469 | 3.116 sec / 10 steps |\n",
      "| step: 424940 | gen_loss: 47.689 | mel_loss: 11.011 | 3.785 sec / 10 steps |\n",
      "| step: 424950 | gen_loss: 49.137 | mel_loss: 10.860 | 3.156 sec / 10 steps |\n",
      "| step: 424960 | gen_loss: 54.284 | mel_loss: 13.545 | 3.117 sec / 10 steps |\n",
      "| step: 424970 | gen_loss: 48.957 | mel_loss: 11.615 | 3.397 sec / 10 steps |\n",
      "| step: 424980 | gen_loss: 45.910 | mel_loss: 11.454 | 3.261 sec / 10 steps |\n",
      "| step: 424990 | gen_loss: 45.278 | mel_loss: 11.121 | 3.526 sec / 10 steps |\n",
      "| step: 425000 | gen_loss: 42.963 | mel_loss: 9.708 | 3.238 sec / 10 steps |\n",
      "Validation mel_loss: 13.986587524414062\n",
      "|| Epoch: 566 ||\n",
      "| step: 425010 | gen_loss: 46.721 | mel_loss: 11.656 | 3.585 sec / 10 steps |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 425020 | gen_loss: 53.457 | mel_loss: 12.983 | 3.032 sec / 10 steps |\n",
      "| step: 425030 | gen_loss: 49.816 | mel_loss: 11.575 | 3.073 sec / 10 steps |\n",
      "| step: 425040 | gen_loss: 49.914 | mel_loss: 12.251 | 3.110 sec / 10 steps |\n",
      "| step: 425050 | gen_loss: 44.258 | mel_loss: 10.472 | 3.233 sec / 10 steps |\n",
      "| step: 425060 | gen_loss: 55.785 | mel_loss: 13.794 | 3.021 sec / 10 steps |\n",
      "| step: 425070 | gen_loss: 39.753 | mel_loss: 8.747 | 3.203 sec / 10 steps |\n",
      "| step: 425080 | gen_loss: 45.591 | mel_loss: 11.817 | 3.089 sec / 10 steps |\n",
      "| step: 425090 | gen_loss: 49.105 | mel_loss: 12.106 | 3.229 sec / 10 steps |\n",
      "| step: 425100 | gen_loss: 47.889 | mel_loss: 11.455 | 3.113 sec / 10 steps |\n",
      "Validation mel_loss: 14.061802864074707\n",
      "| step: 425110 | gen_loss: 43.473 | mel_loss: 10.026 | 3.764 sec / 10 steps |\n",
      "| step: 425120 | gen_loss: 50.850 | mel_loss: 12.038 | 3.093 sec / 10 steps |\n",
      "| step: 425130 | gen_loss: 46.262 | mel_loss: 11.091 | 3.374 sec / 10 steps |\n",
      "| step: 425140 | gen_loss: 42.079 | mel_loss: 9.904 | 3.158 sec / 10 steps |\n",
      "| step: 425150 | gen_loss: 41.490 | mel_loss: 9.151 | 3.431 sec / 10 steps |\n",
      "| step: 425160 | gen_loss: 44.622 | mel_loss: 10.469 | 3.174 sec / 10 steps |\n",
      "| step: 425170 | gen_loss: 49.705 | mel_loss: 12.154 | 3.213 sec / 10 steps |\n",
      "| step: 425180 | gen_loss: 50.514 | mel_loss: 12.265 | 3.613 sec / 10 steps |\n",
      "| step: 425190 | gen_loss: 47.561 | mel_loss: 11.225 | 3.114 sec / 10 steps |\n",
      "| step: 425200 | gen_loss: 50.333 | mel_loss: 12.433 | 3.148 sec / 10 steps |\n",
      "Validation mel_loss: 14.11831283569336\n",
      "| step: 425210 | gen_loss: 46.788 | mel_loss: 11.411 | 3.136 sec / 10 steps |\n",
      "| step: 425220 | gen_loss: 46.395 | mel_loss: 10.059 | 3.643 sec / 10 steps |\n",
      "| step: 425230 | gen_loss: 42.956 | mel_loss: 8.695 | 3.706 sec / 10 steps |\n",
      "| step: 425240 | gen_loss: 47.901 | mel_loss: 11.219 | 3.498 sec / 10 steps |\n",
      "| step: 425250 | gen_loss: 42.230 | mel_loss: 9.472 | 3.515 sec / 10 steps |\n",
      "| step: 425260 | gen_loss: 48.085 | mel_loss: 11.583 | 3.278 sec / 10 steps |\n",
      "| step: 425270 | gen_loss: 45.297 | mel_loss: 9.796 | 3.258 sec / 10 steps |\n",
      "| step: 425280 | gen_loss: 49.622 | mel_loss: 12.606 | 3.254 sec / 10 steps |\n",
      "| step: 425290 | gen_loss: 51.188 | mel_loss: 12.466 | 3.268 sec / 10 steps |\n",
      "| step: 425300 | gen_loss: 50.296 | mel_loss: 12.449 | 3.228 sec / 10 steps |\n",
      "Validation mel_loss: 14.036887168884277\n",
      "| step: 425310 | gen_loss: 40.934 | mel_loss: 8.959 | 3.183 sec / 10 steps |\n",
      "| step: 425320 | gen_loss: 39.274 | mel_loss: 8.650 | 3.447 sec / 10 steps |\n",
      "| step: 425330 | gen_loss: 48.017 | mel_loss: 11.324 | 3.071 sec / 10 steps |\n",
      "| step: 425340 | gen_loss: 48.536 | mel_loss: 11.507 | 3.686 sec / 10 steps |\n",
      "| step: 425350 | gen_loss: 47.370 | mel_loss: 11.785 | 3.123 sec / 10 steps |\n",
      "| step: 425360 | gen_loss: 46.761 | mel_loss: 10.684 | 3.346 sec / 10 steps |\n",
      "| step: 425370 | gen_loss: 42.711 | mel_loss: 9.680 | 4.419 sec / 10 steps |\n",
      "| step: 425380 | gen_loss: 48.958 | mel_loss: 11.977 | 3.053 sec / 10 steps |\n",
      "| step: 425390 | gen_loss: 40.796 | mel_loss: 8.859 | 3.427 sec / 10 steps |\n",
      "| step: 425400 | gen_loss: 51.346 | mel_loss: 12.385 | 2.924 sec / 10 steps |\n",
      "Validation mel_loss: 13.856776237487793\n",
      "| step: 425410 | gen_loss: 47.498 | mel_loss: 11.775 | 3.060 sec / 10 steps |\n",
      "| step: 425420 | gen_loss: 48.589 | mel_loss: 11.797 | 3.221 sec / 10 steps |\n",
      "| step: 425430 | gen_loss: 49.298 | mel_loss: 11.447 | 3.413 sec / 10 steps |\n",
      "| step: 425440 | gen_loss: 47.216 | mel_loss: 10.900 | 3.096 sec / 10 steps |\n",
      "| step: 425450 | gen_loss: 47.020 | mel_loss: 11.324 | 3.038 sec / 10 steps |\n",
      "| step: 425460 | gen_loss: 44.042 | mel_loss: 10.266 | 2.897 sec / 10 steps |\n",
      "| step: 425470 | gen_loss: 48.386 | mel_loss: 11.360 | 3.211 sec / 10 steps |\n",
      "| step: 425480 | gen_loss: 48.869 | mel_loss: 11.571 | 3.230 sec / 10 steps |\n",
      "| step: 425490 | gen_loss: 47.003 | mel_loss: 10.754 | 3.437 sec / 10 steps |\n",
      "| step: 425500 | gen_loss: 48.324 | mel_loss: 11.466 | 3.203 sec / 10 steps |\n",
      "Validation mel_loss: 14.115363121032715\n",
      "| step: 425510 | gen_loss: 52.428 | mel_loss: 12.451 | 3.072 sec / 10 steps |\n",
      "| step: 425520 | gen_loss: 44.607 | mel_loss: 10.185 | 3.110 sec / 10 steps |\n",
      "| step: 425530 | gen_loss: 50.266 | mel_loss: 11.588 | 3.285 sec / 10 steps |\n",
      "| step: 425540 | gen_loss: 44.957 | mel_loss: 10.625 | 3.486 sec / 10 steps |\n",
      "| step: 425550 | gen_loss: 43.532 | mel_loss: 9.614 | 3.344 sec / 10 steps |\n",
      "| step: 425560 | gen_loss: 42.075 | mel_loss: 9.897 | 3.233 sec / 10 steps |\n",
      "| step: 425570 | gen_loss: 40.072 | mel_loss: 10.153 | 3.216 sec / 10 steps |\n",
      "| step: 425580 | gen_loss: 51.724 | mel_loss: 12.123 | 3.071 sec / 10 steps |\n",
      "| step: 425590 | gen_loss: 46.692 | mel_loss: 10.712 | 3.444 sec / 10 steps |\n",
      "| step: 425600 | gen_loss: 49.137 | mel_loss: 11.995 | 3.216 sec / 10 steps |\n",
      "Validation mel_loss: 14.04101276397705\n",
      "| step: 425610 | gen_loss: 44.998 | mel_loss: 10.693 | 3.253 sec / 10 steps |\n",
      "| step: 425620 | gen_loss: 47.193 | mel_loss: 11.256 | 3.357 sec / 10 steps |\n",
      "| step: 425630 | gen_loss: 48.569 | mel_loss: 12.102 | 3.055 sec / 10 steps |\n",
      "| step: 425640 | gen_loss: 41.179 | mel_loss: 9.311 | 3.400 sec / 10 steps |\n",
      "| step: 425650 | gen_loss: 44.607 | mel_loss: 10.193 | 3.608 sec / 10 steps |\n",
      "| step: 425660 | gen_loss: 47.128 | mel_loss: 10.986 | 3.279 sec / 10 steps |\n",
      "| step: 425670 | gen_loss: 46.235 | mel_loss: 11.088 | 3.330 sec / 10 steps |\n",
      "| step: 425680 | gen_loss: 47.523 | mel_loss: 10.740 | 2.965 sec / 10 steps |\n",
      "| step: 425690 | gen_loss: 44.151 | mel_loss: 9.579 | 3.238 sec / 10 steps |\n",
      "| step: 425700 | gen_loss: 42.339 | mel_loss: 10.711 | 3.363 sec / 10 steps |\n",
      "Validation mel_loss: 14.017300605773926\n",
      "| step: 425710 | gen_loss: 35.918 | mel_loss: 8.207 | 3.224 sec / 10 steps |\n",
      "| step: 425720 | gen_loss: 42.622 | mel_loss: 9.683 | 3.215 sec / 10 steps |\n",
      "| step: 425730 | gen_loss: 48.609 | mel_loss: 11.741 | 3.267 sec / 10 steps |\n",
      "| step: 425740 | gen_loss: 45.404 | mel_loss: 10.309 | 3.317 sec / 10 steps |\n",
      "| step: 425750 | gen_loss: 43.512 | mel_loss: 9.206 | 3.362 sec / 10 steps |\n",
      "|| Epoch: 567 ||\n",
      "| step: 425760 | gen_loss: 44.727 | mel_loss: 10.122 | 3.330 sec / 10 steps |\n",
      "| step: 425770 | gen_loss: 42.608 | mel_loss: 9.148 | 3.283 sec / 10 steps |\n",
      "| step: 425780 | gen_loss: 43.582 | mel_loss: 9.492 | 3.057 sec / 10 steps |\n",
      "| step: 425790 | gen_loss: 44.770 | mel_loss: 9.954 | 3.185 sec / 10 steps |\n",
      "| step: 425800 | gen_loss: 41.650 | mel_loss: 9.547 | 3.091 sec / 10 steps |\n",
      "Validation mel_loss: 14.022383689880371\n",
      "| step: 425810 | gen_loss: 52.091 | mel_loss: 12.671 | 3.568 sec / 10 steps |\n",
      "| step: 425820 | gen_loss: 43.601 | mel_loss: 10.154 | 3.147 sec / 10 steps |\n",
      "| step: 425830 | gen_loss: 43.172 | mel_loss: 11.101 | 2.901 sec / 10 steps |\n",
      "| step: 425840 | gen_loss: 47.293 | mel_loss: 11.094 | 3.566 sec / 10 steps |\n",
      "| step: 425850 | gen_loss: 44.797 | mel_loss: 9.870 | 3.198 sec / 10 steps |\n",
      "| step: 425860 | gen_loss: 46.644 | mel_loss: 10.226 | 3.108 sec / 10 steps |\n",
      "| step: 425870 | gen_loss: 48.606 | mel_loss: 11.894 | 3.634 sec / 10 steps |\n",
      "| step: 425880 | gen_loss: 48.419 | mel_loss: 10.526 | 3.234 sec / 10 steps |\n",
      "| step: 425890 | gen_loss: 44.041 | mel_loss: 9.715 | 3.544 sec / 10 steps |\n",
      "| step: 425900 | gen_loss: 52.608 | mel_loss: 12.686 | 3.032 sec / 10 steps |\n",
      "Validation mel_loss: 13.809019088745117\n",
      "| step: 425910 | gen_loss: 50.823 | mel_loss: 12.965 | 3.195 sec / 10 steps |\n",
      "| step: 425920 | gen_loss: 37.754 | mel_loss: 8.665 | 3.167 sec / 10 steps |\n",
      "| step: 425930 | gen_loss: 47.711 | mel_loss: 11.160 | 3.191 sec / 10 steps |\n",
      "| step: 425940 | gen_loss: 42.928 | mel_loss: 9.858 | 3.552 sec / 10 steps |\n",
      "| step: 425950 | gen_loss: 50.299 | mel_loss: 11.988 | 2.967 sec / 10 steps |\n",
      "| step: 425960 | gen_loss: 42.948 | mel_loss: 9.497 | 2.999 sec / 10 steps |\n",
      "| step: 425970 | gen_loss: 50.758 | mel_loss: 13.247 | 3.213 sec / 10 steps |\n",
      "| step: 425980 | gen_loss: 46.561 | mel_loss: 10.984 | 3.164 sec / 10 steps |\n",
      "| step: 425990 | gen_loss: 41.701 | mel_loss: 9.995 | 3.224 sec / 10 steps |\n",
      "| step: 426000 | gen_loss: 48.129 | mel_loss: 10.925 | 3.150 sec / 10 steps |\n",
      "Validation mel_loss: 14.014187812805176\n",
      "| step: 426010 | gen_loss: 47.012 | mel_loss: 10.467 | 3.371 sec / 10 steps |\n",
      "| step: 426020 | gen_loss: 50.712 | mel_loss: 12.427 | 3.410 sec / 10 steps |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 426030 | gen_loss: 44.737 | mel_loss: 9.698 | 3.377 sec / 10 steps |\n",
      "| step: 426040 | gen_loss: 48.274 | mel_loss: 11.618 | 2.887 sec / 10 steps |\n",
      "| step: 426050 | gen_loss: 42.268 | mel_loss: 9.144 | 3.296 sec / 10 steps |\n",
      "| step: 426060 | gen_loss: 44.036 | mel_loss: 10.550 | 3.528 sec / 10 steps |\n",
      "| step: 426070 | gen_loss: 43.992 | mel_loss: 9.853 | 3.725 sec / 10 steps |\n",
      "| step: 426080 | gen_loss: 49.923 | mel_loss: 11.851 | 3.310 sec / 10 steps |\n",
      "| step: 426090 | gen_loss: 49.243 | mel_loss: 10.808 | 3.169 sec / 10 steps |\n",
      "| step: 426100 | gen_loss: 40.642 | mel_loss: 9.274 | 3.318 sec / 10 steps |\n",
      "Validation mel_loss: 13.913726806640625\n",
      "| step: 426110 | gen_loss: 45.671 | mel_loss: 10.627 | 3.713 sec / 10 steps |\n",
      "| step: 426120 | gen_loss: 47.863 | mel_loss: 10.922 | 3.112 sec / 10 steps |\n",
      "| step: 426130 | gen_loss: 49.742 | mel_loss: 12.328 | 3.246 sec / 10 steps |\n",
      "| step: 426140 | gen_loss: 43.796 | mel_loss: 10.296 | 3.150 sec / 10 steps |\n",
      "| step: 426150 | gen_loss: 46.027 | mel_loss: 10.039 | 3.652 sec / 10 steps |\n",
      "| step: 426160 | gen_loss: 50.431 | mel_loss: 12.456 | 3.328 sec / 10 steps |\n",
      "| step: 426170 | gen_loss: 46.415 | mel_loss: 10.456 | 3.039 sec / 10 steps |\n",
      "| step: 426180 | gen_loss: 45.154 | mel_loss: 10.336 | 3.367 sec / 10 steps |\n",
      "| step: 426190 | gen_loss: 46.845 | mel_loss: 10.736 | 3.532 sec / 10 steps |\n",
      "| step: 426200 | gen_loss: 52.671 | mel_loss: 12.477 | 3.169 sec / 10 steps |\n",
      "Validation mel_loss: 14.415640830993652\n",
      "| step: 426210 | gen_loss: 51.027 | mel_loss: 12.713 | 3.055 sec / 10 steps |\n",
      "| step: 426220 | gen_loss: 49.643 | mel_loss: 11.508 | 3.328 sec / 10 steps |\n",
      "| step: 426230 | gen_loss: 49.227 | mel_loss: 12.088 | 3.004 sec / 10 steps |\n",
      "| step: 426240 | gen_loss: 50.097 | mel_loss: 11.682 | 3.519 sec / 10 steps |\n",
      "| step: 426250 | gen_loss: 53.109 | mel_loss: 13.079 | 3.442 sec / 10 steps |\n",
      "| step: 426260 | gen_loss: 50.415 | mel_loss: 12.935 | 3.382 sec / 10 steps |\n",
      "| step: 426270 | gen_loss: 44.433 | mel_loss: 10.298 | 3.698 sec / 10 steps |\n",
      "| step: 426280 | gen_loss: 43.357 | mel_loss: 10.575 | 3.388 sec / 10 steps |\n",
      "| step: 426290 | gen_loss: 40.729 | mel_loss: 8.654 | 3.480 sec / 10 steps |\n",
      "| step: 426300 | gen_loss: 45.414 | mel_loss: 11.238 | 3.151 sec / 10 steps |\n",
      "Validation mel_loss: 14.093278884887695\n",
      "| step: 426310 | gen_loss: 44.337 | mel_loss: 10.150 | 3.369 sec / 10 steps |\n",
      "| step: 426320 | gen_loss: 38.553 | mel_loss: 9.234 | 3.678 sec / 10 steps |\n",
      "| step: 426330 | gen_loss: 54.349 | mel_loss: 12.429 | 3.078 sec / 10 steps |\n",
      "| step: 426340 | gen_loss: 43.183 | mel_loss: 9.645 | 3.250 sec / 10 steps |\n",
      "| step: 426350 | gen_loss: 48.222 | mel_loss: 11.527 | 2.944 sec / 10 steps |\n",
      "| step: 426360 | gen_loss: 41.142 | mel_loss: 8.864 | 3.184 sec / 10 steps |\n",
      "| step: 426370 | gen_loss: 45.584 | mel_loss: 10.950 | 3.396 sec / 10 steps |\n",
      "| step: 426380 | gen_loss: 48.943 | mel_loss: 11.569 | 3.089 sec / 10 steps |\n",
      "| step: 426390 | gen_loss: 48.289 | mel_loss: 11.428 | 3.247 sec / 10 steps |\n",
      "| step: 426400 | gen_loss: 42.919 | mel_loss: 9.241 | 3.271 sec / 10 steps |\n",
      "Validation mel_loss: 13.897074699401855\n",
      "| step: 426410 | gen_loss: 49.939 | mel_loss: 11.972 | 3.155 sec / 10 steps |\n",
      "| step: 426420 | gen_loss: 44.628 | mel_loss: 10.016 | 3.406 sec / 10 steps |\n",
      "| step: 426430 | gen_loss: 50.725 | mel_loss: 13.100 | 2.996 sec / 10 steps |\n",
      "| step: 426440 | gen_loss: 41.723 | mel_loss: 9.232 | 3.208 sec / 10 steps |\n",
      "| step: 426450 | gen_loss: 45.162 | mel_loss: 9.958 | 3.151 sec / 10 steps |\n",
      "| step: 426460 | gen_loss: 45.404 | mel_loss: 11.308 | 2.937 sec / 10 steps |\n",
      "| step: 426470 | gen_loss: 42.468 | mel_loss: 10.074 | 3.284 sec / 10 steps |\n",
      "| step: 426480 | gen_loss: 43.182 | mel_loss: 9.606 | 2.910 sec / 10 steps |\n",
      "| step: 426490 | gen_loss: 45.181 | mel_loss: 9.921 | 2.894 sec / 10 steps |\n",
      "| step: 426500 | gen_loss: 50.410 | mel_loss: 11.666 | 3.147 sec / 10 steps |\n",
      "Validation mel_loss: 13.984684944152832\n",
      "|| Epoch: 568 ||\n",
      "| step: 426510 | gen_loss: 50.589 | mel_loss: 12.483 | 3.202 sec / 10 steps |\n",
      "| step: 426520 | gen_loss: 47.011 | mel_loss: 11.104 | 3.277 sec / 10 steps |\n",
      "| step: 426530 | gen_loss: 40.283 | mel_loss: 8.426 | 3.517 sec / 10 steps |\n",
      "| step: 426540 | gen_loss: 41.311 | mel_loss: 9.791 | 3.447 sec / 10 steps |\n",
      "| step: 426550 | gen_loss: 46.655 | mel_loss: 11.475 | 3.282 sec / 10 steps |\n",
      "| step: 426560 | gen_loss: 47.114 | mel_loss: 12.104 | 2.828 sec / 10 steps |\n",
      "| step: 426570 | gen_loss: 37.907 | mel_loss: 9.763 | 3.162 sec / 10 steps |\n",
      "| step: 426580 | gen_loss: 49.679 | mel_loss: 12.195 | 2.840 sec / 10 steps |\n",
      "| step: 426590 | gen_loss: 44.968 | mel_loss: 10.062 | 3.346 sec / 10 steps |\n",
      "| step: 426600 | gen_loss: 51.099 | mel_loss: 12.061 | 3.047 sec / 10 steps |\n",
      "Validation mel_loss: 13.919116020202637\n",
      "| step: 426610 | gen_loss: 40.623 | mel_loss: 8.858 | 3.132 sec / 10 steps |\n",
      "| step: 426620 | gen_loss: 48.579 | mel_loss: 10.832 | 3.021 sec / 10 steps |\n",
      "| step: 426630 | gen_loss: 44.657 | mel_loss: 10.373 | 3.649 sec / 10 steps |\n",
      "| step: 426640 | gen_loss: 50.518 | mel_loss: 12.553 | 3.222 sec / 10 steps |\n",
      "| step: 426650 | gen_loss: 45.941 | mel_loss: 10.891 | 2.958 sec / 10 steps |\n",
      "| step: 426660 | gen_loss: 47.854 | mel_loss: 11.245 | 2.979 sec / 10 steps |\n",
      "| step: 426670 | gen_loss: 49.141 | mel_loss: 11.617 | 3.162 sec / 10 steps |\n",
      "| step: 426680 | gen_loss: 46.165 | mel_loss: 11.241 | 3.150 sec / 10 steps |\n",
      "| step: 426690 | gen_loss: 47.992 | mel_loss: 11.285 | 3.264 sec / 10 steps |\n",
      "| step: 426700 | gen_loss: 46.456 | mel_loss: 11.782 | 3.223 sec / 10 steps |\n",
      "Validation mel_loss: 14.375078201293945\n",
      "| step: 426710 | gen_loss: 47.085 | mel_loss: 11.356 | 3.534 sec / 10 steps |\n",
      "| step: 426720 | gen_loss: 37.015 | mel_loss: 8.370 | 3.436 sec / 10 steps |\n",
      "| step: 426730 | gen_loss: 48.381 | mel_loss: 11.809 | 3.215 sec / 10 steps |\n",
      "| step: 426740 | gen_loss: 34.743 | mel_loss: 8.991 | 3.342 sec / 10 steps |\n",
      "| step: 426750 | gen_loss: 45.736 | mel_loss: 11.044 | 2.955 sec / 10 steps |\n",
      "| step: 426760 | gen_loss: 50.167 | mel_loss: 12.046 | 2.968 sec / 10 steps |\n",
      "| step: 426770 | gen_loss: 47.926 | mel_loss: 11.184 | 3.245 sec / 10 steps |\n",
      "| step: 426780 | gen_loss: 48.380 | mel_loss: 12.185 | 3.365 sec / 10 steps |\n",
      "| step: 426790 | gen_loss: 49.992 | mel_loss: 11.494 | 3.712 sec / 10 steps |\n",
      "| step: 426800 | gen_loss: 43.682 | mel_loss: 9.725 | 3.649 sec / 10 steps |\n",
      "Validation mel_loss: 13.971911430358887\n",
      "| step: 426810 | gen_loss: 49.883 | mel_loss: 11.411 | 3.297 sec / 10 steps |\n",
      "| step: 426820 | gen_loss: 43.646 | mel_loss: 9.958 | 3.298 sec / 10 steps |\n",
      "| step: 426830 | gen_loss: 46.628 | mel_loss: 11.375 | 3.227 sec / 10 steps |\n",
      "| step: 426840 | gen_loss: 46.225 | mel_loss: 11.856 | 3.123 sec / 10 steps |\n",
      "| step: 426850 | gen_loss: 50.338 | mel_loss: 12.479 | 3.263 sec / 10 steps |\n",
      "| step: 426860 | gen_loss: 44.635 | mel_loss: 10.600 | 3.159 sec / 10 steps |\n",
      "| step: 426870 | gen_loss: 42.758 | mel_loss: 9.249 | 3.972 sec / 10 steps |\n",
      "| step: 426880 | gen_loss: 52.768 | mel_loss: 12.542 | 3.343 sec / 10 steps |\n",
      "| step: 426890 | gen_loss: 47.255 | mel_loss: 10.724 | 3.616 sec / 10 steps |\n",
      "| step: 426900 | gen_loss: 44.872 | mel_loss: 10.550 | 3.711 sec / 10 steps |\n",
      "Validation mel_loss: 13.831453323364258\n",
      "| step: 426910 | gen_loss: 47.297 | mel_loss: 11.737 | 3.526 sec / 10 steps |\n",
      "| step: 426920 | gen_loss: 42.364 | mel_loss: 9.253 | 3.326 sec / 10 steps |\n",
      "| step: 426930 | gen_loss: 42.011 | mel_loss: 9.729 | 3.595 sec / 10 steps |\n",
      "| step: 426940 | gen_loss: 47.731 | mel_loss: 11.200 | 3.678 sec / 10 steps |\n",
      "| step: 426950 | gen_loss: 44.795 | mel_loss: 10.345 | 3.265 sec / 10 steps |\n",
      "| step: 426960 | gen_loss: 52.574 | mel_loss: 12.589 | 3.117 sec / 10 steps |\n",
      "| step: 426970 | gen_loss: 41.295 | mel_loss: 8.702 | 3.200 sec / 10 steps |\n",
      "| step: 426980 | gen_loss: 40.581 | mel_loss: 9.508 | 3.011 sec / 10 steps |\n",
      "| step: 426990 | gen_loss: 44.565 | mel_loss: 9.737 | 3.378 sec / 10 steps |\n",
      "| step: 427000 | gen_loss: 46.414 | mel_loss: 11.061 | 3.312 sec / 10 steps |\n",
      "Validation mel_loss: 14.00625991821289\n",
      "| step: 427010 | gen_loss: 38.242 | mel_loss: 8.979 | 3.208 sec / 10 steps |\n",
      "| step: 427020 | gen_loss: 47.099 | mel_loss: 10.954 | 2.954 sec / 10 steps |\n",
      "| step: 427030 | gen_loss: 42.982 | mel_loss: 9.415 | 3.410 sec / 10 steps |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 427040 | gen_loss: 45.151 | mel_loss: 10.377 | 3.125 sec / 10 steps |\n",
      "| step: 427050 | gen_loss: 48.489 | mel_loss: 12.393 | 3.361 sec / 10 steps |\n",
      "| step: 427060 | gen_loss: 47.593 | mel_loss: 11.763 | 3.047 sec / 10 steps |\n",
      "| step: 427070 | gen_loss: 44.868 | mel_loss: 10.689 | 3.077 sec / 10 steps |\n",
      "| step: 427080 | gen_loss: 41.475 | mel_loss: 8.665 | 3.322 sec / 10 steps |\n",
      "| step: 427090 | gen_loss: 44.895 | mel_loss: 9.503 | 3.389 sec / 10 steps |\n",
      "| step: 427100 | gen_loss: 46.476 | mel_loss: 10.991 | 3.132 sec / 10 steps |\n",
      "Validation mel_loss: 13.871593475341797\n",
      "| step: 427110 | gen_loss: 46.008 | mel_loss: 10.475 | 3.324 sec / 10 steps |\n",
      "| step: 427120 | gen_loss: 46.717 | mel_loss: 10.868 | 3.311 sec / 10 steps |\n",
      "| step: 427130 | gen_loss: 49.227 | mel_loss: 12.324 | 3.247 sec / 10 steps |\n",
      "| step: 427140 | gen_loss: 46.075 | mel_loss: 10.088 | 3.218 sec / 10 steps |\n",
      "| step: 427150 | gen_loss: 52.178 | mel_loss: 12.283 | 3.018 sec / 10 steps |\n",
      "| step: 427160 | gen_loss: 47.934 | mel_loss: 11.808 | 3.365 sec / 10 steps |\n",
      "| step: 427170 | gen_loss: 48.699 | mel_loss: 11.162 | 3.477 sec / 10 steps |\n",
      "| step: 427180 | gen_loss: 51.160 | mel_loss: 12.496 | 3.089 sec / 10 steps |\n",
      "| step: 427190 | gen_loss: 48.680 | mel_loss: 11.175 | 3.078 sec / 10 steps |\n",
      "| step: 427200 | gen_loss: 46.310 | mel_loss: 11.459 | 3.244 sec / 10 steps |\n",
      "Validation mel_loss: 13.88233757019043\n",
      "| step: 427210 | gen_loss: 52.339 | mel_loss: 12.405 | 3.447 sec / 10 steps |\n",
      "| step: 427220 | gen_loss: 41.262 | mel_loss: 9.142 | 2.968 sec / 10 steps |\n",
      "| step: 427230 | gen_loss: 51.506 | mel_loss: 12.677 | 3.148 sec / 10 steps |\n",
      "| step: 427240 | gen_loss: 41.244 | mel_loss: 9.799 | 3.387 sec / 10 steps |\n",
      "| step: 427250 | gen_loss: 47.517 | mel_loss: 11.259 | 3.401 sec / 10 steps |\n",
      "|| Epoch: 569 ||\n",
      "| step: 427260 | gen_loss: 44.852 | mel_loss: 10.110 | 3.165 sec / 10 steps |\n",
      "| step: 427270 | gen_loss: 49.235 | mel_loss: 11.102 | 3.281 sec / 10 steps |\n",
      "| step: 427280 | gen_loss: 48.698 | mel_loss: 11.354 | 3.237 sec / 10 steps |\n",
      "| step: 427290 | gen_loss: 50.312 | mel_loss: 12.374 | 3.182 sec / 10 steps |\n",
      "| step: 427300 | gen_loss: 48.530 | mel_loss: 11.173 | 3.191 sec / 10 steps |\n",
      "Validation mel_loss: 13.894208908081055\n",
      "| step: 427310 | gen_loss: 45.667 | mel_loss: 11.255 | 3.161 sec / 10 steps |\n",
      "| step: 427320 | gen_loss: 51.629 | mel_loss: 12.358 | 3.347 sec / 10 steps |\n",
      "| step: 427330 | gen_loss: 47.450 | mel_loss: 11.070 | 3.191 sec / 10 steps |\n",
      "| step: 427340 | gen_loss: 42.373 | mel_loss: 8.726 | 3.450 sec / 10 steps |\n",
      "| step: 427350 | gen_loss: 47.610 | mel_loss: 11.496 | 3.094 sec / 10 steps |\n",
      "| step: 427360 | gen_loss: 43.653 | mel_loss: 10.988 | 3.368 sec / 10 steps |\n",
      "| step: 427370 | gen_loss: 49.938 | mel_loss: 12.030 | 3.163 sec / 10 steps |\n",
      "| step: 427380 | gen_loss: 49.900 | mel_loss: 12.316 | 3.654 sec / 10 steps |\n",
      "| step: 427390 | gen_loss: 48.960 | mel_loss: 11.777 | 3.445 sec / 10 steps |\n",
      "| step: 427400 | gen_loss: 45.040 | mel_loss: 10.972 | 3.311 sec / 10 steps |\n",
      "Validation mel_loss: 13.984990119934082\n",
      "| step: 427410 | gen_loss: 45.696 | mel_loss: 10.278 | 3.715 sec / 10 steps |\n",
      "| step: 427420 | gen_loss: 47.916 | mel_loss: 11.234 | 3.104 sec / 10 steps |\n",
      "| step: 427430 | gen_loss: 50.005 | mel_loss: 11.861 | 3.288 sec / 10 steps |\n",
      "| step: 427440 | gen_loss: 50.867 | mel_loss: 11.734 | 3.348 sec / 10 steps |\n",
      "| step: 427450 | gen_loss: 45.740 | mel_loss: 10.531 | 3.217 sec / 10 steps |\n",
      "| step: 427460 | gen_loss: 53.512 | mel_loss: 13.050 | 3.142 sec / 10 steps |\n",
      "| step: 427470 | gen_loss: 43.783 | mel_loss: 10.740 | 3.444 sec / 10 steps |\n",
      "| step: 427480 | gen_loss: 39.363 | mel_loss: 10.175 | 3.437 sec / 10 steps |\n",
      "| step: 427490 | gen_loss: 54.468 | mel_loss: 13.818 | 3.120 sec / 10 steps |\n",
      "| step: 427500 | gen_loss: 47.318 | mel_loss: 11.026 | 3.072 sec / 10 steps |\n",
      "Validation mel_loss: 14.234624862670898\n",
      "| step: 427510 | gen_loss: 49.862 | mel_loss: 11.716 | 3.283 sec / 10 steps |\n",
      "| step: 427520 | gen_loss: 40.614 | mel_loss: 8.271 | 3.582 sec / 10 steps |\n",
      "| step: 427530 | gen_loss: 46.085 | mel_loss: 10.447 | 3.394 sec / 10 steps |\n",
      "| step: 427540 | gen_loss: 44.470 | mel_loss: 11.324 | 2.935 sec / 10 steps |\n",
      "| step: 427550 | gen_loss: 52.312 | mel_loss: 12.854 | 3.101 sec / 10 steps |\n",
      "| step: 427560 | gen_loss: 48.814 | mel_loss: 11.098 | 3.384 sec / 10 steps |\n",
      "| step: 427570 | gen_loss: 50.285 | mel_loss: 11.767 | 3.724 sec / 10 steps |\n",
      "| step: 427580 | gen_loss: 46.985 | mel_loss: 10.039 | 3.149 sec / 10 steps |\n",
      "| step: 427590 | gen_loss: 50.425 | mel_loss: 12.335 | 3.313 sec / 10 steps |\n",
      "| step: 427600 | gen_loss: 46.329 | mel_loss: 10.352 | 2.996 sec / 10 steps |\n",
      "Validation mel_loss: 14.045793533325195\n",
      "| step: 427610 | gen_loss: 47.076 | mel_loss: 11.121 | 3.449 sec / 10 steps |\n",
      "| step: 427620 | gen_loss: 44.207 | mel_loss: 9.697 | 3.356 sec / 10 steps |\n",
      "| step: 427630 | gen_loss: 47.768 | mel_loss: 10.884 | 2.926 sec / 10 steps |\n",
      "| step: 427640 | gen_loss: 43.711 | mel_loss: 9.836 | 3.195 sec / 10 steps |\n",
      "| step: 427650 | gen_loss: 38.940 | mel_loss: 9.863 | 3.417 sec / 10 steps |\n",
      "| step: 427660 | gen_loss: 51.370 | mel_loss: 11.805 | 3.073 sec / 10 steps |\n",
      "| step: 427670 | gen_loss: 48.117 | mel_loss: 11.089 | 3.090 sec / 10 steps |\n",
      "| step: 427680 | gen_loss: 42.771 | mel_loss: 8.746 | 3.251 sec / 10 steps |\n",
      "| step: 427690 | gen_loss: 42.656 | mel_loss: 9.312 | 3.435 sec / 10 steps |\n",
      "| step: 427700 | gen_loss: 50.716 | mel_loss: 11.484 | 3.373 sec / 10 steps |\n",
      "Validation mel_loss: 13.847797393798828\n",
      "| step: 427710 | gen_loss: 47.618 | mel_loss: 10.356 | 3.774 sec / 10 steps |\n",
      "| step: 427720 | gen_loss: 46.326 | mel_loss: 10.777 | 3.481 sec / 10 steps |\n",
      "| step: 427730 | gen_loss: 44.078 | mel_loss: 10.918 | 3.549 sec / 10 steps |\n",
      "| step: 427740 | gen_loss: 42.414 | mel_loss: 9.351 | 3.206 sec / 10 steps |\n",
      "| step: 427750 | gen_loss: 48.194 | mel_loss: 11.228 | 3.205 sec / 10 steps |\n",
      "| step: 427760 | gen_loss: 46.716 | mel_loss: 10.694 | 3.258 sec / 10 steps |\n",
      "| step: 427770 | gen_loss: 43.571 | mel_loss: 10.044 | 3.002 sec / 10 steps |\n",
      "| step: 427780 | gen_loss: 40.390 | mel_loss: 9.733 | 3.417 sec / 10 steps |\n",
      "| step: 427790 | gen_loss: 49.727 | mel_loss: 12.581 | 2.962 sec / 10 steps |\n",
      "| step: 427800 | gen_loss: 49.062 | mel_loss: 11.641 | 3.281 sec / 10 steps |\n",
      "Validation mel_loss: 13.846762657165527\n",
      "| step: 427810 | gen_loss: 44.116 | mel_loss: 9.274 | 3.157 sec / 10 steps |\n",
      "| step: 427820 | gen_loss: 48.044 | mel_loss: 11.124 | 3.424 sec / 10 steps |\n",
      "| step: 427830 | gen_loss: 48.999 | mel_loss: 12.075 | 3.186 sec / 10 steps |\n",
      "| step: 427840 | gen_loss: 43.241 | mel_loss: 10.131 | 3.131 sec / 10 steps |\n",
      "| step: 427850 | gen_loss: 50.633 | mel_loss: 12.579 | 3.350 sec / 10 steps |\n",
      "| step: 427860 | gen_loss: 50.946 | mel_loss: 12.899 | 3.372 sec / 10 steps |\n",
      "| step: 427870 | gen_loss: 40.931 | mel_loss: 8.786 | 3.434 sec / 10 steps |\n",
      "| step: 427880 | gen_loss: 45.198 | mel_loss: 10.484 | 3.158 sec / 10 steps |\n",
      "| step: 427890 | gen_loss: 51.566 | mel_loss: 13.109 | 3.100 sec / 10 steps |\n",
      "| step: 427900 | gen_loss: 50.648 | mel_loss: 12.062 | 3.264 sec / 10 steps |\n",
      "Validation mel_loss: 13.988740921020508\n",
      "| step: 427910 | gen_loss: 52.045 | mel_loss: 12.688 | 3.235 sec / 10 steps |\n",
      "| step: 427920 | gen_loss: 50.365 | mel_loss: 12.189 | 3.139 sec / 10 steps |\n",
      "| step: 427930 | gen_loss: 48.505 | mel_loss: 11.768 | 3.734 sec / 10 steps |\n",
      "| step: 427940 | gen_loss: 44.423 | mel_loss: 10.238 | 3.862 sec / 10 steps |\n",
      "| step: 427950 | gen_loss: 46.108 | mel_loss: 10.692 | 3.811 sec / 10 steps |\n",
      "| step: 427960 | gen_loss: 46.579 | mel_loss: 10.249 | 3.317 sec / 10 steps |\n",
      "| step: 427970 | gen_loss: 48.962 | mel_loss: 11.788 | 3.089 sec / 10 steps |\n",
      "| step: 427980 | gen_loss: 41.578 | mel_loss: 9.254 | 3.266 sec / 10 steps |\n",
      "| step: 427990 | gen_loss: 47.784 | mel_loss: 11.219 | 3.299 sec / 10 steps |\n",
      "| step: 428000 | gen_loss: 45.572 | mel_loss: 10.340 | 3.266 sec / 10 steps |\n",
      "Validation mel_loss: 13.939640998840332\n",
      "|| Epoch: 570 ||\n",
      "| step: 428010 | gen_loss: 44.191 | mel_loss: 9.809 | 3.575 sec / 10 steps |\n",
      "| step: 428020 | gen_loss: 47.927 | mel_loss: 11.056 | 3.269 sec / 10 steps |\n",
      "| step: 428030 | gen_loss: 48.634 | mel_loss: 11.521 | 3.684 sec / 10 steps |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 428040 | gen_loss: 52.025 | mel_loss: 12.069 | 3.674 sec / 10 steps |\n",
      "| step: 428050 | gen_loss: 48.487 | mel_loss: 11.165 | 3.160 sec / 10 steps |\n",
      "| step: 428060 | gen_loss: 41.294 | mel_loss: 8.455 | 3.194 sec / 10 steps |\n",
      "| step: 428070 | gen_loss: 49.774 | mel_loss: 11.602 | 3.120 sec / 10 steps |\n",
      "| step: 428080 | gen_loss: 46.225 | mel_loss: 10.713 | 3.081 sec / 10 steps |\n",
      "| step: 428090 | gen_loss: 48.839 | mel_loss: 11.422 | 3.962 sec / 10 steps |\n",
      "| step: 428100 | gen_loss: 49.639 | mel_loss: 10.872 | 3.918 sec / 10 steps |\n",
      "Validation mel_loss: 13.956392288208008\n",
      "| step: 428110 | gen_loss: 47.492 | mel_loss: 11.459 | 3.965 sec / 10 steps |\n",
      "| step: 428120 | gen_loss: 41.915 | mel_loss: 8.879 | 3.812 sec / 10 steps |\n",
      "| step: 428130 | gen_loss: 50.719 | mel_loss: 12.638 | 3.611 sec / 10 steps |\n",
      "| step: 428140 | gen_loss: 41.578 | mel_loss: 9.192 | 4.246 sec / 10 steps |\n",
      "| step: 428150 | gen_loss: 46.902 | mel_loss: 10.938 | 3.770 sec / 10 steps |\n",
      "| step: 428160 | gen_loss: 46.955 | mel_loss: 11.266 | 3.436 sec / 10 steps |\n",
      "| step: 428170 | gen_loss: 47.369 | mel_loss: 11.338 | 3.645 sec / 10 steps |\n",
      "| step: 428180 | gen_loss: 43.227 | mel_loss: 10.469 | 3.142 sec / 10 steps |\n",
      "| step: 428190 | gen_loss: 44.766 | mel_loss: 10.371 | 3.217 sec / 10 steps |\n",
      "| step: 428200 | gen_loss: 44.784 | mel_loss: 11.349 | 3.358 sec / 10 steps |\n",
      "Validation mel_loss: 13.942838668823242\n",
      "| step: 428210 | gen_loss: 43.220 | mel_loss: 9.326 | 3.531 sec / 10 steps |\n",
      "| step: 428220 | gen_loss: 48.983 | mel_loss: 11.478 | 2.866 sec / 10 steps |\n",
      "| step: 428230 | gen_loss: 49.636 | mel_loss: 12.020 | 3.580 sec / 10 steps |\n",
      "| step: 428240 | gen_loss: 37.323 | mel_loss: 9.481 | 3.334 sec / 10 steps |\n",
      "| step: 428250 | gen_loss: 47.445 | mel_loss: 11.601 | 2.879 sec / 10 steps |\n",
      "| step: 428260 | gen_loss: 44.915 | mel_loss: 10.810 | 3.149 sec / 10 steps |\n",
      "| step: 428270 | gen_loss: 43.361 | mel_loss: 10.381 | 3.043 sec / 10 steps |\n",
      "| step: 428280 | gen_loss: 48.632 | mel_loss: 11.334 | 3.335 sec / 10 steps |\n",
      "| step: 428290 | gen_loss: 45.428 | mel_loss: 10.838 | 3.322 sec / 10 steps |\n",
      "| step: 428300 | gen_loss: 48.744 | mel_loss: 11.293 | 3.355 sec / 10 steps |\n",
      "Validation mel_loss: 13.882585525512695\n",
      "| step: 428310 | gen_loss: 42.495 | mel_loss: 9.315 | 3.262 sec / 10 steps |\n",
      "| step: 428320 | gen_loss: 48.729 | mel_loss: 10.907 | 3.418 sec / 10 steps |\n",
      "| step: 428330 | gen_loss: 45.777 | mel_loss: 9.897 | 3.223 sec / 10 steps |\n",
      "| step: 428340 | gen_loss: 46.268 | mel_loss: 10.831 | 3.130 sec / 10 steps |\n",
      "| step: 428350 | gen_loss: 42.740 | mel_loss: 9.935 | 3.199 sec / 10 steps |\n",
      "| step: 428360 | gen_loss: 51.808 | mel_loss: 12.601 | 2.905 sec / 10 steps |\n",
      "| step: 428370 | gen_loss: 48.030 | mel_loss: 10.972 | 3.477 sec / 10 steps |\n",
      "| step: 428380 | gen_loss: 50.132 | mel_loss: 12.298 | 3.178 sec / 10 steps |\n",
      "| step: 428390 | gen_loss: 45.710 | mel_loss: 11.218 | 3.459 sec / 10 steps |\n",
      "| step: 428400 | gen_loss: 46.260 | mel_loss: 10.548 | 3.270 sec / 10 steps |\n",
      "Validation mel_loss: 13.953165054321289\n",
      "| step: 428410 | gen_loss: 51.782 | mel_loss: 12.787 | 3.383 sec / 10 steps |\n",
      "| step: 428420 | gen_loss: 45.229 | mel_loss: 10.457 | 2.956 sec / 10 steps |\n",
      "| step: 428430 | gen_loss: 49.618 | mel_loss: 11.302 | 3.044 sec / 10 steps |\n",
      "| step: 428440 | gen_loss: 47.433 | mel_loss: 12.125 | 3.261 sec / 10 steps |\n",
      "| step: 428450 | gen_loss: 46.846 | mel_loss: 11.344 | 3.420 sec / 10 steps |\n",
      "| step: 428460 | gen_loss: 48.824 | mel_loss: 11.730 | 2.918 sec / 10 steps |\n",
      "| step: 428470 | gen_loss: 49.700 | mel_loss: 11.782 | 3.122 sec / 10 steps |\n",
      "| step: 428480 | gen_loss: 43.303 | mel_loss: 9.467 | 2.996 sec / 10 steps |\n",
      "| step: 428490 | gen_loss: 50.539 | mel_loss: 12.414 | 3.126 sec / 10 steps |\n",
      "| step: 428500 | gen_loss: 49.332 | mel_loss: 12.028 | 3.036 sec / 10 steps |\n",
      "Validation mel_loss: 13.961030960083008\n",
      "| step: 428510 | gen_loss: 49.107 | mel_loss: 11.325 | 3.668 sec / 10 steps |\n",
      "| step: 428520 | gen_loss: 45.702 | mel_loss: 10.615 | 2.834 sec / 10 steps |\n",
      "| step: 428530 | gen_loss: 44.703 | mel_loss: 10.394 | 3.366 sec / 10 steps |\n",
      "| step: 428540 | gen_loss: 40.965 | mel_loss: 8.711 | 3.422 sec / 10 steps |\n",
      "| step: 428550 | gen_loss: 46.060 | mel_loss: 10.783 | 3.226 sec / 10 steps |\n",
      "| step: 428560 | gen_loss: 51.386 | mel_loss: 11.798 | 3.290 sec / 10 steps |\n",
      "| step: 428570 | gen_loss: 43.866 | mel_loss: 9.723 | 2.913 sec / 10 steps |\n",
      "| step: 428580 | gen_loss: 41.004 | mel_loss: 9.112 | 3.309 sec / 10 steps |\n",
      "| step: 428590 | gen_loss: 51.454 | mel_loss: 12.352 | 2.935 sec / 10 steps |\n",
      "| step: 428600 | gen_loss: 44.678 | mel_loss: 10.312 | 3.271 sec / 10 steps |\n",
      "Validation mel_loss: 13.835272789001465\n",
      "| step: 428610 | gen_loss: 44.444 | mel_loss: 10.191 | 3.319 sec / 10 steps |\n",
      "| step: 428620 | gen_loss: 49.447 | mel_loss: 11.813 | 3.205 sec / 10 steps |\n",
      "| step: 428630 | gen_loss: 46.925 | mel_loss: 11.309 | 3.524 sec / 10 steps |\n",
      "| step: 428640 | gen_loss: 48.408 | mel_loss: 11.079 | 3.272 sec / 10 steps |\n",
      "| step: 428650 | gen_loss: 53.857 | mel_loss: 12.290 | 3.597 sec / 10 steps |\n",
      "| step: 428660 | gen_loss: 46.943 | mel_loss: 11.062 | 3.302 sec / 10 steps |\n",
      "| step: 428670 | gen_loss: 48.196 | mel_loss: 10.748 | 2.974 sec / 10 steps |\n",
      "| step: 428680 | gen_loss: 47.858 | mel_loss: 11.663 | 2.969 sec / 10 steps |\n",
      "| step: 428690 | gen_loss: 50.339 | mel_loss: 12.146 | 3.589 sec / 10 steps |\n",
      "| step: 428700 | gen_loss: 47.896 | mel_loss: 11.625 | 3.109 sec / 10 steps |\n",
      "Validation mel_loss: 13.91059398651123\n",
      "| step: 428710 | gen_loss: 41.685 | mel_loss: 8.831 | 3.202 sec / 10 steps |\n",
      "| step: 428720 | gen_loss: 44.625 | mel_loss: 10.572 | 3.067 sec / 10 steps |\n",
      "| step: 428730 | gen_loss: 42.303 | mel_loss: 9.932 | 3.074 sec / 10 steps |\n",
      "| step: 428740 | gen_loss: 44.006 | mel_loss: 10.511 | 3.757 sec / 10 steps |\n",
      "| step: 428750 | gen_loss: 45.550 | mel_loss: 11.063 | 3.006 sec / 10 steps |\n",
      "|| Epoch: 571 ||\n",
      "| step: 428760 | gen_loss: 52.471 | mel_loss: 12.373 | 3.038 sec / 10 steps |\n",
      "| step: 428770 | gen_loss: 47.937 | mel_loss: 10.560 | 3.197 sec / 10 steps |\n",
      "| step: 428780 | gen_loss: 42.174 | mel_loss: 11.031 | 3.131 sec / 10 steps |\n",
      "| step: 428790 | gen_loss: 51.893 | mel_loss: 12.724 | 2.935 sec / 10 steps |\n",
      "| step: 428800 | gen_loss: 46.721 | mel_loss: 10.517 | 3.603 sec / 10 steps |\n",
      "Validation mel_loss: 13.805051803588867\n",
      "| step: 428810 | gen_loss: 55.583 | mel_loss: 12.880 | 3.060 sec / 10 steps |\n",
      "| step: 428820 | gen_loss: 46.350 | mel_loss: 10.962 | 3.165 sec / 10 steps |\n",
      "| step: 428830 | gen_loss: 45.511 | mel_loss: 11.199 | 3.038 sec / 10 steps |\n",
      "| step: 428840 | gen_loss: 41.649 | mel_loss: 8.645 | 3.698 sec / 10 steps |\n",
      "| step: 428850 | gen_loss: 47.702 | mel_loss: 10.845 | 3.489 sec / 10 steps |\n",
      "| step: 428860 | gen_loss: 44.099 | mel_loss: 9.747 | 3.245 sec / 10 steps |\n",
      "| step: 428870 | gen_loss: 42.922 | mel_loss: 9.241 | 3.097 sec / 10 steps |\n",
      "| step: 428880 | gen_loss: 47.761 | mel_loss: 10.493 | 3.378 sec / 10 steps |\n",
      "| step: 428890 | gen_loss: 51.206 | mel_loss: 12.395 | 3.308 sec / 10 steps |\n",
      "| step: 428900 | gen_loss: 47.199 | mel_loss: 10.983 | 3.682 sec / 10 steps |\n",
      "Validation mel_loss: 13.957883834838867\n",
      "| step: 428910 | gen_loss: 46.786 | mel_loss: 11.885 | 2.807 sec / 10 steps |\n",
      "| step: 428920 | gen_loss: 47.394 | mel_loss: 11.063 | 3.077 sec / 10 steps |\n",
      "| step: 428930 | gen_loss: 52.111 | mel_loss: 12.337 | 3.202 sec / 10 steps |\n",
      "| step: 428940 | gen_loss: 45.384 | mel_loss: 10.342 | 3.043 sec / 10 steps |\n",
      "| step: 428950 | gen_loss: 37.925 | mel_loss: 8.888 | 3.307 sec / 10 steps |\n",
      "| step: 428960 | gen_loss: 48.051 | mel_loss: 11.828 | 3.167 sec / 10 steps |\n",
      "| step: 428970 | gen_loss: 44.616 | mel_loss: 10.482 | 3.411 sec / 10 steps |\n",
      "| step: 428980 | gen_loss: 51.111 | mel_loss: 12.254 | 2.934 sec / 10 steps |\n",
      "| step: 428990 | gen_loss: 44.552 | mel_loss: 10.569 | 3.050 sec / 10 steps |\n",
      "| step: 429000 | gen_loss: 48.311 | mel_loss: 11.829 | 3.324 sec / 10 steps |\n",
      "Validation mel_loss: 13.983842849731445\n",
      "| step: 429010 | gen_loss: 40.339 | mel_loss: 9.240 | 3.389 sec / 10 steps |\n",
      "| step: 429020 | gen_loss: 51.097 | mel_loss: 11.669 | 2.874 sec / 10 steps |\n",
      "| step: 429030 | gen_loss: 44.634 | mel_loss: 10.227 | 3.419 sec / 10 steps |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 429040 | gen_loss: 41.326 | mel_loss: 8.080 | 3.449 sec / 10 steps |\n",
      "| step: 429050 | gen_loss: 45.227 | mel_loss: 10.271 | 3.325 sec / 10 steps |\n",
      "| step: 429060 | gen_loss: 46.529 | mel_loss: 11.123 | 3.178 sec / 10 steps |\n",
      "| step: 429070 | gen_loss: 40.375 | mel_loss: 8.694 | 3.301 sec / 10 steps |\n",
      "| step: 429080 | gen_loss: 48.552 | mel_loss: 11.958 | 3.128 sec / 10 steps |\n",
      "| step: 429090 | gen_loss: 48.191 | mel_loss: 10.940 | 3.100 sec / 10 steps |\n",
      "| step: 429100 | gen_loss: 48.460 | mel_loss: 11.700 | 3.666 sec / 10 steps |\n",
      "Validation mel_loss: 13.906181335449219\n",
      "| step: 429110 | gen_loss: 43.101 | mel_loss: 9.405 | 3.399 sec / 10 steps |\n",
      "| step: 429120 | gen_loss: 44.318 | mel_loss: 9.828 | 3.457 sec / 10 steps |\n",
      "| step: 429130 | gen_loss: 42.170 | mel_loss: 8.662 | 3.181 sec / 10 steps |\n",
      "| step: 429140 | gen_loss: 46.607 | mel_loss: 10.320 | 3.357 sec / 10 steps |\n",
      "| step: 429150 | gen_loss: 49.794 | mel_loss: 12.163 | 3.333 sec / 10 steps |\n",
      "| step: 429160 | gen_loss: 45.334 | mel_loss: 10.595 | 3.451 sec / 10 steps |\n",
      "| step: 429170 | gen_loss: 39.901 | mel_loss: 8.311 | 3.306 sec / 10 steps |\n",
      "| step: 429180 | gen_loss: 42.200 | mel_loss: 9.412 | 3.501 sec / 10 steps |\n",
      "| step: 429190 | gen_loss: 41.148 | mel_loss: 10.561 | 3.385 sec / 10 steps |\n",
      "| step: 429200 | gen_loss: 38.858 | mel_loss: 9.639 | 3.088 sec / 10 steps |\n",
      "Validation mel_loss: 13.79973316192627\n",
      "| step: 429210 | gen_loss: 47.681 | mel_loss: 12.257 | 3.145 sec / 10 steps |\n",
      "| step: 429220 | gen_loss: 48.144 | mel_loss: 11.047 | 3.424 sec / 10 steps |\n",
      "| step: 429230 | gen_loss: 43.105 | mel_loss: 9.079 | 3.432 sec / 10 steps |\n",
      "| step: 429240 | gen_loss: 43.739 | mel_loss: 9.370 | 3.226 sec / 10 steps |\n",
      "| step: 429250 | gen_loss: 49.487 | mel_loss: 11.516 | 3.129 sec / 10 steps |\n",
      "| step: 429260 | gen_loss: 49.025 | mel_loss: 11.212 | 3.139 sec / 10 steps |\n",
      "| step: 429270 | gen_loss: 44.708 | mel_loss: 9.932 | 3.296 sec / 10 steps |\n",
      "| step: 429280 | gen_loss: 39.324 | mel_loss: 9.220 | 3.406 sec / 10 steps |\n",
      "| step: 429290 | gen_loss: 45.726 | mel_loss: 11.259 | 3.179 sec / 10 steps |\n",
      "| step: 429300 | gen_loss: 46.934 | mel_loss: 10.904 | 3.251 sec / 10 steps |\n",
      "Validation mel_loss: 13.984475135803223\n",
      "| step: 429310 | gen_loss: 47.498 | mel_loss: 11.561 | 3.133 sec / 10 steps |\n",
      "| step: 429320 | gen_loss: 42.447 | mel_loss: 9.198 | 3.511 sec / 10 steps |\n",
      "| step: 429330 | gen_loss: 52.153 | mel_loss: 13.036 | 3.317 sec / 10 steps |\n",
      "| step: 429340 | gen_loss: 52.284 | mel_loss: 11.750 | 3.190 sec / 10 steps |\n",
      "| step: 429350 | gen_loss: 47.413 | mel_loss: 10.853 | 3.332 sec / 10 steps |\n",
      "| step: 429360 | gen_loss: 50.439 | mel_loss: 11.734 | 3.263 sec / 10 steps |\n",
      "| step: 429370 | gen_loss: 42.855 | mel_loss: 8.903 | 3.400 sec / 10 steps |\n",
      "| step: 429380 | gen_loss: 49.090 | mel_loss: 12.098 | 3.291 sec / 10 steps |\n",
      "| step: 429390 | gen_loss: 50.260 | mel_loss: 11.891 | 3.272 sec / 10 steps |\n",
      "| step: 429400 | gen_loss: 44.039 | mel_loss: 9.383 | 3.296 sec / 10 steps |\n",
      "Validation mel_loss: 13.93887710571289\n",
      "| step: 429410 | gen_loss: 49.372 | mel_loss: 11.421 | 3.197 sec / 10 steps |\n",
      "| step: 429420 | gen_loss: 49.388 | mel_loss: 12.094 | 2.897 sec / 10 steps |\n",
      "| step: 429430 | gen_loss: 44.324 | mel_loss: 10.513 | 3.024 sec / 10 steps |\n",
      "| step: 429440 | gen_loss: 51.248 | mel_loss: 12.187 | 3.268 sec / 10 steps |\n",
      "| step: 429450 | gen_loss: 50.559 | mel_loss: 11.772 | 3.329 sec / 10 steps |\n",
      "| step: 429460 | gen_loss: 47.144 | mel_loss: 10.645 | 3.271 sec / 10 steps |\n",
      "| step: 429470 | gen_loss: 47.857 | mel_loss: 11.191 | 2.812 sec / 10 steps |\n",
      "| step: 429480 | gen_loss: 43.262 | mel_loss: 10.789 | 3.347 sec / 10 steps |\n",
      "| step: 429490 | gen_loss: 41.146 | mel_loss: 9.202 | 3.149 sec / 10 steps |\n",
      "| step: 429500 | gen_loss: 40.328 | mel_loss: 9.563 | 3.410 sec / 10 steps |\n",
      "Validation mel_loss: 13.866046905517578\n",
      "|| Epoch: 572 ||\n",
      "| step: 429510 | gen_loss: 43.387 | mel_loss: 9.604 | 3.397 sec / 10 steps |\n",
      "| step: 429520 | gen_loss: 52.856 | mel_loss: 12.442 | 3.194 sec / 10 steps |\n",
      "| step: 429530 | gen_loss: 48.198 | mel_loss: 11.595 | 3.370 sec / 10 steps |\n",
      "| step: 429540 | gen_loss: 42.740 | mel_loss: 9.833 | 3.586 sec / 10 steps |\n",
      "| step: 429550 | gen_loss: 48.527 | mel_loss: 11.741 | 3.417 sec / 10 steps |\n",
      "| step: 429560 | gen_loss: 50.413 | mel_loss: 12.706 | 2.890 sec / 10 steps |\n",
      "| step: 429570 | gen_loss: 48.395 | mel_loss: 11.149 | 3.068 sec / 10 steps |\n",
      "| step: 429580 | gen_loss: 44.205 | mel_loss: 9.818 | 3.332 sec / 10 steps |\n",
      "| step: 429590 | gen_loss: 42.387 | mel_loss: 9.234 | 3.318 sec / 10 steps |\n",
      "| step: 429600 | gen_loss: 48.250 | mel_loss: 11.241 | 3.332 sec / 10 steps |\n",
      "Validation mel_loss: 13.997102737426758\n",
      "| step: 429610 | gen_loss: 53.249 | mel_loss: 12.296 | 3.373 sec / 10 steps |\n",
      "| step: 429620 | gen_loss: 45.811 | mel_loss: 10.379 | 3.277 sec / 10 steps |\n",
      "| step: 429630 | gen_loss: 47.073 | mel_loss: 10.725 | 3.338 sec / 10 steps |\n",
      "| step: 429640 | gen_loss: 52.423 | mel_loss: 13.074 | 2.872 sec / 10 steps |\n",
      "| step: 429650 | gen_loss: 47.690 | mel_loss: 11.743 | 3.059 sec / 10 steps |\n",
      "| step: 429660 | gen_loss: 45.667 | mel_loss: 10.450 | 3.375 sec / 10 steps |\n",
      "| step: 429670 | gen_loss: 42.703 | mel_loss: 9.183 | 3.262 sec / 10 steps |\n",
      "| step: 429680 | gen_loss: 49.181 | mel_loss: 11.116 | 3.162 sec / 10 steps |\n",
      "| step: 429690 | gen_loss: 53.491 | mel_loss: 12.847 | 3.410 sec / 10 steps |\n",
      "| step: 429700 | gen_loss: 42.780 | mel_loss: 9.001 | 3.682 sec / 10 steps |\n",
      "Validation mel_loss: 13.988677978515625\n",
      "| step: 429710 | gen_loss: 50.031 | mel_loss: 12.120 | 3.234 sec / 10 steps |\n",
      "| step: 429720 | gen_loss: 45.978 | mel_loss: 11.072 | 3.585 sec / 10 steps |\n",
      "| step: 429730 | gen_loss: 53.431 | mel_loss: 13.122 | 3.343 sec / 10 steps |\n",
      "| step: 429740 | gen_loss: 49.481 | mel_loss: 12.568 | 3.053 sec / 10 steps |\n",
      "| step: 429750 | gen_loss: 48.162 | mel_loss: 10.825 | 3.542 sec / 10 steps |\n",
      "| step: 429760 | gen_loss: 48.471 | mel_loss: 11.150 | 3.351 sec / 10 steps |\n",
      "| step: 429770 | gen_loss: 52.885 | mel_loss: 12.321 | 2.956 sec / 10 steps |\n",
      "| step: 429780 | gen_loss: 48.460 | mel_loss: 11.649 | 3.501 sec / 10 steps |\n",
      "| step: 429790 | gen_loss: 42.775 | mel_loss: 9.052 | 3.421 sec / 10 steps |\n",
      "| step: 429800 | gen_loss: 47.666 | mel_loss: 11.610 | 3.192 sec / 10 steps |\n",
      "Validation mel_loss: 13.839052200317383\n",
      "| step: 429810 | gen_loss: 44.351 | mel_loss: 9.820 | 3.157 sec / 10 steps |\n",
      "| step: 429820 | gen_loss: 51.607 | mel_loss: 13.011 | 3.133 sec / 10 steps |\n",
      "| step: 429830 | gen_loss: 49.243 | mel_loss: 11.828 | 3.163 sec / 10 steps |\n",
      "| step: 429840 | gen_loss: 49.130 | mel_loss: 11.391 | 3.329 sec / 10 steps |\n",
      "| step: 429850 | gen_loss: 42.734 | mel_loss: 9.511 | 3.238 sec / 10 steps |\n",
      "| step: 429860 | gen_loss: 44.013 | mel_loss: 9.179 | 3.282 sec / 10 steps |\n",
      "| step: 429870 | gen_loss: 44.777 | mel_loss: 10.340 | 3.612 sec / 10 steps |\n",
      "| step: 429880 | gen_loss: 47.040 | mel_loss: 10.872 | 3.373 sec / 10 steps |\n",
      "| step: 429890 | gen_loss: 41.949 | mel_loss: 9.351 | 3.448 sec / 10 steps |\n",
      "| step: 429900 | gen_loss: 47.963 | mel_loss: 11.136 | 3.138 sec / 10 steps |\n",
      "Validation mel_loss: 13.974815368652344\n",
      "| step: 429910 | gen_loss: 47.150 | mel_loss: 11.434 | 3.353 sec / 10 steps |\n",
      "| step: 429920 | gen_loss: 44.936 | mel_loss: 10.039 | 3.572 sec / 10 steps |\n",
      "| step: 429930 | gen_loss: 47.953 | mel_loss: 10.941 | 3.485 sec / 10 steps |\n",
      "| step: 429940 | gen_loss: 48.612 | mel_loss: 11.573 | 3.336 sec / 10 steps |\n",
      "| step: 429950 | gen_loss: 47.929 | mel_loss: 10.770 | 3.226 sec / 10 steps |\n",
      "| step: 429960 | gen_loss: 48.010 | mel_loss: 10.854 | 3.217 sec / 10 steps |\n",
      "| step: 429970 | gen_loss: 51.205 | mel_loss: 12.032 | 3.478 sec / 10 steps |\n",
      "| step: 429980 | gen_loss: 46.143 | mel_loss: 11.257 | 3.412 sec / 10 steps |\n",
      "| step: 429990 | gen_loss: 50.555 | mel_loss: 11.911 | 3.300 sec / 10 steps |\n",
      "| step: 430000 | gen_loss: 39.476 | mel_loss: 8.074 | 3.539 sec / 10 steps |\n",
      "Validation mel_loss: 13.92202091217041\n",
      "| step: 430010 | gen_loss: 52.559 | mel_loss: 12.720 | 3.379 sec / 10 steps |\n",
      "| step: 430020 | gen_loss: 49.732 | mel_loss: 12.525 | 3.092 sec / 10 steps |\n",
      "| step: 430030 | gen_loss: 47.535 | mel_loss: 11.195 | 3.346 sec / 10 steps |\n",
      "| step: 430040 | gen_loss: 45.495 | mel_loss: 9.793 | 3.478 sec / 10 steps |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 430050 | gen_loss: 43.311 | mel_loss: 9.623 | 3.341 sec / 10 steps |\n",
      "| step: 430060 | gen_loss: 53.442 | mel_loss: 12.425 | 3.297 sec / 10 steps |\n",
      "| step: 430070 | gen_loss: 46.941 | mel_loss: 11.172 | 3.333 sec / 10 steps |\n",
      "| step: 430080 | gen_loss: 46.730 | mel_loss: 11.118 | 3.112 sec / 10 steps |\n",
      "| step: 430090 | gen_loss: 49.403 | mel_loss: 12.796 | 3.085 sec / 10 steps |\n",
      "| step: 430100 | gen_loss: 49.710 | mel_loss: 12.097 | 3.165 sec / 10 steps |\n",
      "Validation mel_loss: 14.009896278381348\n",
      "| step: 430110 | gen_loss: 47.724 | mel_loss: 11.406 | 3.112 sec / 10 steps |\n",
      "| step: 430120 | gen_loss: 49.997 | mel_loss: 12.663 | 3.195 sec / 10 steps |\n",
      "| step: 430130 | gen_loss: 50.006 | mel_loss: 12.494 | 3.256 sec / 10 steps |\n",
      "| step: 430140 | gen_loss: 44.895 | mel_loss: 9.252 | 3.395 sec / 10 steps |\n",
      "| step: 430150 | gen_loss: 50.054 | mel_loss: 12.192 | 2.876 sec / 10 steps |\n",
      "| step: 430160 | gen_loss: 49.538 | mel_loss: 11.883 | 3.280 sec / 10 steps |\n",
      "| step: 430170 | gen_loss: 45.433 | mel_loss: 10.824 | 3.254 sec / 10 steps |\n",
      "| step: 430180 | gen_loss: 47.981 | mel_loss: 11.983 | 3.220 sec / 10 steps |\n",
      "| step: 430190 | gen_loss: 51.696 | mel_loss: 13.009 | 3.122 sec / 10 steps |\n",
      "| step: 430200 | gen_loss: 41.628 | mel_loss: 9.731 | 3.537 sec / 10 steps |\n",
      "Validation mel_loss: 13.899641036987305\n",
      "| step: 430210 | gen_loss: 53.036 | mel_loss: 13.055 | 3.228 sec / 10 steps |\n",
      "| step: 430220 | gen_loss: 43.015 | mel_loss: 10.621 | 3.337 sec / 10 steps |\n",
      "| step: 430230 | gen_loss: 49.054 | mel_loss: 11.719 | 3.189 sec / 10 steps |\n",
      "| step: 430240 | gen_loss: 49.341 | mel_loss: 11.814 | 3.405 sec / 10 steps |\n",
      "| step: 430250 | gen_loss: 48.890 | mel_loss: 11.012 | 3.247 sec / 10 steps |\n",
      "|| Epoch: 573 ||\n",
      "| step: 430260 | gen_loss: 47.310 | mel_loss: 11.149 | 2.847 sec / 10 steps |\n",
      "| step: 430270 | gen_loss: 49.981 | mel_loss: 13.041 | 3.229 sec / 10 steps |\n",
      "| step: 430280 | gen_loss: 43.587 | mel_loss: 9.248 | 3.974 sec / 10 steps |\n",
      "| step: 430290 | gen_loss: 43.363 | mel_loss: 9.463 | 3.345 sec / 10 steps |\n",
      "| step: 430300 | gen_loss: 47.309 | mel_loss: 10.961 | 3.399 sec / 10 steps |\n",
      "Validation mel_loss: 13.875697135925293\n",
      "| step: 430310 | gen_loss: 52.877 | mel_loss: 13.206 | 3.464 sec / 10 steps |\n",
      "| step: 430320 | gen_loss: 42.458 | mel_loss: 9.729 | 3.571 sec / 10 steps |\n",
      "| step: 430330 | gen_loss: 45.666 | mel_loss: 10.639 | 3.286 sec / 10 steps |\n",
      "| step: 430340 | gen_loss: 44.234 | mel_loss: 9.938 | 3.132 sec / 10 steps |\n",
      "| step: 430350 | gen_loss: 49.944 | mel_loss: 12.358 | 3.292 sec / 10 steps |\n",
      "| step: 430360 | gen_loss: 47.630 | mel_loss: 11.076 | 3.408 sec / 10 steps |\n",
      "| step: 430370 | gen_loss: 42.016 | mel_loss: 8.517 | 3.233 sec / 10 steps |\n",
      "| step: 430380 | gen_loss: 48.786 | mel_loss: 10.879 | 2.894 sec / 10 steps |\n",
      "| step: 430390 | gen_loss: 44.649 | mel_loss: 10.446 | 3.300 sec / 10 steps |\n",
      "| step: 430400 | gen_loss: 46.436 | mel_loss: 10.907 | 3.065 sec / 10 steps |\n",
      "Validation mel_loss: 14.18387508392334\n",
      "| step: 430410 | gen_loss: 44.962 | mel_loss: 10.578 | 3.450 sec / 10 steps |\n",
      "| step: 430420 | gen_loss: 51.376 | mel_loss: 12.230 | 3.175 sec / 10 steps |\n",
      "| step: 430430 | gen_loss: 48.283 | mel_loss: 11.772 | 3.075 sec / 10 steps |\n",
      "| step: 430440 | gen_loss: 46.846 | mel_loss: 11.434 | 3.229 sec / 10 steps |\n",
      "| step: 430450 | gen_loss: 45.662 | mel_loss: 10.812 | 3.351 sec / 10 steps |\n",
      "| step: 430460 | gen_loss: 45.090 | mel_loss: 10.461 | 3.675 sec / 10 steps |\n",
      "| step: 430470 | gen_loss: 51.136 | mel_loss: 12.028 | 3.229 sec / 10 steps |\n",
      "| step: 430480 | gen_loss: 44.788 | mel_loss: 10.020 | 3.160 sec / 10 steps |\n",
      "| step: 430490 | gen_loss: 49.120 | mel_loss: 11.581 | 3.265 sec / 10 steps |\n",
      "| step: 430500 | gen_loss: 48.342 | mel_loss: 11.726 | 3.095 sec / 10 steps |\n",
      "Validation mel_loss: 14.013630867004395\n",
      "| step: 430510 | gen_loss: 51.714 | mel_loss: 12.809 | 3.565 sec / 10 steps |\n",
      "| step: 430520 | gen_loss: 47.091 | mel_loss: 11.099 | 3.609 sec / 10 steps |\n",
      "| step: 430530 | gen_loss: 50.629 | mel_loss: 11.776 | 3.409 sec / 10 steps |\n",
      "| step: 430540 | gen_loss: 43.364 | mel_loss: 9.724 | 3.506 sec / 10 steps |\n",
      "| step: 430550 | gen_loss: 49.440 | mel_loss: 11.934 | 3.469 sec / 10 steps |\n",
      "| step: 430560 | gen_loss: 48.147 | mel_loss: 11.466 | 3.276 sec / 10 steps |\n",
      "| step: 430570 | gen_loss: 42.538 | mel_loss: 9.315 | 3.837 sec / 10 steps |\n",
      "| step: 430580 | gen_loss: 42.546 | mel_loss: 9.070 | 3.542 sec / 10 steps |\n",
      "| step: 430590 | gen_loss: 49.243 | mel_loss: 12.358 | 3.166 sec / 10 steps |\n",
      "| step: 430600 | gen_loss: 46.243 | mel_loss: 11.072 | 3.008 sec / 10 steps |\n",
      "Validation mel_loss: 14.084535598754883\n",
      "| step: 430610 | gen_loss: 43.964 | mel_loss: 9.579 | 3.353 sec / 10 steps |\n",
      "| step: 430620 | gen_loss: 48.452 | mel_loss: 11.603 | 3.446 sec / 10 steps |\n",
      "| step: 430630 | gen_loss: 47.472 | mel_loss: 10.711 | 3.223 sec / 10 steps |\n",
      "| step: 430640 | gen_loss: 42.185 | mel_loss: 9.170 | 3.193 sec / 10 steps |\n",
      "| step: 430650 | gen_loss: 46.034 | mel_loss: 11.666 | 2.875 sec / 10 steps |\n",
      "| step: 430660 | gen_loss: 50.780 | mel_loss: 12.284 | 3.204 sec / 10 steps |\n",
      "| step: 430670 | gen_loss: 44.255 | mel_loss: 10.219 | 3.175 sec / 10 steps |\n",
      "| step: 430680 | gen_loss: 43.405 | mel_loss: 9.521 | 3.869 sec / 10 steps |\n",
      "| step: 430690 | gen_loss: 44.630 | mel_loss: 10.575 | 3.389 sec / 10 steps |\n",
      "| step: 430700 | gen_loss: 51.063 | mel_loss: 12.453 | 3.239 sec / 10 steps |\n",
      "Validation mel_loss: 13.898935317993164\n",
      "| step: 430710 | gen_loss: 49.598 | mel_loss: 12.155 | 3.304 sec / 10 steps |\n",
      "| step: 430720 | gen_loss: 48.954 | mel_loss: 11.641 | 3.412 sec / 10 steps |\n",
      "| step: 430730 | gen_loss: 48.934 | mel_loss: 11.972 | 3.420 sec / 10 steps |\n",
      "| step: 430740 | gen_loss: 44.251 | mel_loss: 9.369 | 3.385 sec / 10 steps |\n",
      "| step: 430750 | gen_loss: 48.548 | mel_loss: 12.442 | 3.332 sec / 10 steps |\n",
      "| step: 430760 | gen_loss: 43.144 | mel_loss: 9.671 | 3.507 sec / 10 steps |\n",
      "| step: 430770 | gen_loss: 48.776 | mel_loss: 11.025 | 3.084 sec / 10 steps |\n",
      "| step: 430780 | gen_loss: 52.268 | mel_loss: 12.119 | 2.883 sec / 10 steps |\n",
      "| step: 430790 | gen_loss: 44.813 | mel_loss: 11.043 | 3.410 sec / 10 steps |\n",
      "| step: 430800 | gen_loss: 49.690 | mel_loss: 11.673 | 3.670 sec / 10 steps |\n",
      "Validation mel_loss: 13.986845016479492\n",
      "| step: 430810 | gen_loss: 45.482 | mel_loss: 10.948 | 3.226 sec / 10 steps |\n",
      "| step: 430820 | gen_loss: 42.848 | mel_loss: 9.621 | 3.266 sec / 10 steps |\n",
      "| step: 430830 | gen_loss: 46.040 | mel_loss: 10.423 | 3.032 sec / 10 steps |\n",
      "| step: 430840 | gen_loss: 45.931 | mel_loss: 10.777 | 2.943 sec / 10 steps |\n",
      "| step: 430850 | gen_loss: 45.616 | mel_loss: 10.217 | 3.387 sec / 10 steps |\n",
      "| step: 430860 | gen_loss: 43.584 | mel_loss: 9.234 | 3.327 sec / 10 steps |\n",
      "| step: 430870 | gen_loss: 46.650 | mel_loss: 11.285 | 3.259 sec / 10 steps |\n",
      "| step: 430880 | gen_loss: 45.619 | mel_loss: 10.123 | 3.431 sec / 10 steps |\n",
      "| step: 430890 | gen_loss: 47.461 | mel_loss: 10.647 | 3.592 sec / 10 steps |\n",
      "| step: 430900 | gen_loss: 49.133 | mel_loss: 12.712 | 3.440 sec / 10 steps |\n",
      "Validation mel_loss: 13.967782974243164\n",
      "| step: 430910 | gen_loss: 52.412 | mel_loss: 12.237 | 3.000 sec / 10 steps |\n",
      "| step: 430920 | gen_loss: 48.462 | mel_loss: 11.092 | 3.175 sec / 10 steps |\n",
      "| step: 430930 | gen_loss: 47.491 | mel_loss: 11.499 | 2.968 sec / 10 steps |\n",
      "| step: 430940 | gen_loss: 50.228 | mel_loss: 11.647 | 3.161 sec / 10 steps |\n",
      "| step: 430950 | gen_loss: 46.089 | mel_loss: 10.570 | 2.916 sec / 10 steps |\n",
      "| step: 430960 | gen_loss: 45.932 | mel_loss: 10.294 | 3.048 sec / 10 steps |\n",
      "| step: 430970 | gen_loss: 49.247 | mel_loss: 12.084 | 3.097 sec / 10 steps |\n",
      "| step: 430980 | gen_loss: 49.019 | mel_loss: 11.701 | 3.432 sec / 10 steps |\n",
      "| step: 430990 | gen_loss: 46.456 | mel_loss: 10.696 | 3.271 sec / 10 steps |\n",
      "| step: 431000 | gen_loss: 50.309 | mel_loss: 12.053 | 3.028 sec / 10 steps |\n",
      "Validation mel_loss: 14.274179458618164\n",
      "|| Epoch: 574 ||\n",
      "| step: 431010 | gen_loss: 49.885 | mel_loss: 12.364 | 3.005 sec / 10 steps |\n",
      "| step: 431020 | gen_loss: 46.777 | mel_loss: 11.488 | 3.045 sec / 10 steps |\n",
      "| step: 431030 | gen_loss: 37.336 | mel_loss: 8.017 | 3.261 sec / 10 steps |\n",
      "| step: 431040 | gen_loss: 50.289 | mel_loss: 11.866 | 3.018 sec / 10 steps |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 431050 | gen_loss: 45.442 | mel_loss: 10.280 | 3.292 sec / 10 steps |\n",
      "| step: 431060 | gen_loss: 47.582 | mel_loss: 12.009 | 3.111 sec / 10 steps |\n",
      "| step: 431070 | gen_loss: 46.695 | mel_loss: 10.597 | 3.504 sec / 10 steps |\n",
      "| step: 431080 | gen_loss: 44.219 | mel_loss: 10.234 | 2.932 sec / 10 steps |\n",
      "| step: 431090 | gen_loss: 43.171 | mel_loss: 9.662 | 3.094 sec / 10 steps |\n",
      "| step: 431100 | gen_loss: 45.250 | mel_loss: 10.701 | 3.286 sec / 10 steps |\n",
      "Validation mel_loss: 13.950276374816895\n",
      "| step: 431110 | gen_loss: 43.532 | mel_loss: 9.864 | 3.471 sec / 10 steps |\n",
      "| step: 431120 | gen_loss: 46.019 | mel_loss: 10.478 | 3.101 sec / 10 steps |\n",
      "| step: 431130 | gen_loss: 52.369 | mel_loss: 13.068 | 3.206 sec / 10 steps |\n",
      "| step: 431140 | gen_loss: 48.764 | mel_loss: 11.352 | 3.265 sec / 10 steps |\n",
      "| step: 431150 | gen_loss: 51.611 | mel_loss: 12.399 | 3.166 sec / 10 steps |\n",
      "| step: 431160 | gen_loss: 44.323 | mel_loss: 9.294 | 3.358 sec / 10 steps |\n",
      "| step: 431170 | gen_loss: 47.969 | mel_loss: 11.258 | 3.361 sec / 10 steps |\n",
      "| step: 431180 | gen_loss: 44.214 | mel_loss: 10.785 | 2.913 sec / 10 steps |\n",
      "| step: 431190 | gen_loss: 41.192 | mel_loss: 8.508 | 3.335 sec / 10 steps |\n",
      "| step: 431200 | gen_loss: 48.317 | mel_loss: 11.309 | 3.162 sec / 10 steps |\n",
      "Validation mel_loss: 13.805037498474121\n",
      "| step: 431210 | gen_loss: 45.386 | mel_loss: 9.608 | 3.417 sec / 10 steps |\n",
      "| step: 431220 | gen_loss: 50.395 | mel_loss: 11.432 | 2.919 sec / 10 steps |\n",
      "| step: 431230 | gen_loss: 45.417 | mel_loss: 10.862 | 3.679 sec / 10 steps |\n",
      "| step: 431240 | gen_loss: 47.276 | mel_loss: 10.397 | 3.218 sec / 10 steps |\n",
      "| step: 431250 | gen_loss: 46.419 | mel_loss: 11.072 | 3.435 sec / 10 steps |\n",
      "| step: 431260 | gen_loss: 44.360 | mel_loss: 9.784 | 3.085 sec / 10 steps |\n",
      "| step: 431270 | gen_loss: 41.796 | mel_loss: 9.241 | 3.221 sec / 10 steps |\n",
      "| step: 431280 | gen_loss: 44.333 | mel_loss: 10.830 | 3.415 sec / 10 steps |\n",
      "| step: 431290 | gen_loss: 49.400 | mel_loss: 11.783 | 3.289 sec / 10 steps |\n",
      "| step: 431300 | gen_loss: 52.941 | mel_loss: 12.514 | 2.947 sec / 10 steps |\n",
      "Validation mel_loss: 13.982236862182617\n",
      "| step: 431310 | gen_loss: 47.918 | mel_loss: 10.569 | 3.183 sec / 10 steps |\n",
      "| step: 431320 | gen_loss: 45.568 | mel_loss: 10.872 | 3.144 sec / 10 steps |\n",
      "| step: 431330 | gen_loss: 55.029 | mel_loss: 13.284 | 3.254 sec / 10 steps |\n",
      "| step: 431340 | gen_loss: 44.582 | mel_loss: 10.516 | 3.166 sec / 10 steps |\n",
      "| step: 431350 | gen_loss: 42.361 | mel_loss: 9.341 | 3.347 sec / 10 steps |\n",
      "| step: 431360 | gen_loss: 43.797 | mel_loss: 9.753 | 3.378 sec / 10 steps |\n",
      "| step: 431370 | gen_loss: 52.377 | mel_loss: 12.579 | 3.119 sec / 10 steps |\n",
      "| step: 431380 | gen_loss: 45.854 | mel_loss: 10.154 | 3.508 sec / 10 steps |\n",
      "| step: 431390 | gen_loss: 42.995 | mel_loss: 9.687 | 3.155 sec / 10 steps |\n",
      "| step: 431400 | gen_loss: 44.890 | mel_loss: 11.026 | 3.334 sec / 10 steps |\n",
      "Validation mel_loss: 14.03500747680664\n",
      "| step: 431410 | gen_loss: 47.739 | mel_loss: 11.339 | 3.358 sec / 10 steps |\n",
      "| step: 431420 | gen_loss: 49.236 | mel_loss: 11.548 | 3.245 sec / 10 steps |\n",
      "| step: 431430 | gen_loss: 42.652 | mel_loss: 9.273 | 3.658 sec / 10 steps |\n",
      "| step: 431440 | gen_loss: 47.590 | mel_loss: 11.315 | 3.270 sec / 10 steps |\n",
      "| step: 431450 | gen_loss: 45.329 | mel_loss: 10.972 | 3.477 sec / 10 steps |\n",
      "| step: 431460 | gen_loss: 41.182 | mel_loss: 9.014 | 3.047 sec / 10 steps |\n",
      "| step: 431470 | gen_loss: 44.388 | mel_loss: 9.832 | 3.541 sec / 10 steps |\n",
      "| step: 431480 | gen_loss: 52.586 | mel_loss: 12.925 | 2.925 sec / 10 steps |\n",
      "| step: 431490 | gen_loss: 50.682 | mel_loss: 12.353 | 3.521 sec / 10 steps |\n",
      "| step: 431500 | gen_loss: 53.201 | mel_loss: 13.243 | 2.820 sec / 10 steps |\n",
      "Validation mel_loss: 13.934379577636719\n",
      "| step: 431510 | gen_loss: 44.924 | mel_loss: 10.416 | 3.344 sec / 10 steps |\n",
      "| step: 431520 | gen_loss: 45.440 | mel_loss: 10.318 | 3.323 sec / 10 steps |\n",
      "| step: 431530 | gen_loss: 43.209 | mel_loss: 9.291 | 3.075 sec / 10 steps |\n",
      "| step: 431540 | gen_loss: 46.528 | mel_loss: 10.491 | 2.937 sec / 10 steps |\n",
      "| step: 431550 | gen_loss: 41.917 | mel_loss: 8.645 | 3.317 sec / 10 steps |\n",
      "| step: 431560 | gen_loss: 49.682 | mel_loss: 12.177 | 3.186 sec / 10 steps |\n",
      "| step: 431570 | gen_loss: 40.297 | mel_loss: 8.207 | 3.463 sec / 10 steps |\n",
      "| step: 431580 | gen_loss: 44.134 | mel_loss: 10.862 | 3.139 sec / 10 steps |\n",
      "| step: 431590 | gen_loss: 47.013 | mel_loss: 10.490 | 3.331 sec / 10 steps |\n",
      "| step: 431600 | gen_loss: 43.269 | mel_loss: 9.605 | 3.477 sec / 10 steps |\n",
      "Validation mel_loss: 13.802668571472168\n",
      "| step: 431610 | gen_loss: 43.901 | mel_loss: 9.444 | 3.440 sec / 10 steps |\n",
      "| step: 431620 | gen_loss: 52.305 | mel_loss: 12.662 | 2.764 sec / 10 steps |\n",
      "| step: 431630 | gen_loss: 46.301 | mel_loss: 10.454 | 3.322 sec / 10 steps |\n",
      "| step: 431640 | gen_loss: 44.023 | mel_loss: 9.478 | 3.189 sec / 10 steps |\n",
      "| step: 431650 | gen_loss: 49.473 | mel_loss: 12.143 | 3.448 sec / 10 steps |\n",
      "| step: 431660 | gen_loss: 46.385 | mel_loss: 10.444 | 3.176 sec / 10 steps |\n",
      "| step: 431670 | gen_loss: 43.876 | mel_loss: 9.324 | 3.629 sec / 10 steps |\n",
      "| step: 431680 | gen_loss: 40.086 | mel_loss: 8.753 | 3.219 sec / 10 steps |\n",
      "| step: 431690 | gen_loss: 47.566 | mel_loss: 11.313 | 3.801 sec / 10 steps |\n",
      "| step: 431700 | gen_loss: 43.539 | mel_loss: 10.374 | 3.399 sec / 10 steps |\n",
      "Validation mel_loss: 13.839056015014648\n",
      "| step: 431710 | gen_loss: 44.796 | mel_loss: 9.683 | 3.336 sec / 10 steps |\n",
      "| step: 431720 | gen_loss: 47.869 | mel_loss: 10.738 | 3.149 sec / 10 steps |\n",
      "| step: 431730 | gen_loss: 49.142 | mel_loss: 11.195 | 3.167 sec / 10 steps |\n",
      "| step: 431740 | gen_loss: 45.819 | mel_loss: 10.672 | 2.952 sec / 10 steps |\n",
      "| step: 431750 | gen_loss: 45.993 | mel_loss: 10.766 | 2.889 sec / 10 steps |\n",
      "|| Epoch: 575 ||\n",
      "| step: 431760 | gen_loss: 38.717 | mel_loss: 9.624 | 3.023 sec / 10 steps |\n",
      "| step: 431770 | gen_loss: 50.507 | mel_loss: 12.449 | 3.081 sec / 10 steps |\n",
      "| step: 431780 | gen_loss: 44.482 | mel_loss: 10.512 | 3.096 sec / 10 steps |\n",
      "| step: 431790 | gen_loss: 49.424 | mel_loss: 11.903 | 2.879 sec / 10 steps |\n",
      "| step: 431800 | gen_loss: 46.192 | mel_loss: 11.104 | 3.107 sec / 10 steps |\n",
      "Validation mel_loss: 13.950492858886719\n",
      "| step: 431810 | gen_loss: 51.244 | mel_loss: 12.206 | 3.175 sec / 10 steps |\n",
      "| step: 431820 | gen_loss: 49.011 | mel_loss: 11.198 | 3.102 sec / 10 steps |\n",
      "| step: 431830 | gen_loss: 48.912 | mel_loss: 11.786 | 3.300 sec / 10 steps |\n",
      "| step: 431840 | gen_loss: 44.650 | mel_loss: 9.707 | 3.283 sec / 10 steps |\n",
      "| step: 431850 | gen_loss: 48.201 | mel_loss: 11.414 | 3.045 sec / 10 steps |\n",
      "| step: 431860 | gen_loss: 45.838 | mel_loss: 11.092 | 3.068 sec / 10 steps |\n",
      "| step: 431870 | gen_loss: 41.653 | mel_loss: 10.255 | 3.371 sec / 10 steps |\n",
      "| step: 431880 | gen_loss: 49.513 | mel_loss: 10.960 | 3.098 sec / 10 steps |\n",
      "| step: 431890 | gen_loss: 45.850 | mel_loss: 10.400 | 3.273 sec / 10 steps |\n",
      "| step: 431900 | gen_loss: 48.514 | mel_loss: 11.667 | 3.182 sec / 10 steps |\n",
      "Validation mel_loss: 13.868009567260742\n",
      "| step: 431910 | gen_loss: 42.615 | mel_loss: 9.375 | 3.323 sec / 10 steps |\n",
      "| step: 431920 | gen_loss: 39.646 | mel_loss: 9.281 | 3.600 sec / 10 steps |\n",
      "| step: 431930 | gen_loss: 47.248 | mel_loss: 10.475 | 3.183 sec / 10 steps |\n",
      "| step: 431940 | gen_loss: 49.587 | mel_loss: 11.557 | 3.284 sec / 10 steps |\n",
      "| step: 431950 | gen_loss: 48.633 | mel_loss: 11.691 | 3.391 sec / 10 steps |\n",
      "| step: 431960 | gen_loss: 42.971 | mel_loss: 9.689 | 3.226 sec / 10 steps |\n",
      "| step: 431970 | gen_loss: 50.299 | mel_loss: 11.721 | 3.187 sec / 10 steps |\n",
      "| step: 431980 | gen_loss: 47.113 | mel_loss: 11.567 | 3.235 sec / 10 steps |\n",
      "| step: 431990 | gen_loss: 50.593 | mel_loss: 12.290 | 3.348 sec / 10 steps |\n",
      "| step: 432000 | gen_loss: 52.712 | mel_loss: 13.116 | 3.339 sec / 10 steps |\n",
      "Validation mel_loss: 13.85708999633789\n",
      "| step: 432010 | gen_loss: 43.774 | mel_loss: 8.618 | 3.598 sec / 10 steps |\n",
      "| step: 432020 | gen_loss: 48.324 | mel_loss: 11.573 | 2.968 sec / 10 steps |\n",
      "| step: 432030 | gen_loss: 48.568 | mel_loss: 11.226 | 3.493 sec / 10 steps |\n",
      "| step: 432040 | gen_loss: 44.295 | mel_loss: 9.512 | 2.974 sec / 10 steps |\n",
      "| step: 432050 | gen_loss: 41.541 | mel_loss: 9.744 | 3.236 sec / 10 steps |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 432060 | gen_loss: 47.960 | mel_loss: 11.400 | 3.199 sec / 10 steps |\n",
      "| step: 432070 | gen_loss: 43.529 | mel_loss: 9.362 | 3.222 sec / 10 steps |\n",
      "| step: 432080 | gen_loss: 45.261 | mel_loss: 10.673 | 3.154 sec / 10 steps |\n",
      "| step: 432090 | gen_loss: 38.597 | mel_loss: 8.441 | 3.114 sec / 10 steps |\n",
      "| step: 432100 | gen_loss: 48.764 | mel_loss: 11.843 | 3.192 sec / 10 steps |\n",
      "Validation mel_loss: 13.954605102539062\n",
      "| step: 432110 | gen_loss: 52.446 | mel_loss: 12.270 | 3.341 sec / 10 steps |\n",
      "| step: 432120 | gen_loss: 34.712 | mel_loss: 8.035 | 3.134 sec / 10 steps |\n",
      "| step: 432130 | gen_loss: 47.538 | mel_loss: 11.665 | 3.336 sec / 10 steps |\n",
      "| step: 432140 | gen_loss: 44.951 | mel_loss: 10.120 | 2.965 sec / 10 steps |\n",
      "| step: 432150 | gen_loss: 47.753 | mel_loss: 10.603 | 3.114 sec / 10 steps |\n",
      "| step: 432160 | gen_loss: 44.520 | mel_loss: 10.005 | 3.289 sec / 10 steps |\n",
      "| step: 432170 | gen_loss: 45.288 | mel_loss: 9.754 | 3.655 sec / 10 steps |\n",
      "| step: 432180 | gen_loss: 49.011 | mel_loss: 11.390 | 3.217 sec / 10 steps |\n",
      "| step: 432190 | gen_loss: 48.409 | mel_loss: 10.844 | 3.061 sec / 10 steps |\n",
      "| step: 432200 | gen_loss: 47.647 | mel_loss: 10.165 | 3.253 sec / 10 steps |\n",
      "Validation mel_loss: 13.883687019348145\n",
      "| step: 432210 | gen_loss: 41.765 | mel_loss: 9.380 | 3.282 sec / 10 steps |\n",
      "| step: 432220 | gen_loss: 47.063 | mel_loss: 11.463 | 3.392 sec / 10 steps |\n",
      "| step: 432230 | gen_loss: 44.513 | mel_loss: 10.457 | 2.892 sec / 10 steps |\n",
      "| step: 432240 | gen_loss: 48.962 | mel_loss: 12.020 | 3.178 sec / 10 steps |\n",
      "| step: 432250 | gen_loss: 45.662 | mel_loss: 10.786 | 3.176 sec / 10 steps |\n",
      "| step: 432260 | gen_loss: 48.354 | mel_loss: 11.382 | 3.025 sec / 10 steps |\n",
      "| step: 432270 | gen_loss: 44.574 | mel_loss: 10.275 | 3.293 sec / 10 steps |\n",
      "| step: 432280 | gen_loss: 49.221 | mel_loss: 11.180 | 3.228 sec / 10 steps |\n",
      "| step: 432290 | gen_loss: 44.653 | mel_loss: 9.670 | 3.288 sec / 10 steps |\n",
      "| step: 432300 | gen_loss: 41.065 | mel_loss: 9.389 | 3.205 sec / 10 steps |\n",
      "Validation mel_loss: 13.973726272583008\n",
      "| step: 432310 | gen_loss: 49.369 | mel_loss: 11.319 | 3.273 sec / 10 steps |\n",
      "| step: 432320 | gen_loss: 46.527 | mel_loss: 10.331 | 3.472 sec / 10 steps |\n",
      "| step: 432330 | gen_loss: 49.818 | mel_loss: 11.608 | 3.051 sec / 10 steps |\n",
      "| step: 432340 | gen_loss: 48.950 | mel_loss: 11.307 | 3.313 sec / 10 steps |\n",
      "| step: 432350 | gen_loss: 49.191 | mel_loss: 11.283 | 3.295 sec / 10 steps |\n",
      "| step: 432360 | gen_loss: 50.504 | mel_loss: 12.468 | 3.244 sec / 10 steps |\n",
      "| step: 432370 | gen_loss: 46.956 | mel_loss: 11.759 | 3.282 sec / 10 steps |\n",
      "| step: 432380 | gen_loss: 48.945 | mel_loss: 11.821 | 3.250 sec / 10 steps |\n",
      "| step: 432390 | gen_loss: 48.255 | mel_loss: 11.412 | 3.377 sec / 10 steps |\n",
      "| step: 432400 | gen_loss: 45.348 | mel_loss: 9.936 | 3.301 sec / 10 steps |\n",
      "Validation mel_loss: 13.818269729614258\n",
      "| step: 432410 | gen_loss: 48.162 | mel_loss: 10.359 | 3.334 sec / 10 steps |\n",
      "| step: 432420 | gen_loss: 52.137 | mel_loss: 12.575 | 3.106 sec / 10 steps |\n",
      "| step: 432430 | gen_loss: 52.313 | mel_loss: 11.992 | 2.912 sec / 10 steps |\n",
      "| step: 432440 | gen_loss: 40.794 | mel_loss: 8.345 | 3.247 sec / 10 steps |\n",
      "| step: 432450 | gen_loss: 39.839 | mel_loss: 8.544 | 3.568 sec / 10 steps |\n",
      "| step: 432460 | gen_loss: 41.001 | mel_loss: 8.869 | 3.607 sec / 10 steps |\n",
      "| step: 432470 | gen_loss: 51.369 | mel_loss: 13.003 | 3.280 sec / 10 steps |\n",
      "| step: 432480 | gen_loss: 43.994 | mel_loss: 9.019 | 3.146 sec / 10 steps |\n",
      "| step: 432490 | gen_loss: 44.922 | mel_loss: 10.065 | 3.301 sec / 10 steps |\n",
      "| step: 432500 | gen_loss: 48.791 | mel_loss: 11.177 | 3.170 sec / 10 steps |\n",
      "Validation mel_loss: 13.94505786895752\n",
      "|| Epoch: 576 ||\n",
      "| step: 432510 | gen_loss: 43.393 | mel_loss: 9.831 | 3.340 sec / 10 steps |\n",
      "| step: 432520 | gen_loss: 52.469 | mel_loss: 12.263 | 3.268 sec / 10 steps |\n",
      "| step: 432530 | gen_loss: 46.163 | mel_loss: 11.144 | 3.123 sec / 10 steps |\n",
      "| step: 432540 | gen_loss: 47.526 | mel_loss: 10.801 | 3.134 sec / 10 steps |\n",
      "| step: 432550 | gen_loss: 44.900 | mel_loss: 10.248 | 3.285 sec / 10 steps |\n",
      "| step: 432560 | gen_loss: 53.147 | mel_loss: 12.324 | 3.093 sec / 10 steps |\n",
      "| step: 432570 | gen_loss: 44.287 | mel_loss: 10.193 | 3.585 sec / 10 steps |\n",
      "| step: 432580 | gen_loss: 47.640 | mel_loss: 11.308 | 3.419 sec / 10 steps |\n",
      "| step: 432590 | gen_loss: 48.688 | mel_loss: 11.044 | 3.198 sec / 10 steps |\n",
      "| step: 432600 | gen_loss: 45.142 | mel_loss: 9.993 | 3.202 sec / 10 steps |\n",
      "Validation mel_loss: 13.824183464050293\n",
      "| step: 432610 | gen_loss: 42.420 | mel_loss: 8.831 | 3.464 sec / 10 steps |\n",
      "| step: 432620 | gen_loss: 47.246 | mel_loss: 10.316 | 3.396 sec / 10 steps |\n",
      "| step: 432630 | gen_loss: 48.735 | mel_loss: 12.012 | 3.535 sec / 10 steps |\n",
      "| step: 432640 | gen_loss: 48.464 | mel_loss: 11.230 | 2.969 sec / 10 steps |\n",
      "| step: 432650 | gen_loss: 44.298 | mel_loss: 9.832 | 3.304 sec / 10 steps |\n",
      "| step: 432660 | gen_loss: 48.546 | mel_loss: 11.523 | 3.359 sec / 10 steps |\n",
      "| step: 432670 | gen_loss: 43.220 | mel_loss: 8.920 | 3.085 sec / 10 steps |\n",
      "| step: 432680 | gen_loss: 45.775 | mel_loss: 10.783 | 3.498 sec / 10 steps |\n",
      "| step: 432690 | gen_loss: 47.668 | mel_loss: 11.813 | 3.224 sec / 10 steps |\n",
      "| step: 432700 | gen_loss: 39.827 | mel_loss: 8.402 | 3.469 sec / 10 steps |\n",
      "Validation mel_loss: 13.94196891784668\n",
      "| step: 432710 | gen_loss: 47.544 | mel_loss: 11.290 | 3.298 sec / 10 steps |\n",
      "| step: 432720 | gen_loss: 48.384 | mel_loss: 11.649 | 3.091 sec / 10 steps |\n",
      "| step: 432730 | gen_loss: 44.105 | mel_loss: 9.156 | 2.951 sec / 10 steps |\n",
      "| step: 432740 | gen_loss: 44.138 | mel_loss: 9.597 | 3.622 sec / 10 steps |\n",
      "| step: 432750 | gen_loss: 49.497 | mel_loss: 12.154 | 3.268 sec / 10 steps |\n",
      "| step: 432760 | gen_loss: 52.313 | mel_loss: 12.785 | 3.181 sec / 10 steps |\n",
      "| step: 432770 | gen_loss: 38.361 | mel_loss: 9.155 | 3.272 sec / 10 steps |\n",
      "| step: 432780 | gen_loss: 46.648 | mel_loss: 11.190 | 3.331 sec / 10 steps |\n",
      "| step: 432790 | gen_loss: 48.291 | mel_loss: 11.928 | 2.929 sec / 10 steps |\n",
      "| step: 432800 | gen_loss: 53.588 | mel_loss: 13.059 | 3.207 sec / 10 steps |\n",
      "Validation mel_loss: 14.191136360168457\n",
      "| step: 432810 | gen_loss: 43.463 | mel_loss: 9.857 | 3.157 sec / 10 steps |\n",
      "| step: 432820 | gen_loss: 45.968 | mel_loss: 11.634 | 3.346 sec / 10 steps |\n",
      "| step: 432830 | gen_loss: 50.312 | mel_loss: 12.868 | 3.043 sec / 10 steps |\n",
      "| step: 432840 | gen_loss: 50.029 | mel_loss: 12.065 | 3.108 sec / 10 steps |\n",
      "| step: 432850 | gen_loss: 50.359 | mel_loss: 12.139 | 3.040 sec / 10 steps |\n",
      "| step: 432860 | gen_loss: 48.331 | mel_loss: 11.094 | 3.115 sec / 10 steps |\n",
      "| step: 432870 | gen_loss: 50.572 | mel_loss: 11.061 | 3.410 sec / 10 steps |\n",
      "| step: 432880 | gen_loss: 47.896 | mel_loss: 12.036 | 3.005 sec / 10 steps |\n",
      "| step: 432890 | gen_loss: 49.149 | mel_loss: 10.190 | 3.274 sec / 10 steps |\n",
      "| step: 432900 | gen_loss: 44.130 | mel_loss: 10.886 | 3.297 sec / 10 steps |\n",
      "Validation mel_loss: 13.903979301452637\n",
      "| step: 432910 | gen_loss: 49.236 | mel_loss: 10.956 | 3.198 sec / 10 steps |\n",
      "| step: 432920 | gen_loss: 47.043 | mel_loss: 11.021 | 3.259 sec / 10 steps |\n",
      "| step: 432930 | gen_loss: 48.026 | mel_loss: 10.859 | 3.687 sec / 10 steps |\n",
      "| step: 432940 | gen_loss: 49.135 | mel_loss: 11.981 | 3.328 sec / 10 steps |\n",
      "| step: 432950 | gen_loss: 48.114 | mel_loss: 10.489 | 2.884 sec / 10 steps |\n",
      "| step: 432960 | gen_loss: 48.655 | mel_loss: 11.569 | 3.093 sec / 10 steps |\n",
      "| step: 432970 | gen_loss: 47.726 | mel_loss: 11.955 | 2.902 sec / 10 steps |\n",
      "| step: 432980 | gen_loss: 52.397 | mel_loss: 12.254 | 3.154 sec / 10 steps |\n",
      "| step: 432990 | gen_loss: 47.661 | mel_loss: 11.359 | 3.151 sec / 10 steps |\n",
      "| step: 433000 | gen_loss: 50.423 | mel_loss: 12.028 | 2.977 sec / 10 steps |\n",
      "Validation mel_loss: 13.968737602233887\n",
      "| step: 433010 | gen_loss: 50.038 | mel_loss: 12.226 | 3.603 sec / 10 steps |\n",
      "| step: 433020 | gen_loss: 48.267 | mel_loss: 11.927 | 3.106 sec / 10 steps |\n",
      "| step: 433030 | gen_loss: 45.927 | mel_loss: 11.474 | 2.954 sec / 10 steps |\n",
      "| step: 433040 | gen_loss: 44.906 | mel_loss: 10.683 | 3.197 sec / 10 steps |\n",
      "| step: 433050 | gen_loss: 46.245 | mel_loss: 11.215 | 3.280 sec / 10 steps |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 433060 | gen_loss: 43.991 | mel_loss: 9.798 | 3.172 sec / 10 steps |\n",
      "| step: 433070 | gen_loss: 47.059 | mel_loss: 10.711 | 3.539 sec / 10 steps |\n",
      "| step: 433080 | gen_loss: 51.542 | mel_loss: 12.327 | 2.967 sec / 10 steps |\n",
      "| step: 433090 | gen_loss: 48.252 | mel_loss: 11.808 | 3.213 sec / 10 steps |\n",
      "| step: 433100 | gen_loss: 44.530 | mel_loss: 10.007 | 3.061 sec / 10 steps |\n",
      "Validation mel_loss: 13.901386260986328\n",
      "| step: 433110 | gen_loss: 46.310 | mel_loss: 10.991 | 3.458 sec / 10 steps |\n",
      "| step: 433120 | gen_loss: 43.114 | mel_loss: 9.351 | 3.500 sec / 10 steps |\n",
      "| step: 433130 | gen_loss: 45.972 | mel_loss: 10.900 | 3.444 sec / 10 steps |\n",
      "| step: 433140 | gen_loss: 47.546 | mel_loss: 10.529 | 2.919 sec / 10 steps |\n",
      "| step: 433150 | gen_loss: 51.116 | mel_loss: 12.517 | 3.370 sec / 10 steps |\n",
      "| step: 433160 | gen_loss: 51.646 | mel_loss: 12.726 | 3.264 sec / 10 steps |\n",
      "| step: 433170 | gen_loss: 49.771 | mel_loss: 12.075 | 3.044 sec / 10 steps |\n",
      "| step: 433180 | gen_loss: 42.570 | mel_loss: 9.521 | 3.547 sec / 10 steps |\n",
      "| step: 433190 | gen_loss: 42.596 | mel_loss: 9.318 | 3.408 sec / 10 steps |\n",
      "| step: 433200 | gen_loss: 47.831 | mel_loss: 10.701 | 3.287 sec / 10 steps |\n",
      "Validation mel_loss: 14.021623611450195\n",
      "| step: 433210 | gen_loss: 48.333 | mel_loss: 12.638 | 3.315 sec / 10 steps |\n",
      "| step: 433220 | gen_loss: 44.421 | mel_loss: 10.155 | 3.249 sec / 10 steps |\n",
      "| step: 433230 | gen_loss: 42.796 | mel_loss: 10.310 | 2.981 sec / 10 steps |\n",
      "| step: 433240 | gen_loss: 45.972 | mel_loss: 10.966 | 3.552 sec / 10 steps |\n",
      "| step: 433250 | gen_loss: 44.191 | mel_loss: 9.775 | 3.092 sec / 10 steps |\n",
      "|| Epoch: 577 ||\n",
      "| step: 433260 | gen_loss: 48.660 | mel_loss: 11.743 | 3.245 sec / 10 steps |\n",
      "| step: 433270 | gen_loss: 46.367 | mel_loss: 11.251 | 3.204 sec / 10 steps |\n",
      "| step: 433280 | gen_loss: 45.048 | mel_loss: 10.381 | 3.355 sec / 10 steps |\n",
      "| step: 433290 | gen_loss: 45.523 | mel_loss: 10.536 | 3.501 sec / 10 steps |\n",
      "| step: 433300 | gen_loss: 48.350 | mel_loss: 11.354 | 3.125 sec / 10 steps |\n",
      "Validation mel_loss: 13.892404556274414\n",
      "| step: 433310 | gen_loss: 47.467 | mel_loss: 11.220 | 3.516 sec / 10 steps |\n",
      "| step: 433320 | gen_loss: 45.835 | mel_loss: 10.177 | 3.386 sec / 10 steps |\n",
      "| step: 433330 | gen_loss: 50.420 | mel_loss: 11.318 | 3.053 sec / 10 steps |\n",
      "| step: 433340 | gen_loss: 52.539 | mel_loss: 12.571 | 3.035 sec / 10 steps |\n",
      "| step: 433350 | gen_loss: 50.071 | mel_loss: 12.104 | 3.058 sec / 10 steps |\n",
      "| step: 433360 | gen_loss: 45.883 | mel_loss: 10.654 | 3.503 sec / 10 steps |\n",
      "| step: 433370 | gen_loss: 50.629 | mel_loss: 12.430 | 3.157 sec / 10 steps |\n",
      "| step: 433380 | gen_loss: 48.801 | mel_loss: 12.014 | 3.325 sec / 10 steps |\n",
      "| step: 433390 | gen_loss: 45.102 | mel_loss: 9.834 | 3.300 sec / 10 steps |\n",
      "| step: 433400 | gen_loss: 49.146 | mel_loss: 12.071 | 3.396 sec / 10 steps |\n",
      "Validation mel_loss: 13.86136245727539\n",
      "| step: 433410 | gen_loss: 47.686 | mel_loss: 11.483 | 3.318 sec / 10 steps |\n",
      "| step: 433420 | gen_loss: 43.580 | mel_loss: 9.496 | 3.534 sec / 10 steps |\n",
      "| step: 433430 | gen_loss: 47.138 | mel_loss: 10.892 | 3.402 sec / 10 steps |\n",
      "| step: 433440 | gen_loss: 41.824 | mel_loss: 9.269 | 3.142 sec / 10 steps |\n",
      "| step: 433450 | gen_loss: 49.319 | mel_loss: 11.676 | 2.978 sec / 10 steps |\n",
      "| step: 433460 | gen_loss: 48.726 | mel_loss: 10.693 | 3.357 sec / 10 steps |\n",
      "| step: 433470 | gen_loss: 45.312 | mel_loss: 10.162 | 3.285 sec / 10 steps |\n",
      "| step: 433480 | gen_loss: 47.992 | mel_loss: 10.706 | 3.552 sec / 10 steps |\n",
      "| step: 433490 | gen_loss: 50.233 | mel_loss: 12.274 | 2.966 sec / 10 steps |\n",
      "| step: 433500 | gen_loss: 45.189 | mel_loss: 10.899 | 3.172 sec / 10 steps |\n",
      "Validation mel_loss: 13.939507484436035\n",
      "| step: 433510 | gen_loss: 45.708 | mel_loss: 10.878 | 3.294 sec / 10 steps |\n",
      "| step: 433520 | gen_loss: 48.735 | mel_loss: 10.773 | 2.946 sec / 10 steps |\n",
      "| step: 433530 | gen_loss: 49.182 | mel_loss: 11.043 | 3.134 sec / 10 steps |\n",
      "| step: 433540 | gen_loss: 50.387 | mel_loss: 12.763 | 3.079 sec / 10 steps |\n",
      "| step: 433550 | gen_loss: 48.220 | mel_loss: 11.609 | 3.146 sec / 10 steps |\n",
      "| step: 433560 | gen_loss: 52.665 | mel_loss: 13.297 | 2.978 sec / 10 steps |\n",
      "| step: 433570 | gen_loss: 50.616 | mel_loss: 11.956 | 3.088 sec / 10 steps |\n",
      "| step: 433580 | gen_loss: 48.743 | mel_loss: 11.704 | 3.029 sec / 10 steps |\n",
      "| step: 433590 | gen_loss: 44.428 | mel_loss: 9.866 | 3.528 sec / 10 steps |\n",
      "| step: 433600 | gen_loss: 41.189 | mel_loss: 10.444 | 3.409 sec / 10 steps |\n",
      "Validation mel_loss: 13.96066665649414\n",
      "| step: 433610 | gen_loss: 39.290 | mel_loss: 9.205 | 3.534 sec / 10 steps |\n",
      "| step: 433620 | gen_loss: 40.174 | mel_loss: 10.217 | 3.111 sec / 10 steps |\n",
      "| step: 433630 | gen_loss: 49.238 | mel_loss: 11.708 | 2.688 sec / 10 steps |\n",
      "| step: 433640 | gen_loss: 40.159 | mel_loss: 8.439 | 3.123 sec / 10 steps |\n",
      "| step: 433650 | gen_loss: 48.476 | mel_loss: 12.062 | 3.317 sec / 10 steps |\n",
      "| step: 433660 | gen_loss: 38.974 | mel_loss: 8.774 | 3.267 sec / 10 steps |\n",
      "| step: 433670 | gen_loss: 51.517 | mel_loss: 11.799 | 3.009 sec / 10 steps |\n",
      "| step: 433680 | gen_loss: 49.780 | mel_loss: 11.960 | 3.263 sec / 10 steps |\n",
      "| step: 433690 | gen_loss: 48.935 | mel_loss: 12.164 | 3.321 sec / 10 steps |\n",
      "| step: 433700 | gen_loss: 51.440 | mel_loss: 11.980 | 3.405 sec / 10 steps |\n",
      "Validation mel_loss: 14.017183303833008\n",
      "| step: 433710 | gen_loss: 45.824 | mel_loss: 11.303 | 3.119 sec / 10 steps |\n",
      "| step: 433720 | gen_loss: 45.392 | mel_loss: 10.414 | 3.477 sec / 10 steps |\n",
      "| step: 433730 | gen_loss: 47.618 | mel_loss: 10.993 | 3.130 sec / 10 steps |\n",
      "| step: 433740 | gen_loss: 47.035 | mel_loss: 10.711 | 3.088 sec / 10 steps |\n",
      "| step: 433750 | gen_loss: 48.075 | mel_loss: 12.266 | 3.088 sec / 10 steps |\n",
      "| step: 433760 | gen_loss: 46.851 | mel_loss: 11.602 | 2.997 sec / 10 steps |\n",
      "| step: 433770 | gen_loss: 48.696 | mel_loss: 12.432 | 3.299 sec / 10 steps |\n",
      "| step: 433780 | gen_loss: 46.629 | mel_loss: 11.141 | 3.378 sec / 10 steps |\n",
      "| step: 433790 | gen_loss: 43.671 | mel_loss: 10.171 | 3.236 sec / 10 steps |\n",
      "| step: 433800 | gen_loss: 40.504 | mel_loss: 8.517 | 3.232 sec / 10 steps |\n",
      "Validation mel_loss: 14.143779754638672\n",
      "| step: 433810 | gen_loss: 49.328 | mel_loss: 12.560 | 3.670 sec / 10 steps |\n",
      "| step: 433820 | gen_loss: 42.682 | mel_loss: 9.316 | 3.739 sec / 10 steps |\n",
      "| step: 433830 | gen_loss: 47.314 | mel_loss: 11.533 | 3.312 sec / 10 steps |\n",
      "| step: 433840 | gen_loss: 43.446 | mel_loss: 9.802 | 3.350 sec / 10 steps |\n",
      "| step: 433850 | gen_loss: 48.099 | mel_loss: 12.017 | 3.054 sec / 10 steps |\n",
      "| step: 433860 | gen_loss: 46.962 | mel_loss: 10.676 | 3.713 sec / 10 steps |\n",
      "| step: 433870 | gen_loss: 52.827 | mel_loss: 12.530 | 2.953 sec / 10 steps |\n",
      "| step: 433880 | gen_loss: 47.179 | mel_loss: 11.287 | 3.107 sec / 10 steps |\n",
      "| step: 433890 | gen_loss: 49.411 | mel_loss: 11.933 | 3.388 sec / 10 steps |\n",
      "| step: 433900 | gen_loss: 38.458 | mel_loss: 8.144 | 3.314 sec / 10 steps |\n",
      "Validation mel_loss: 14.08299446105957\n",
      "| step: 433910 | gen_loss: 42.025 | mel_loss: 9.513 | 3.466 sec / 10 steps |\n",
      "| step: 433920 | gen_loss: 39.779 | mel_loss: 10.068 | 3.147 sec / 10 steps |\n",
      "| step: 433930 | gen_loss: 45.333 | mel_loss: 10.954 | 3.277 sec / 10 steps |\n",
      "| step: 433940 | gen_loss: 52.283 | mel_loss: 13.209 | 3.125 sec / 10 steps |\n",
      "| step: 433950 | gen_loss: 45.236 | mel_loss: 10.852 | 3.051 sec / 10 steps |\n",
      "| step: 433960 | gen_loss: 46.798 | mel_loss: 11.063 | 3.103 sec / 10 steps |\n",
      "| step: 433970 | gen_loss: 45.233 | mel_loss: 10.504 | 3.094 sec / 10 steps |\n",
      "| step: 433980 | gen_loss: 43.059 | mel_loss: 10.282 | 3.575 sec / 10 steps |\n",
      "| step: 433990 | gen_loss: 47.677 | mel_loss: 11.601 | 3.023 sec / 10 steps |\n",
      "| step: 434000 | gen_loss: 54.028 | mel_loss: 12.889 | 3.263 sec / 10 steps |\n",
      "Validation mel_loss: 14.313472747802734\n",
      "|| Epoch: 578 ||\n",
      "| step: 434010 | gen_loss: 46.985 | mel_loss: 11.349 | 3.282 sec / 10 steps |\n",
      "| step: 434020 | gen_loss: 49.734 | mel_loss: 11.851 | 3.163 sec / 10 steps |\n",
      "| step: 434030 | gen_loss: 48.659 | mel_loss: 11.437 | 3.319 sec / 10 steps |\n",
      "| step: 434040 | gen_loss: 48.632 | mel_loss: 11.298 | 3.195 sec / 10 steps |\n",
      "| step: 434050 | gen_loss: 47.343 | mel_loss: 11.712 | 3.457 sec / 10 steps |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 434060 | gen_loss: 51.929 | mel_loss: 12.349 | 3.110 sec / 10 steps |\n",
      "| step: 434070 | gen_loss: 46.376 | mel_loss: 11.003 | 3.180 sec / 10 steps |\n",
      "| step: 434080 | gen_loss: 46.854 | mel_loss: 11.815 | 2.910 sec / 10 steps |\n",
      "| step: 434090 | gen_loss: 46.471 | mel_loss: 10.797 | 3.201 sec / 10 steps |\n",
      "| step: 434100 | gen_loss: 39.609 | mel_loss: 8.445 | 3.220 sec / 10 steps |\n",
      "Validation mel_loss: 14.166763305664062\n",
      "| step: 434110 | gen_loss: 34.414 | mel_loss: 7.901 | 3.532 sec / 10 steps |\n",
      "| step: 434120 | gen_loss: 41.390 | mel_loss: 9.964 | 3.189 sec / 10 steps |\n",
      "| step: 434130 | gen_loss: 42.829 | mel_loss: 10.442 | 3.193 sec / 10 steps |\n",
      "| step: 434140 | gen_loss: 43.551 | mel_loss: 10.593 | 3.413 sec / 10 steps |\n",
      "| step: 434150 | gen_loss: 40.710 | mel_loss: 10.318 | 3.157 sec / 10 steps |\n",
      "| step: 434160 | gen_loss: 48.279 | mel_loss: 12.200 | 3.271 sec / 10 steps |\n",
      "| step: 434170 | gen_loss: 44.580 | mel_loss: 11.564 | 3.034 sec / 10 steps |\n",
      "| step: 434180 | gen_loss: 51.080 | mel_loss: 12.826 | 2.936 sec / 10 steps |\n",
      "| step: 434190 | gen_loss: 37.126 | mel_loss: 9.080 | 3.272 sec / 10 steps |\n",
      "| step: 434200 | gen_loss: 47.564 | mel_loss: 10.441 | 3.678 sec / 10 steps |\n",
      "Validation mel_loss: 14.176051139831543\n",
      "| step: 434210 | gen_loss: 48.455 | mel_loss: 11.795 | 3.590 sec / 10 steps |\n",
      "| step: 434220 | gen_loss: 45.903 | mel_loss: 10.519 | 3.352 sec / 10 steps |\n",
      "| step: 434230 | gen_loss: 43.934 | mel_loss: 9.571 | 3.651 sec / 10 steps |\n",
      "| step: 434240 | gen_loss: 45.323 | mel_loss: 10.777 | 3.582 sec / 10 steps |\n",
      "| step: 434250 | gen_loss: 44.316 | mel_loss: 9.381 | 3.636 sec / 10 steps |\n",
      "| step: 434260 | gen_loss: 49.989 | mel_loss: 11.841 | 2.968 sec / 10 steps |\n",
      "| step: 434270 | gen_loss: 46.742 | mel_loss: 10.572 | 3.393 sec / 10 steps |\n",
      "| step: 434280 | gen_loss: 45.182 | mel_loss: 9.842 | 3.119 sec / 10 steps |\n",
      "| step: 434290 | gen_loss: 48.946 | mel_loss: 11.717 | 3.173 sec / 10 steps |\n",
      "| step: 434300 | gen_loss: 44.391 | mel_loss: 9.683 | 3.505 sec / 10 steps |\n",
      "Validation mel_loss: 14.235818862915039\n",
      "| step: 434310 | gen_loss: 50.350 | mel_loss: 12.413 | 3.636 sec / 10 steps |\n",
      "| step: 434320 | gen_loss: 44.826 | mel_loss: 10.054 | 2.905 sec / 10 steps |\n",
      "| step: 434330 | gen_loss: 50.497 | mel_loss: 11.584 | 3.270 sec / 10 steps |\n",
      "| step: 434340 | gen_loss: 49.558 | mel_loss: 11.720 | 3.423 sec / 10 steps |\n",
      "| step: 434350 | gen_loss: 45.423 | mel_loss: 10.311 | 3.219 sec / 10 steps |\n",
      "| step: 434360 | gen_loss: 43.639 | mel_loss: 9.344 | 3.648 sec / 10 steps |\n",
      "| step: 434370 | gen_loss: 53.873 | mel_loss: 13.366 | 3.251 sec / 10 steps |\n",
      "| step: 434380 | gen_loss: 49.891 | mel_loss: 12.475 | 3.146 sec / 10 steps |\n",
      "| step: 434390 | gen_loss: 49.511 | mel_loss: 11.485 | 3.379 sec / 10 steps |\n",
      "| step: 434400 | gen_loss: 40.118 | mel_loss: 8.529 | 3.687 sec / 10 steps |\n",
      "Validation mel_loss: 14.206974029541016\n",
      "| step: 434410 | gen_loss: 49.215 | mel_loss: 12.675 | 3.236 sec / 10 steps |\n",
      "| step: 434420 | gen_loss: 46.869 | mel_loss: 11.766 | 3.248 sec / 10 steps |\n",
      "| step: 434430 | gen_loss: 44.564 | mel_loss: 9.813 | 3.234 sec / 10 steps |\n",
      "| step: 434440 | gen_loss: 49.840 | mel_loss: 11.885 | 3.201 sec / 10 steps |\n",
      "| step: 434450 | gen_loss: 45.529 | mel_loss: 10.149 | 3.350 sec / 10 steps |\n",
      "| step: 434460 | gen_loss: 42.498 | mel_loss: 9.673 | 3.457 sec / 10 steps |\n",
      "| step: 434470 | gen_loss: 53.428 | mel_loss: 13.154 | 3.512 sec / 10 steps |\n",
      "| step: 434480 | gen_loss: 41.285 | mel_loss: 9.103 | 3.392 sec / 10 steps |\n",
      "| step: 434490 | gen_loss: 46.892 | mel_loss: 10.995 | 2.978 sec / 10 steps |\n",
      "| step: 434500 | gen_loss: 53.983 | mel_loss: 12.724 | 3.238 sec / 10 steps |\n",
      "Validation mel_loss: 14.284066200256348\n",
      "| step: 434510 | gen_loss: 48.448 | mel_loss: 11.021 | 3.592 sec / 10 steps |\n",
      "| step: 434520 | gen_loss: 46.836 | mel_loss: 12.169 | 3.452 sec / 10 steps |\n",
      "| step: 434530 | gen_loss: 47.945 | mel_loss: 11.988 | 3.254 sec / 10 steps |\n",
      "| step: 434540 | gen_loss: 49.236 | mel_loss: 11.604 | 3.334 sec / 10 steps |\n",
      "| step: 434550 | gen_loss: 49.318 | mel_loss: 11.767 | 3.191 sec / 10 steps |\n",
      "| step: 434560 | gen_loss: 41.813 | mel_loss: 9.869 | 3.163 sec / 10 steps |\n",
      "| step: 434570 | gen_loss: 42.836 | mel_loss: 9.552 | 3.424 sec / 10 steps |\n",
      "| step: 434580 | gen_loss: 45.829 | mel_loss: 11.255 | 3.356 sec / 10 steps |\n",
      "| step: 434590 | gen_loss: 53.176 | mel_loss: 13.088 | 3.280 sec / 10 steps |\n",
      "| step: 434600 | gen_loss: 44.624 | mel_loss: 10.099 | 3.182 sec / 10 steps |\n",
      "Validation mel_loss: 14.267779350280762\n",
      "| step: 434610 | gen_loss: 42.343 | mel_loss: 10.298 | 3.153 sec / 10 steps |\n",
      "| step: 434620 | gen_loss: 40.639 | mel_loss: 9.883 | 3.201 sec / 10 steps |\n",
      "| step: 434630 | gen_loss: 49.422 | mel_loss: 12.295 | 3.024 sec / 10 steps |\n",
      "| step: 434640 | gen_loss: 48.518 | mel_loss: 12.305 | 3.333 sec / 10 steps |\n",
      "| step: 434650 | gen_loss: 46.246 | mel_loss: 11.932 | 3.299 sec / 10 steps |\n",
      "| step: 434660 | gen_loss: 46.339 | mel_loss: 11.457 | 3.011 sec / 10 steps |\n",
      "| step: 434670 | gen_loss: 49.145 | mel_loss: 11.203 | 3.274 sec / 10 steps |\n",
      "| step: 434680 | gen_loss: 44.072 | mel_loss: 10.018 | 3.190 sec / 10 steps |\n",
      "| step: 434690 | gen_loss: 46.485 | mel_loss: 11.411 | 3.238 sec / 10 steps |\n",
      "| step: 434700 | gen_loss: 48.411 | mel_loss: 11.451 | 3.800 sec / 10 steps |\n",
      "Validation mel_loss: 14.233797073364258\n",
      "| step: 434710 | gen_loss: 46.671 | mel_loss: 10.499 | 3.186 sec / 10 steps |\n",
      "| step: 434720 | gen_loss: 48.529 | mel_loss: 11.331 | 3.069 sec / 10 steps |\n",
      "| step: 434730 | gen_loss: 42.409 | mel_loss: 9.818 | 3.309 sec / 10 steps |\n",
      "| step: 434740 | gen_loss: 43.497 | mel_loss: 10.840 | 3.279 sec / 10 steps |\n",
      "| step: 434750 | gen_loss: 52.256 | mel_loss: 13.161 | 3.077 sec / 10 steps |\n",
      "|| Epoch: 579 ||\n",
      "| step: 434760 | gen_loss: 41.762 | mel_loss: 9.474 | 3.266 sec / 10 steps |\n",
      "| step: 434770 | gen_loss: 47.587 | mel_loss: 11.224 | 3.389 sec / 10 steps |\n",
      "| step: 434780 | gen_loss: 43.793 | mel_loss: 11.513 | 3.258 sec / 10 steps |\n",
      "| step: 434790 | gen_loss: 48.603 | mel_loss: 11.806 | 3.375 sec / 10 steps |\n",
      "| step: 434800 | gen_loss: 46.563 | mel_loss: 10.236 | 3.604 sec / 10 steps |\n",
      "Validation mel_loss: 14.329206466674805\n",
      "| step: 434810 | gen_loss: 48.960 | mel_loss: 11.580 | 3.258 sec / 10 steps |\n",
      "| step: 434820 | gen_loss: 50.025 | mel_loss: 11.849 | 3.324 sec / 10 steps |\n",
      "| step: 434830 | gen_loss: 43.529 | mel_loss: 9.550 | 3.676 sec / 10 steps |\n",
      "| step: 434840 | gen_loss: 52.040 | mel_loss: 12.342 | 2.998 sec / 10 steps |\n",
      "| step: 434850 | gen_loss: 49.921 | mel_loss: 12.052 | 3.357 sec / 10 steps |\n",
      "| step: 434860 | gen_loss: 48.365 | mel_loss: 11.466 | 3.258 sec / 10 steps |\n",
      "| step: 434870 | gen_loss: 46.596 | mel_loss: 10.588 | 3.327 sec / 10 steps |\n",
      "| step: 434880 | gen_loss: 42.450 | mel_loss: 9.546 | 3.236 sec / 10 steps |\n",
      "| step: 434890 | gen_loss: 50.552 | mel_loss: 12.619 | 3.184 sec / 10 steps |\n",
      "| step: 434900 | gen_loss: 45.497 | mel_loss: 10.292 | 3.082 sec / 10 steps |\n",
      "Validation mel_loss: 14.1944580078125\n",
      "| step: 434910 | gen_loss: 49.695 | mel_loss: 11.744 | 3.153 sec / 10 steps |\n",
      "| step: 434920 | gen_loss: 43.336 | mel_loss: 9.702 | 3.222 sec / 10 steps |\n",
      "| step: 434930 | gen_loss: 49.452 | mel_loss: 11.384 | 3.173 sec / 10 steps |\n",
      "| step: 434940 | gen_loss: 50.544 | mel_loss: 12.045 | 3.146 sec / 10 steps |\n",
      "| step: 434950 | gen_loss: 49.530 | mel_loss: 11.847 | 3.227 sec / 10 steps |\n",
      "| step: 434960 | gen_loss: 41.041 | mel_loss: 9.564 | 3.657 sec / 10 steps |\n",
      "| step: 434970 | gen_loss: 50.350 | mel_loss: 11.919 | 3.057 sec / 10 steps |\n",
      "| step: 434980 | gen_loss: 50.005 | mel_loss: 11.855 | 3.238 sec / 10 steps |\n",
      "| step: 434990 | gen_loss: 51.131 | mel_loss: 13.501 | 3.494 sec / 10 steps |\n",
      "| step: 435000 | gen_loss: 43.421 | mel_loss: 9.859 | 3.324 sec / 10 steps |\n",
      "Validation mel_loss: 14.097870826721191\n",
      "| step: 435010 | gen_loss: 44.460 | mel_loss: 10.335 | 3.505 sec / 10 steps |\n",
      "| step: 435020 | gen_loss: 43.553 | mel_loss: 9.725 | 3.172 sec / 10 steps |\n",
      "| step: 435030 | gen_loss: 43.892 | mel_loss: 9.681 | 3.386 sec / 10 steps |\n",
      "| step: 435040 | gen_loss: 46.184 | mel_loss: 10.238 | 3.221 sec / 10 steps |\n",
      "| step: 435050 | gen_loss: 50.813 | mel_loss: 12.180 | 3.339 sec / 10 steps |\n",
      "| step: 435060 | gen_loss: 49.640 | mel_loss: 12.053 | 3.410 sec / 10 steps |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 435070 | gen_loss: 50.825 | mel_loss: 12.921 | 3.372 sec / 10 steps |\n",
      "| step: 435080 | gen_loss: 47.391 | mel_loss: 10.346 | 3.332 sec / 10 steps |\n",
      "| step: 435090 | gen_loss: 43.485 | mel_loss: 9.731 | 3.488 sec / 10 steps |\n",
      "| step: 435100 | gen_loss: 51.070 | mel_loss: 12.499 | 2.873 sec / 10 steps |\n",
      "Validation mel_loss: 14.2636079788208\n",
      "| step: 435110 | gen_loss: 46.495 | mel_loss: 11.234 | 3.852 sec / 10 steps |\n",
      "| step: 435120 | gen_loss: 46.812 | mel_loss: 12.046 | 3.434 sec / 10 steps |\n",
      "| step: 435130 | gen_loss: 50.535 | mel_loss: 11.729 | 3.226 sec / 10 steps |\n",
      "| step: 435140 | gen_loss: 48.178 | mel_loss: 11.963 | 3.116 sec / 10 steps |\n",
      "| step: 435150 | gen_loss: 46.597 | mel_loss: 10.496 | 3.333 sec / 10 steps |\n",
      "| step: 435160 | gen_loss: 53.562 | mel_loss: 13.962 | 2.935 sec / 10 steps |\n",
      "| step: 435170 | gen_loss: 48.209 | mel_loss: 12.074 | 3.511 sec / 10 steps |\n",
      "| step: 435180 | gen_loss: 44.293 | mel_loss: 11.429 | 3.312 sec / 10 steps |\n",
      "| step: 435190 | gen_loss: 38.980 | mel_loss: 9.213 | 3.363 sec / 10 steps |\n",
      "| step: 435200 | gen_loss: 46.405 | mel_loss: 11.142 | 3.536 sec / 10 steps |\n",
      "Validation mel_loss: 14.322315216064453\n",
      "| step: 435210 | gen_loss: 42.809 | mel_loss: 9.410 | 3.723 sec / 10 steps |\n",
      "| step: 435220 | gen_loss: 43.677 | mel_loss: 10.819 | 3.157 sec / 10 steps |\n",
      "| step: 435230 | gen_loss: 49.789 | mel_loss: 12.028 | 3.268 sec / 10 steps |\n",
      "| step: 435240 | gen_loss: 47.538 | mel_loss: 11.711 | 3.380 sec / 10 steps |\n",
      "| step: 435250 | gen_loss: 53.588 | mel_loss: 13.229 | 3.289 sec / 10 steps |\n",
      "| step: 435260 | gen_loss: 45.092 | mel_loss: 10.722 | 3.172 sec / 10 steps |\n",
      "| step: 435270 | gen_loss: 50.606 | mel_loss: 12.733 | 2.961 sec / 10 steps |\n",
      "| step: 435280 | gen_loss: 50.533 | mel_loss: 12.833 | 3.540 sec / 10 steps |\n",
      "| step: 435290 | gen_loss: 48.905 | mel_loss: 11.738 | 3.329 sec / 10 steps |\n",
      "| step: 435300 | gen_loss: 41.079 | mel_loss: 9.319 | 3.409 sec / 10 steps |\n",
      "Validation mel_loss: 14.253388404846191\n",
      "| step: 435310 | gen_loss: 44.002 | mel_loss: 9.738 | 3.520 sec / 10 steps |\n",
      "| step: 435320 | gen_loss: 44.126 | mel_loss: 10.194 | 3.278 sec / 10 steps |\n",
      "| step: 435330 | gen_loss: 47.614 | mel_loss: 11.140 | 3.310 sec / 10 steps |\n",
      "| step: 435340 | gen_loss: 46.423 | mel_loss: 11.844 | 3.266 sec / 10 steps |\n",
      "| step: 435350 | gen_loss: 42.406 | mel_loss: 9.766 | 3.505 sec / 10 steps |\n",
      "| step: 435360 | gen_loss: 46.496 | mel_loss: 10.950 | 3.199 sec / 10 steps |\n",
      "| step: 435370 | gen_loss: 41.296 | mel_loss: 10.437 | 3.179 sec / 10 steps |\n",
      "| step: 435380 | gen_loss: 47.805 | mel_loss: 12.345 | 3.356 sec / 10 steps |\n",
      "| step: 435390 | gen_loss: 46.993 | mel_loss: 11.822 | 3.564 sec / 10 steps |\n",
      "| step: 435400 | gen_loss: 47.013 | mel_loss: 11.001 | 3.359 sec / 10 steps |\n",
      "Validation mel_loss: 14.178105354309082\n",
      "| step: 435410 | gen_loss: 52.428 | mel_loss: 13.175 | 3.374 sec / 10 steps |\n",
      "| step: 435420 | gen_loss: 40.596 | mel_loss: 9.771 | 3.211 sec / 10 steps |\n",
      "| step: 435430 | gen_loss: 42.491 | mel_loss: 10.399 | 3.326 sec / 10 steps |\n",
      "| step: 435440 | gen_loss: 42.656 | mel_loss: 10.397 | 3.243 sec / 10 steps |\n",
      "| step: 435450 | gen_loss: 46.828 | mel_loss: 10.916 | 3.474 sec / 10 steps |\n",
      "| step: 435460 | gen_loss: 42.802 | mel_loss: 9.902 | 2.948 sec / 10 steps |\n",
      "| step: 435470 | gen_loss: 51.014 | mel_loss: 12.235 | 3.340 sec / 10 steps |\n",
      "| step: 435480 | gen_loss: 47.522 | mel_loss: 11.070 | 3.410 sec / 10 steps |\n",
      "| step: 435490 | gen_loss: 48.792 | mel_loss: 11.901 | 3.228 sec / 10 steps |\n",
      "| step: 435500 | gen_loss: 46.601 | mel_loss: 11.514 | 3.305 sec / 10 steps |\n",
      "Validation mel_loss: 14.298104286193848\n",
      "|| Epoch: 580 ||\n",
      "| step: 435510 | gen_loss: 44.639 | mel_loss: 11.037 | 3.256 sec / 10 steps |\n",
      "| step: 435520 | gen_loss: 43.977 | mel_loss: 9.361 | 3.194 sec / 10 steps |\n",
      "| step: 435530 | gen_loss: 47.440 | mel_loss: 10.622 | 3.413 sec / 10 steps |\n",
      "| step: 435540 | gen_loss: 45.551 | mel_loss: 10.783 | 3.402 sec / 10 steps |\n",
      "| step: 435550 | gen_loss: 43.338 | mel_loss: 9.789 | 3.590 sec / 10 steps |\n",
      "| step: 435560 | gen_loss: 44.678 | mel_loss: 9.143 | 3.319 sec / 10 steps |\n",
      "| step: 435570 | gen_loss: 48.138 | mel_loss: 12.224 | 3.648 sec / 10 steps |\n",
      "| step: 435580 | gen_loss: 47.345 | mel_loss: 11.269 | 3.563 sec / 10 steps |\n",
      "| step: 435590 | gen_loss: 47.544 | mel_loss: 12.159 | 3.043 sec / 10 steps |\n",
      "| step: 435600 | gen_loss: 41.656 | mel_loss: 9.420 | 3.077 sec / 10 steps |\n",
      "Validation mel_loss: 14.363950729370117\n",
      "| step: 435610 | gen_loss: 47.001 | mel_loss: 11.237 | 3.793 sec / 10 steps |\n",
      "| step: 435620 | gen_loss: 49.791 | mel_loss: 11.846 | 3.366 sec / 10 steps |\n",
      "| step: 435630 | gen_loss: 47.323 | mel_loss: 12.310 | 3.612 sec / 10 steps |\n",
      "| step: 435640 | gen_loss: 44.449 | mel_loss: 10.445 | 3.525 sec / 10 steps |\n",
      "| step: 435650 | gen_loss: 39.478 | mel_loss: 8.585 | 3.175 sec / 10 steps |\n",
      "| step: 435660 | gen_loss: 39.101 | mel_loss: 9.408 | 3.255 sec / 10 steps |\n",
      "| step: 435670 | gen_loss: 49.269 | mel_loss: 11.291 | 3.263 sec / 10 steps |\n",
      "| step: 435680 | gen_loss: 40.115 | mel_loss: 9.136 | 3.452 sec / 10 steps |\n",
      "| step: 435690 | gen_loss: 44.290 | mel_loss: 9.842 | 3.673 sec / 10 steps |\n",
      "| step: 435700 | gen_loss: 43.230 | mel_loss: 10.193 | 3.339 sec / 10 steps |\n",
      "Validation mel_loss: 14.16781234741211\n",
      "| step: 435710 | gen_loss: 37.623 | mel_loss: 8.750 | 3.304 sec / 10 steps |\n",
      "| step: 435720 | gen_loss: 42.168 | mel_loss: 9.295 | 3.297 sec / 10 steps |\n",
      "| step: 435730 | gen_loss: 51.219 | mel_loss: 12.844 | 2.960 sec / 10 steps |\n",
      "| step: 435740 | gen_loss: 44.643 | mel_loss: 9.863 | 3.654 sec / 10 steps |\n",
      "| step: 435750 | gen_loss: 46.494 | mel_loss: 11.626 | 3.033 sec / 10 steps |\n",
      "| step: 435760 | gen_loss: 47.461 | mel_loss: 10.152 | 3.303 sec / 10 steps |\n",
      "| step: 435770 | gen_loss: 43.751 | mel_loss: 9.419 | 3.422 sec / 10 steps |\n",
      "| step: 435780 | gen_loss: 52.325 | mel_loss: 12.237 | 3.361 sec / 10 steps |\n",
      "| step: 435790 | gen_loss: 52.776 | mel_loss: 13.212 | 2.817 sec / 10 steps |\n",
      "| step: 435800 | gen_loss: 49.504 | mel_loss: 12.183 | 3.201 sec / 10 steps |\n",
      "Validation mel_loss: 14.340578079223633\n",
      "| step: 435810 | gen_loss: 51.232 | mel_loss: 12.721 | 3.017 sec / 10 steps |\n",
      "| step: 435820 | gen_loss: 44.016 | mel_loss: 10.150 | 3.025 sec / 10 steps |\n",
      "| step: 435830 | gen_loss: 48.359 | mel_loss: 11.789 | 3.273 sec / 10 steps |\n",
      "| step: 435840 | gen_loss: 53.812 | mel_loss: 12.755 | 3.600 sec / 10 steps |\n",
      "| step: 435850 | gen_loss: 49.597 | mel_loss: 12.315 | 2.906 sec / 10 steps |\n",
      "| step: 435860 | gen_loss: 48.937 | mel_loss: 12.038 | 3.471 sec / 10 steps |\n",
      "| step: 435870 | gen_loss: 44.414 | mel_loss: 10.376 | 3.386 sec / 10 steps |\n",
      "| step: 435880 | gen_loss: 31.273 | mel_loss: 7.887 | 3.272 sec / 10 steps |\n",
      "| step: 435890 | gen_loss: 53.102 | mel_loss: 13.055 | 3.557 sec / 10 steps |\n",
      "| step: 435900 | gen_loss: 46.969 | mel_loss: 11.009 | 3.208 sec / 10 steps |\n",
      "Validation mel_loss: 14.207808494567871\n",
      "| step: 435910 | gen_loss: 43.951 | mel_loss: 10.425 | 3.157 sec / 10 steps |\n",
      "| step: 435920 | gen_loss: 46.073 | mel_loss: 9.929 | 3.468 sec / 10 steps |\n",
      "| step: 435930 | gen_loss: 45.161 | mel_loss: 11.443 | 2.876 sec / 10 steps |\n",
      "| step: 435940 | gen_loss: 45.530 | mel_loss: 11.482 | 3.171 sec / 10 steps |\n",
      "| step: 435950 | gen_loss: 47.144 | mel_loss: 11.530 | 3.417 sec / 10 steps |\n",
      "| step: 435960 | gen_loss: 46.405 | mel_loss: 10.372 | 3.335 sec / 10 steps |\n",
      "| step: 435970 | gen_loss: 48.519 | mel_loss: 12.010 | 3.122 sec / 10 steps |\n",
      "| step: 435980 | gen_loss: 50.025 | mel_loss: 11.425 | 3.041 sec / 10 steps |\n",
      "| step: 435990 | gen_loss: 43.599 | mel_loss: 9.302 | 3.356 sec / 10 steps |\n",
      "| step: 436000 | gen_loss: 45.134 | mel_loss: 11.759 | 2.969 sec / 10 steps |\n",
      "Validation mel_loss: 14.304840087890625\n",
      "| step: 436010 | gen_loss: 43.850 | mel_loss: 10.522 | 3.233 sec / 10 steps |\n",
      "| step: 436020 | gen_loss: 40.961 | mel_loss: 9.870 | 3.156 sec / 10 steps |\n",
      "| step: 436030 | gen_loss: 47.072 | mel_loss: 11.401 | 3.640 sec / 10 steps |\n",
      "| step: 436040 | gen_loss: 47.712 | mel_loss: 12.048 | 2.949 sec / 10 steps |\n",
      "| step: 436050 | gen_loss: 47.444 | mel_loss: 11.655 | 3.256 sec / 10 steps |\n",
      "| step: 436060 | gen_loss: 48.020 | mel_loss: 11.747 | 3.078 sec / 10 steps |\n",
      "| step: 436070 | gen_loss: 43.502 | mel_loss: 9.900 | 3.637 sec / 10 steps |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 436080 | gen_loss: 46.051 | mel_loss: 9.743 | 3.302 sec / 10 steps |\n",
      "| step: 436090 | gen_loss: 45.035 | mel_loss: 10.594 | 3.382 sec / 10 steps |\n",
      "| step: 436100 | gen_loss: 49.038 | mel_loss: 11.807 | 3.265 sec / 10 steps |\n",
      "Validation mel_loss: 14.29291820526123\n",
      "| step: 436110 | gen_loss: 45.746 | mel_loss: 10.310 | 3.403 sec / 10 steps |\n",
      "| step: 436120 | gen_loss: 50.848 | mel_loss: 13.598 | 3.170 sec / 10 steps |\n",
      "| step: 436130 | gen_loss: 49.841 | mel_loss: 12.171 | 3.402 sec / 10 steps |\n",
      "| step: 436140 | gen_loss: 48.740 | mel_loss: 12.075 | 3.073 sec / 10 steps |\n",
      "| step: 436150 | gen_loss: 49.157 | mel_loss: 11.736 | 3.136 sec / 10 steps |\n",
      "| step: 436160 | gen_loss: 44.752 | mel_loss: 10.395 | 3.385 sec / 10 steps |\n",
      "| step: 436170 | gen_loss: 50.483 | mel_loss: 12.804 | 2.942 sec / 10 steps |\n",
      "| step: 436180 | gen_loss: 50.966 | mel_loss: 12.646 | 3.103 sec / 10 steps |\n",
      "| step: 436190 | gen_loss: 51.857 | mel_loss: 12.836 | 3.215 sec / 10 steps |\n",
      "| step: 436200 | gen_loss: 48.562 | mel_loss: 12.046 | 3.514 sec / 10 steps |\n",
      "Validation mel_loss: 14.302104949951172\n",
      "| step: 436210 | gen_loss: 51.087 | mel_loss: 13.109 | 3.003 sec / 10 steps |\n",
      "| step: 436220 | gen_loss: 48.575 | mel_loss: 11.246 | 3.226 sec / 10 steps |\n",
      "| step: 436230 | gen_loss: 43.420 | mel_loss: 9.401 | 3.595 sec / 10 steps |\n",
      "| step: 436240 | gen_loss: 43.399 | mel_loss: 11.138 | 3.185 sec / 10 steps |\n",
      "| step: 436250 | gen_loss: 49.397 | mel_loss: 13.029 | 3.489 sec / 10 steps |\n",
      "|| Epoch: 581 ||\n",
      "| step: 436260 | gen_loss: 42.125 | mel_loss: 9.744 | 3.227 sec / 10 steps |\n",
      "| step: 436270 | gen_loss: 50.655 | mel_loss: 12.389 | 2.876 sec / 10 steps |\n",
      "| step: 436280 | gen_loss: 48.155 | mel_loss: 11.936 | 3.411 sec / 10 steps |\n",
      "| step: 436290 | gen_loss: 46.640 | mel_loss: 12.117 | 3.433 sec / 10 steps |\n",
      "| step: 436300 | gen_loss: 48.398 | mel_loss: 10.935 | 2.947 sec / 10 steps |\n",
      "Validation mel_loss: 14.180699348449707\n",
      "| step: 436310 | gen_loss: 52.344 | mel_loss: 12.606 | 3.254 sec / 10 steps |\n",
      "| step: 436320 | gen_loss: 46.107 | mel_loss: 10.269 | 3.174 sec / 10 steps |\n",
      "| step: 436330 | gen_loss: 49.249 | mel_loss: 11.797 | 3.387 sec / 10 steps |\n",
      "| step: 436340 | gen_loss: 46.585 | mel_loss: 10.133 | 3.187 sec / 10 steps |\n",
      "| step: 436350 | gen_loss: 49.857 | mel_loss: 11.479 | 3.219 sec / 10 steps |\n",
      "| step: 436360 | gen_loss: 50.209 | mel_loss: 11.484 | 3.529 sec / 10 steps |\n",
      "| step: 436370 | gen_loss: 49.882 | mel_loss: 11.720 | 3.406 sec / 10 steps |\n",
      "| step: 436380 | gen_loss: 45.928 | mel_loss: 10.859 | 3.200 sec / 10 steps |\n",
      "| step: 436390 | gen_loss: 52.330 | mel_loss: 11.920 | 3.276 sec / 10 steps |\n",
      "| step: 436400 | gen_loss: 46.183 | mel_loss: 11.043 | 3.222 sec / 10 steps |\n",
      "Validation mel_loss: 14.084342002868652\n",
      "| step: 436410 | gen_loss: 46.607 | mel_loss: 10.218 | 3.271 sec / 10 steps |\n",
      "| step: 436420 | gen_loss: 43.281 | mel_loss: 9.533 | 3.344 sec / 10 steps |\n",
      "| step: 436430 | gen_loss: 53.287 | mel_loss: 12.797 | 2.904 sec / 10 steps |\n",
      "| step: 436440 | gen_loss: 50.940 | mel_loss: 11.119 | 3.171 sec / 10 steps |\n",
      "| step: 436450 | gen_loss: 47.686 | mel_loss: 11.340 | 3.515 sec / 10 steps |\n",
      "| step: 436460 | gen_loss: 52.831 | mel_loss: 12.303 | 3.299 sec / 10 steps |\n",
      "| step: 436470 | gen_loss: 43.750 | mel_loss: 8.955 | 3.342 sec / 10 steps |\n",
      "| step: 436480 | gen_loss: 46.282 | mel_loss: 11.331 | 3.267 sec / 10 steps |\n",
      "| step: 436490 | gen_loss: 49.244 | mel_loss: 11.497 | 3.095 sec / 10 steps |\n",
      "| step: 436500 | gen_loss: 50.321 | mel_loss: 11.803 | 3.214 sec / 10 steps |\n",
      "Validation mel_loss: 14.078545570373535\n",
      "| step: 436510 | gen_loss: 47.906 | mel_loss: 11.045 | 3.182 sec / 10 steps |\n",
      "| step: 436520 | gen_loss: 42.993 | mel_loss: 9.581 | 2.933 sec / 10 steps |\n",
      "| step: 436530 | gen_loss: 51.345 | mel_loss: 12.426 | 3.285 sec / 10 steps |\n",
      "| step: 436540 | gen_loss: 45.314 | mel_loss: 10.678 | 3.320 sec / 10 steps |\n",
      "| step: 436550 | gen_loss: 45.562 | mel_loss: 10.452 | 3.631 sec / 10 steps |\n",
      "| step: 436560 | gen_loss: 45.891 | mel_loss: 10.663 | 3.801 sec / 10 steps |\n",
      "| step: 436570 | gen_loss: 45.359 | mel_loss: 9.994 | 3.138 sec / 10 steps |\n",
      "| step: 436580 | gen_loss: 52.564 | mel_loss: 13.073 | 3.217 sec / 10 steps |\n",
      "| step: 436590 | gen_loss: 47.073 | mel_loss: 11.448 | 3.197 sec / 10 steps |\n",
      "| step: 436600 | gen_loss: 46.805 | mel_loss: 10.293 | 3.645 sec / 10 steps |\n",
      "Validation mel_loss: 14.13603401184082\n",
      "| step: 436610 | gen_loss: 41.840 | mel_loss: 8.821 | 3.102 sec / 10 steps |\n",
      "| step: 436620 | gen_loss: 49.124 | mel_loss: 11.545 | 3.320 sec / 10 steps |\n",
      "| step: 436630 | gen_loss: 46.296 | mel_loss: 10.283 | 3.398 sec / 10 steps |\n",
      "| step: 436640 | gen_loss: 46.506 | mel_loss: 10.515 | 3.073 sec / 10 steps |\n",
      "| step: 436650 | gen_loss: 42.931 | mel_loss: 9.287 | 3.091 sec / 10 steps |\n",
      "| step: 436660 | gen_loss: 47.587 | mel_loss: 11.688 | 3.285 sec / 10 steps |\n",
      "| step: 436670 | gen_loss: 51.869 | mel_loss: 13.444 | 3.458 sec / 10 steps |\n",
      "| step: 436680 | gen_loss: 53.208 | mel_loss: 12.413 | 3.605 sec / 10 steps |\n",
      "| step: 436690 | gen_loss: 49.946 | mel_loss: 11.802 | 3.637 sec / 10 steps |\n",
      "| step: 436700 | gen_loss: 44.373 | mel_loss: 9.593 | 3.348 sec / 10 steps |\n",
      "Validation mel_loss: 14.131993293762207\n",
      "| step: 436710 | gen_loss: 46.068 | mel_loss: 9.848 | 3.042 sec / 10 steps |\n",
      "| step: 436720 | gen_loss: 46.429 | mel_loss: 11.073 | 3.328 sec / 10 steps |\n",
      "| step: 436730 | gen_loss: 45.981 | mel_loss: 11.215 | 3.129 sec / 10 steps |\n",
      "| step: 436740 | gen_loss: 47.640 | mel_loss: 11.067 | 3.514 sec / 10 steps |\n",
      "| step: 436750 | gen_loss: 45.049 | mel_loss: 10.356 | 3.545 sec / 10 steps |\n",
      "| step: 436760 | gen_loss: 43.975 | mel_loss: 9.961 | 3.320 sec / 10 steps |\n",
      "| step: 436770 | gen_loss: 47.350 | mel_loss: 10.820 | 2.902 sec / 10 steps |\n",
      "| step: 436780 | gen_loss: 46.739 | mel_loss: 11.739 | 3.721 sec / 10 steps |\n",
      "| step: 436790 | gen_loss: 45.941 | mel_loss: 10.626 | 3.346 sec / 10 steps |\n",
      "| step: 436800 | gen_loss: 45.039 | mel_loss: 10.833 | 3.241 sec / 10 steps |\n",
      "Validation mel_loss: 14.17415714263916\n",
      "| step: 436810 | gen_loss: 42.941 | mel_loss: 9.223 | 3.366 sec / 10 steps |\n",
      "| step: 436820 | gen_loss: 46.983 | mel_loss: 12.083 | 3.322 sec / 10 steps |\n",
      "| step: 436830 | gen_loss: 42.850 | mel_loss: 9.418 | 3.597 sec / 10 steps |\n",
      "| step: 436840 | gen_loss: 51.617 | mel_loss: 12.352 | 3.649 sec / 10 steps |\n",
      "| step: 436850 | gen_loss: 51.197 | mel_loss: 12.403 | 3.180 sec / 10 steps |\n",
      "| step: 436860 | gen_loss: 47.725 | mel_loss: 11.179 | 3.343 sec / 10 steps |\n",
      "| step: 436870 | gen_loss: 47.819 | mel_loss: 11.637 | 3.470 sec / 10 steps |\n",
      "| step: 436880 | gen_loss: 49.210 | mel_loss: 12.085 | 3.118 sec / 10 steps |\n",
      "| step: 436890 | gen_loss: 44.059 | mel_loss: 10.317 | 3.545 sec / 10 steps |\n",
      "| step: 436900 | gen_loss: 45.971 | mel_loss: 10.444 | 3.763 sec / 10 steps |\n",
      "Validation mel_loss: 14.309511184692383\n",
      "| step: 436910 | gen_loss: 49.415 | mel_loss: 11.732 | 3.306 sec / 10 steps |\n",
      "| step: 436920 | gen_loss: 48.415 | mel_loss: 11.703 | 3.469 sec / 10 steps |\n",
      "| step: 436930 | gen_loss: 49.603 | mel_loss: 12.417 | 3.235 sec / 10 steps |\n",
      "| step: 436940 | gen_loss: 40.792 | mel_loss: 8.915 | 3.715 sec / 10 steps |\n",
      "| step: 436950 | gen_loss: 49.178 | mel_loss: 11.690 | 3.165 sec / 10 steps |\n",
      "| step: 436960 | gen_loss: 49.145 | mel_loss: 11.601 | 2.934 sec / 10 steps |\n",
      "| step: 436970 | gen_loss: 45.583 | mel_loss: 10.609 | 3.246 sec / 10 steps |\n",
      "| step: 436980 | gen_loss: 46.446 | mel_loss: 10.811 | 3.170 sec / 10 steps |\n",
      "| step: 436990 | gen_loss: 41.981 | mel_loss: 11.009 | 3.366 sec / 10 steps |\n",
      "| step: 437000 | gen_loss: 48.840 | mel_loss: 12.003 | 3.253 sec / 10 steps |\n",
      "Validation mel_loss: 14.059005737304688\n",
      "|| Epoch: 582 ||\n",
      "| step: 437010 | gen_loss: 48.270 | mel_loss: 11.929 | 3.017 sec / 10 steps |\n",
      "| step: 437020 | gen_loss: 45.284 | mel_loss: 10.484 | 3.391 sec / 10 steps |\n",
      "| step: 437030 | gen_loss: 47.048 | mel_loss: 11.151 | 3.014 sec / 10 steps |\n",
      "| step: 437040 | gen_loss: 44.155 | mel_loss: 9.801 | 3.509 sec / 10 steps |\n",
      "| step: 437050 | gen_loss: 45.855 | mel_loss: 11.031 | 3.478 sec / 10 steps |\n",
      "| step: 437060 | gen_loss: 49.259 | mel_loss: 11.617 | 3.303 sec / 10 steps |\n",
      "| step: 437070 | gen_loss: 42.818 | mel_loss: 8.911 | 3.499 sec / 10 steps |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 437080 | gen_loss: 48.164 | mel_loss: 11.525 | 2.929 sec / 10 steps |\n",
      "| step: 437090 | gen_loss: 51.808 | mel_loss: 12.749 | 3.320 sec / 10 steps |\n",
      "| step: 437100 | gen_loss: 43.209 | mel_loss: 9.832 | 3.424 sec / 10 steps |\n",
      "Validation mel_loss: 14.198042869567871\n",
      "| step: 437110 | gen_loss: 50.922 | mel_loss: 12.771 | 3.081 sec / 10 steps |\n",
      "| step: 437120 | gen_loss: 53.610 | mel_loss: 12.778 | 3.064 sec / 10 steps |\n",
      "| step: 437130 | gen_loss: 49.343 | mel_loss: 12.239 | 2.907 sec / 10 steps |\n",
      "| step: 437140 | gen_loss: 48.673 | mel_loss: 11.709 | 3.244 sec / 10 steps |\n",
      "| step: 437150 | gen_loss: 50.432 | mel_loss: 13.078 | 3.068 sec / 10 steps |\n",
      "| step: 437160 | gen_loss: 43.641 | mel_loss: 9.874 | 3.405 sec / 10 steps |\n",
      "| step: 437170 | gen_loss: 49.541 | mel_loss: 12.287 | 3.188 sec / 10 steps |\n",
      "| step: 437180 | gen_loss: 48.474 | mel_loss: 11.141 | 3.294 sec / 10 steps |\n",
      "| step: 437190 | gen_loss: 51.874 | mel_loss: 12.271 | 3.184 sec / 10 steps |\n",
      "| step: 437200 | gen_loss: 50.243 | mel_loss: 12.277 | 2.917 sec / 10 steps |\n",
      "Validation mel_loss: 14.339910507202148\n",
      "| step: 437210 | gen_loss: 50.051 | mel_loss: 11.864 | 3.002 sec / 10 steps |\n",
      "| step: 437220 | gen_loss: 48.610 | mel_loss: 11.327 | 3.341 sec / 10 steps |\n",
      "| step: 437230 | gen_loss: 43.516 | mel_loss: 10.459 | 3.585 sec / 10 steps |\n",
      "| step: 437240 | gen_loss: 50.570 | mel_loss: 12.305 | 3.416 sec / 10 steps |\n",
      "| step: 437250 | gen_loss: 48.246 | mel_loss: 11.452 | 3.309 sec / 10 steps |\n",
      "| step: 437260 | gen_loss: 46.080 | mel_loss: 10.038 | 3.227 sec / 10 steps |\n",
      "| step: 437270 | gen_loss: 45.966 | mel_loss: 10.273 | 3.359 sec / 10 steps |\n",
      "| step: 437280 | gen_loss: 49.964 | mel_loss: 11.052 | 3.321 sec / 10 steps |\n",
      "| step: 437290 | gen_loss: 46.170 | mel_loss: 10.259 | 3.423 sec / 10 steps |\n",
      "| step: 437300 | gen_loss: 46.519 | mel_loss: 11.028 | 3.276 sec / 10 steps |\n",
      "Validation mel_loss: 14.28880786895752\n",
      "| step: 437310 | gen_loss: 51.805 | mel_loss: 11.656 | 3.216 sec / 10 steps |\n",
      "| step: 437320 | gen_loss: 51.131 | mel_loss: 12.482 | 3.207 sec / 10 steps |\n",
      "| step: 437330 | gen_loss: 46.376 | mel_loss: 10.384 | 3.239 sec / 10 steps |\n",
      "| step: 437340 | gen_loss: 51.302 | mel_loss: 12.666 | 3.111 sec / 10 steps |\n",
      "| step: 437350 | gen_loss: 42.814 | mel_loss: 9.940 | 3.380 sec / 10 steps |\n",
      "| step: 437360 | gen_loss: 44.071 | mel_loss: 9.961 | 3.265 sec / 10 steps |\n",
      "| step: 437370 | gen_loss: 48.261 | mel_loss: 12.097 | 2.901 sec / 10 steps |\n",
      "| step: 437380 | gen_loss: 45.096 | mel_loss: 10.776 | 3.329 sec / 10 steps |\n",
      "| step: 437390 | gen_loss: 43.649 | mel_loss: 9.670 | 3.621 sec / 10 steps |\n",
      "| step: 437400 | gen_loss: 50.408 | mel_loss: 12.614 | 2.894 sec / 10 steps |\n",
      "Validation mel_loss: 14.55646800994873\n",
      "| step: 437410 | gen_loss: 48.317 | mel_loss: 10.169 | 3.536 sec / 10 steps |\n",
      "| step: 437420 | gen_loss: 47.587 | mel_loss: 11.051 | 3.474 sec / 10 steps |\n",
      "| step: 437430 | gen_loss: 41.330 | mel_loss: 8.738 | 3.490 sec / 10 steps |\n",
      "| step: 437440 | gen_loss: 49.098 | mel_loss: 12.098 | 3.369 sec / 10 steps |\n",
      "| step: 437450 | gen_loss: 49.526 | mel_loss: 12.075 | 3.088 sec / 10 steps |\n",
      "| step: 437460 | gen_loss: 48.192 | mel_loss: 11.843 | 3.281 sec / 10 steps |\n",
      "| step: 437470 | gen_loss: 42.158 | mel_loss: 10.232 | 2.997 sec / 10 steps |\n",
      "| step: 437480 | gen_loss: 48.141 | mel_loss: 12.098 | 3.435 sec / 10 steps |\n",
      "| step: 437490 | gen_loss: 47.949 | mel_loss: 12.090 | 3.015 sec / 10 steps |\n",
      "| step: 437500 | gen_loss: 52.974 | mel_loss: 12.872 | 3.333 sec / 10 steps |\n",
      "Validation mel_loss: 14.13349723815918\n",
      "| step: 437510 | gen_loss: 47.495 | mel_loss: 11.452 | 3.505 sec / 10 steps |\n",
      "| step: 437520 | gen_loss: 48.054 | mel_loss: 12.011 | 3.126 sec / 10 steps |\n",
      "| step: 437530 | gen_loss: 44.818 | mel_loss: 10.472 | 3.103 sec / 10 steps |\n",
      "| step: 437540 | gen_loss: 48.628 | mel_loss: 10.519 | 2.896 sec / 10 steps |\n",
      "| step: 437550 | gen_loss: 42.959 | mel_loss: 9.707 | 3.556 sec / 10 steps |\n",
      "| step: 437560 | gen_loss: 42.185 | mel_loss: 8.818 | 3.300 sec / 10 steps |\n",
      "| step: 437570 | gen_loss: 47.369 | mel_loss: 10.815 | 3.254 sec / 10 steps |\n",
      "| step: 437580 | gen_loss: 47.916 | mel_loss: 11.366 | 3.063 sec / 10 steps |\n",
      "| step: 437590 | gen_loss: 41.149 | mel_loss: 9.113 | 3.569 sec / 10 steps |\n",
      "| step: 437600 | gen_loss: 46.351 | mel_loss: 10.417 | 3.496 sec / 10 steps |\n",
      "Validation mel_loss: 13.994430541992188\n",
      "| step: 437610 | gen_loss: 44.916 | mel_loss: 10.811 | 3.116 sec / 10 steps |\n",
      "| step: 437620 | gen_loss: 43.452 | mel_loss: 10.267 | 3.653 sec / 10 steps |\n",
      "| step: 437630 | gen_loss: 50.965 | mel_loss: 11.851 | 3.245 sec / 10 steps |\n",
      "| step: 437640 | gen_loss: 47.010 | mel_loss: 11.012 | 3.516 sec / 10 steps |\n",
      "| step: 437650 | gen_loss: 46.912 | mel_loss: 11.855 | 3.465 sec / 10 steps |\n",
      "| step: 437660 | gen_loss: 46.959 | mel_loss: 10.906 | 3.488 sec / 10 steps |\n",
      "| step: 437670 | gen_loss: 51.567 | mel_loss: 12.823 | 3.625 sec / 10 steps |\n",
      "| step: 437680 | gen_loss: 48.418 | mel_loss: 11.644 | 3.169 sec / 10 steps |\n",
      "| step: 437690 | gen_loss: 46.127 | mel_loss: 10.455 | 3.425 sec / 10 steps |\n",
      "| step: 437700 | gen_loss: 50.002 | mel_loss: 12.460 | 3.112 sec / 10 steps |\n",
      "Validation mel_loss: 14.190322875976562\n",
      "| step: 437710 | gen_loss: 46.718 | mel_loss: 11.120 | 3.509 sec / 10 steps |\n",
      "| step: 437720 | gen_loss: 43.323 | mel_loss: 10.642 | 3.278 sec / 10 steps |\n",
      "| step: 437730 | gen_loss: 48.745 | mel_loss: 11.868 | 3.296 sec / 10 steps |\n",
      "| step: 437740 | gen_loss: 48.016 | mel_loss: 12.145 | 3.498 sec / 10 steps |\n",
      "| step: 437750 | gen_loss: 47.867 | mel_loss: 11.676 | 2.935 sec / 10 steps |\n",
      "|| Epoch: 583 ||\n",
      "| step: 437760 | gen_loss: 39.537 | mel_loss: 9.786 | 2.974 sec / 10 steps |\n",
      "| step: 437770 | gen_loss: 44.245 | mel_loss: 10.286 | 3.264 sec / 10 steps |\n",
      "| step: 437780 | gen_loss: 52.482 | mel_loss: 12.554 | 2.927 sec / 10 steps |\n",
      "| step: 437790 | gen_loss: 47.143 | mel_loss: 11.399 | 3.291 sec / 10 steps |\n",
      "| step: 437800 | gen_loss: 49.509 | mel_loss: 11.732 | 3.239 sec / 10 steps |\n",
      "Validation mel_loss: 14.280196189880371\n",
      "| step: 437810 | gen_loss: 53.761 | mel_loss: 12.937 | 3.729 sec / 10 steps |\n",
      "| step: 437820 | gen_loss: 50.782 | mel_loss: 13.500 | 3.501 sec / 10 steps |\n",
      "| step: 437830 | gen_loss: 53.526 | mel_loss: 13.368 | 3.197 sec / 10 steps |\n",
      "| step: 437840 | gen_loss: 49.050 | mel_loss: 12.248 | 3.382 sec / 10 steps |\n",
      "| step: 437850 | gen_loss: 43.789 | mel_loss: 9.447 | 2.956 sec / 10 steps |\n",
      "| step: 437860 | gen_loss: 50.747 | mel_loss: 12.727 | 3.262 sec / 10 steps |\n",
      "| step: 437870 | gen_loss: 37.618 | mel_loss: 8.200 | 3.607 sec / 10 steps |\n",
      "| step: 437880 | gen_loss: 44.513 | mel_loss: 10.438 | 3.361 sec / 10 steps |\n",
      "| step: 437890 | gen_loss: 50.385 | mel_loss: 11.936 | 3.162 sec / 10 steps |\n",
      "| step: 437900 | gen_loss: 47.006 | mel_loss: 10.516 | 3.205 sec / 10 steps |\n",
      "Validation mel_loss: 14.106331825256348\n",
      "| step: 437910 | gen_loss: 47.186 | mel_loss: 11.299 | 3.539 sec / 10 steps |\n",
      "| step: 437920 | gen_loss: 47.866 | mel_loss: 11.619 | 3.325 sec / 10 steps |\n",
      "| step: 437930 | gen_loss: 53.703 | mel_loss: 13.293 | 3.220 sec / 10 steps |\n",
      "| step: 437940 | gen_loss: 49.005 | mel_loss: 12.876 | 2.968 sec / 10 steps |\n",
      "| step: 437950 | gen_loss: 47.728 | mel_loss: 12.275 | 3.131 sec / 10 steps |\n",
      "| step: 437960 | gen_loss: 49.310 | mel_loss: 11.466 | 3.382 sec / 10 steps |\n",
      "| step: 437970 | gen_loss: 44.704 | mel_loss: 9.335 | 3.504 sec / 10 steps |\n",
      "| step: 437980 | gen_loss: 45.668 | mel_loss: 10.658 | 3.253 sec / 10 steps |\n",
      "| step: 437990 | gen_loss: 48.857 | mel_loss: 11.154 | 3.789 sec / 10 steps |\n",
      "| step: 438000 | gen_loss: 44.710 | mel_loss: 11.068 | 3.419 sec / 10 steps |\n",
      "Validation mel_loss: 14.081396102905273\n",
      "| step: 438010 | gen_loss: 49.737 | mel_loss: 12.914 | 3.430 sec / 10 steps |\n",
      "| step: 438020 | gen_loss: 48.336 | mel_loss: 11.356 | 3.731 sec / 10 steps |\n",
      "| step: 438030 | gen_loss: 49.461 | mel_loss: 11.786 | 3.198 sec / 10 steps |\n",
      "| step: 438040 | gen_loss: 41.654 | mel_loss: 9.090 | 3.305 sec / 10 steps |\n",
      "| step: 438050 | gen_loss: 44.335 | mel_loss: 9.694 | 3.384 sec / 10 steps |\n",
      "| step: 438060 | gen_loss: 50.488 | mel_loss: 12.289 | 3.078 sec / 10 steps |\n",
      "| step: 438070 | gen_loss: 49.980 | mel_loss: 11.777 | 3.474 sec / 10 steps |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 438080 | gen_loss: 49.124 | mel_loss: 11.615 | 3.117 sec / 10 steps |\n",
      "| step: 438090 | gen_loss: 47.947 | mel_loss: 11.366 | 3.307 sec / 10 steps |\n",
      "| step: 438100 | gen_loss: 51.579 | mel_loss: 12.519 | 3.125 sec / 10 steps |\n",
      "Validation mel_loss: 14.140478134155273\n",
      "| step: 438110 | gen_loss: 47.418 | mel_loss: 11.324 | 3.283 sec / 10 steps |\n",
      "| step: 438120 | gen_loss: 42.046 | mel_loss: 9.838 | 3.184 sec / 10 steps |\n",
      "| step: 438130 | gen_loss: 48.190 | mel_loss: 11.446 | 3.302 sec / 10 steps |\n",
      "| step: 438140 | gen_loss: 49.230 | mel_loss: 11.328 | 3.074 sec / 10 steps |\n",
      "| step: 438150 | gen_loss: 43.492 | mel_loss: 9.618 | 3.551 sec / 10 steps |\n",
      "| step: 438160 | gen_loss: 47.123 | mel_loss: 11.437 | 3.363 sec / 10 steps |\n",
      "| step: 438170 | gen_loss: 43.734 | mel_loss: 9.890 | 3.362 sec / 10 steps |\n",
      "| step: 438180 | gen_loss: 48.670 | mel_loss: 11.973 | 3.173 sec / 10 steps |\n",
      "| step: 438190 | gen_loss: 49.744 | mel_loss: 12.085 | 3.196 sec / 10 steps |\n",
      "| step: 438200 | gen_loss: 41.915 | mel_loss: 8.797 | 3.371 sec / 10 steps |\n",
      "Validation mel_loss: 14.018291473388672\n",
      "| step: 438210 | gen_loss: 42.728 | mel_loss: 10.655 | 3.326 sec / 10 steps |\n",
      "| step: 438220 | gen_loss: 46.171 | mel_loss: 10.358 | 3.199 sec / 10 steps |\n",
      "| step: 438230 | gen_loss: 49.075 | mel_loss: 11.301 | 2.997 sec / 10 steps |\n",
      "| step: 438240 | gen_loss: 48.834 | mel_loss: 12.199 | 3.341 sec / 10 steps |\n",
      "| step: 438250 | gen_loss: 37.845 | mel_loss: 8.758 | 3.596 sec / 10 steps |\n",
      "| step: 438260 | gen_loss: 47.787 | mel_loss: 12.342 | 3.585 sec / 10 steps |\n",
      "| step: 438270 | gen_loss: 41.386 | mel_loss: 8.646 | 3.510 sec / 10 steps |\n",
      "| step: 438280 | gen_loss: 48.127 | mel_loss: 11.977 | 3.621 sec / 10 steps |\n",
      "| step: 438290 | gen_loss: 41.767 | mel_loss: 10.817 | 3.201 sec / 10 steps |\n",
      "| step: 438300 | gen_loss: 45.125 | mel_loss: 10.857 | 3.281 sec / 10 steps |\n",
      "Validation mel_loss: 14.142706871032715\n",
      "| step: 438310 | gen_loss: 46.827 | mel_loss: 11.468 | 3.276 sec / 10 steps |\n",
      "| step: 438320 | gen_loss: 44.220 | mel_loss: 10.105 | 3.565 sec / 10 steps |\n",
      "| step: 438330 | gen_loss: 49.835 | mel_loss: 11.334 | 3.680 sec / 10 steps |\n",
      "| step: 438340 | gen_loss: 43.886 | mel_loss: 9.640 | 3.292 sec / 10 steps |\n",
      "| step: 438350 | gen_loss: 47.986 | mel_loss: 10.896 | 3.329 sec / 10 steps |\n",
      "| step: 438360 | gen_loss: 43.084 | mel_loss: 9.754 | 3.519 sec / 10 steps |\n",
      "| step: 438370 | gen_loss: 49.488 | mel_loss: 12.278 | 3.042 sec / 10 steps |\n",
      "| step: 438380 | gen_loss: 50.703 | mel_loss: 12.274 | 3.135 sec / 10 steps |\n",
      "| step: 438390 | gen_loss: 47.200 | mel_loss: 10.853 | 3.320 sec / 10 steps |\n",
      "| step: 438400 | gen_loss: 45.188 | mel_loss: 11.059 | 2.969 sec / 10 steps |\n",
      "Validation mel_loss: 14.191795349121094\n",
      "| step: 438410 | gen_loss: 42.683 | mel_loss: 8.935 | 3.470 sec / 10 steps |\n",
      "| step: 438420 | gen_loss: 50.510 | mel_loss: 12.289 | 3.382 sec / 10 steps |\n",
      "| step: 438430 | gen_loss: 50.688 | mel_loss: 12.094 | 3.439 sec / 10 steps |\n",
      "| step: 438440 | gen_loss: 46.730 | mel_loss: 11.394 | 3.238 sec / 10 steps |\n",
      "| step: 438450 | gen_loss: 45.066 | mel_loss: 10.614 | 3.534 sec / 10 steps |\n",
      "| step: 438460 | gen_loss: 46.084 | mel_loss: 10.900 | 3.227 sec / 10 steps |\n",
      "| step: 438470 | gen_loss: 42.222 | mel_loss: 9.501 | 3.170 sec / 10 steps |\n",
      "| step: 438480 | gen_loss: 46.541 | mel_loss: 10.974 | 3.115 sec / 10 steps |\n",
      "| step: 438490 | gen_loss: 45.271 | mel_loss: 10.085 | 3.629 sec / 10 steps |\n",
      "| step: 438500 | gen_loss: 46.646 | mel_loss: 10.808 | 3.155 sec / 10 steps |\n",
      "Validation mel_loss: 14.109848022460938\n",
      "|| Epoch: 584 ||\n",
      "| step: 438510 | gen_loss: 46.850 | mel_loss: 11.102 | 3.285 sec / 10 steps |\n",
      "| step: 438520 | gen_loss: 45.156 | mel_loss: 11.175 | 2.862 sec / 10 steps |\n",
      "| step: 438530 | gen_loss: 46.558 | mel_loss: 10.241 | 3.008 sec / 10 steps |\n",
      "| step: 438540 | gen_loss: 44.389 | mel_loss: 9.908 | 3.294 sec / 10 steps |\n",
      "| step: 438550 | gen_loss: 46.999 | mel_loss: 11.211 | 3.386 sec / 10 steps |\n",
      "| step: 438560 | gen_loss: 48.468 | mel_loss: 10.936 | 3.788 sec / 10 steps |\n",
      "| step: 438570 | gen_loss: 48.904 | mel_loss: 11.962 | 3.464 sec / 10 steps |\n",
      "| step: 438580 | gen_loss: 44.965 | mel_loss: 10.719 | 3.310 sec / 10 steps |\n",
      "| step: 438590 | gen_loss: 38.358 | mel_loss: 7.997 | 3.389 sec / 10 steps |\n",
      "| step: 438600 | gen_loss: 42.717 | mel_loss: 10.526 | 3.217 sec / 10 steps |\n",
      "Validation mel_loss: 14.077021598815918\n",
      "| step: 438610 | gen_loss: 50.266 | mel_loss: 12.904 | 3.099 sec / 10 steps |\n",
      "| step: 438620 | gen_loss: 51.762 | mel_loss: 12.818 | 3.282 sec / 10 steps |\n",
      "| step: 438630 | gen_loss: 40.128 | mel_loss: 10.045 | 3.492 sec / 10 steps |\n",
      "| step: 438640 | gen_loss: 40.085 | mel_loss: 9.323 | 3.247 sec / 10 steps |\n",
      "| step: 438650 | gen_loss: 45.483 | mel_loss: 10.990 | 3.005 sec / 10 steps |\n",
      "| step: 438660 | gen_loss: 43.076 | mel_loss: 9.808 | 3.244 sec / 10 steps |\n",
      "| step: 438670 | gen_loss: 52.201 | mel_loss: 13.156 | 3.715 sec / 10 steps |\n",
      "| step: 438680 | gen_loss: 44.130 | mel_loss: 10.482 | 3.571 sec / 10 steps |\n",
      "| step: 438690 | gen_loss: 43.036 | mel_loss: 10.214 | 3.247 sec / 10 steps |\n",
      "| step: 438700 | gen_loss: 44.245 | mel_loss: 10.091 | 2.971 sec / 10 steps |\n",
      "Validation mel_loss: 14.130995750427246\n",
      "| step: 438710 | gen_loss: 49.567 | mel_loss: 12.075 | 3.746 sec / 10 steps |\n",
      "| step: 438720 | gen_loss: 41.380 | mel_loss: 9.763 | 3.367 sec / 10 steps |\n",
      "| step: 438730 | gen_loss: 50.252 | mel_loss: 12.486 | 3.401 sec / 10 steps |\n",
      "| step: 438740 | gen_loss: 49.858 | mel_loss: 12.208 | 3.170 sec / 10 steps |\n",
      "| step: 438750 | gen_loss: 41.461 | mel_loss: 8.970 | 3.407 sec / 10 steps |\n",
      "| step: 438760 | gen_loss: 42.754 | mel_loss: 9.629 | 3.444 sec / 10 steps |\n",
      "| step: 438770 | gen_loss: 44.244 | mel_loss: 9.913 | 3.524 sec / 10 steps |\n",
      "| step: 438780 | gen_loss: 42.936 | mel_loss: 9.767 | 3.460 sec / 10 steps |\n",
      "| step: 438790 | gen_loss: 48.685 | mel_loss: 11.536 | 3.539 sec / 10 steps |\n",
      "| step: 438800 | gen_loss: 47.869 | mel_loss: 11.075 | 3.314 sec / 10 steps |\n",
      "Validation mel_loss: 14.159843444824219\n",
      "| step: 438810 | gen_loss: 43.139 | mel_loss: 10.296 | 3.364 sec / 10 steps |\n",
      "| step: 438820 | gen_loss: 53.017 | mel_loss: 12.389 | 3.327 sec / 10 steps |\n",
      "| step: 438830 | gen_loss: 47.125 | mel_loss: 11.687 | 3.248 sec / 10 steps |\n",
      "| step: 438840 | gen_loss: 43.989 | mel_loss: 9.839 | 3.572 sec / 10 steps |\n",
      "| step: 438850 | gen_loss: 45.554 | mel_loss: 10.935 | 3.792 sec / 10 steps |\n",
      "| step: 438860 | gen_loss: 46.489 | mel_loss: 11.548 | 3.058 sec / 10 steps |\n",
      "| step: 438870 | gen_loss: 45.100 | mel_loss: 10.537 | 3.388 sec / 10 steps |\n",
      "| step: 438880 | gen_loss: 43.079 | mel_loss: 10.612 | 3.555 sec / 10 steps |\n",
      "| step: 438890 | gen_loss: 43.882 | mel_loss: 9.700 | 3.012 sec / 10 steps |\n",
      "| step: 438900 | gen_loss: 46.228 | mel_loss: 10.528 | 3.266 sec / 10 steps |\n",
      "Validation mel_loss: 14.320831298828125\n",
      "| step: 438910 | gen_loss: 49.009 | mel_loss: 11.102 | 3.064 sec / 10 steps |\n",
      "| step: 438920 | gen_loss: 50.534 | mel_loss: 12.386 | 3.202 sec / 10 steps |\n",
      "| step: 438930 | gen_loss: 45.749 | mel_loss: 10.577 | 3.702 sec / 10 steps |\n",
      "| step: 438940 | gen_loss: 48.448 | mel_loss: 11.139 | 3.388 sec / 10 steps |\n",
      "| step: 438950 | gen_loss: 48.052 | mel_loss: 11.322 | 3.094 sec / 10 steps |\n",
      "| step: 438960 | gen_loss: 46.972 | mel_loss: 11.062 | 3.026 sec / 10 steps |\n",
      "| step: 438970 | gen_loss: 43.542 | mel_loss: 10.320 | 3.392 sec / 10 steps |\n",
      "| step: 438980 | gen_loss: 50.050 | mel_loss: 12.373 | 3.377 sec / 10 steps |\n",
      "| step: 438990 | gen_loss: 46.010 | mel_loss: 10.629 | 3.365 sec / 10 steps |\n",
      "| step: 439000 | gen_loss: 45.572 | mel_loss: 9.963 | 3.637 sec / 10 steps |\n",
      "Validation mel_loss: 14.216763496398926\n",
      "| step: 439010 | gen_loss: 50.600 | mel_loss: 12.217 | 3.137 sec / 10 steps |\n",
      "| step: 439020 | gen_loss: 48.071 | mel_loss: 11.860 | 3.263 sec / 10 steps |\n",
      "| step: 439030 | gen_loss: 48.161 | mel_loss: 11.103 | 3.080 sec / 10 steps |\n",
      "| step: 439040 | gen_loss: 37.219 | mel_loss: 8.031 | 3.059 sec / 10 steps |\n",
      "| step: 439050 | gen_loss: 49.173 | mel_loss: 11.242 | 3.103 sec / 10 steps |\n",
      "| step: 439060 | gen_loss: 43.474 | mel_loss: 9.711 | 3.270 sec / 10 steps |\n",
      "| step: 439070 | gen_loss: 52.294 | mel_loss: 12.964 | 3.088 sec / 10 steps |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 439080 | gen_loss: 48.715 | mel_loss: 11.049 | 3.020 sec / 10 steps |\n",
      "| step: 439090 | gen_loss: 50.490 | mel_loss: 12.167 | 3.037 sec / 10 steps |\n",
      "| step: 439100 | gen_loss: 48.547 | mel_loss: 11.799 | 3.149 sec / 10 steps |\n",
      "Validation mel_loss: 14.083762168884277\n",
      "| step: 439110 | gen_loss: 46.672 | mel_loss: 11.317 | 3.811 sec / 10 steps |\n",
      "| step: 439120 | gen_loss: 53.425 | mel_loss: 13.104 | 3.180 sec / 10 steps |\n",
      "| step: 439130 | gen_loss: 45.036 | mel_loss: 10.750 | 2.949 sec / 10 steps |\n",
      "| step: 439140 | gen_loss: 51.299 | mel_loss: 12.208 | 3.267 sec / 10 steps |\n",
      "| step: 439150 | gen_loss: 50.068 | mel_loss: 11.597 | 3.517 sec / 10 steps |\n",
      "| step: 439160 | gen_loss: 46.018 | mel_loss: 9.806 | 3.270 sec / 10 steps |\n",
      "| step: 439170 | gen_loss: 46.525 | mel_loss: 11.260 | 3.467 sec / 10 steps |\n",
      "| step: 439180 | gen_loss: 46.368 | mel_loss: 10.634 | 3.135 sec / 10 steps |\n",
      "| step: 439190 | gen_loss: 40.270 | mel_loss: 8.923 | 3.079 sec / 10 steps |\n",
      "| step: 439200 | gen_loss: 47.672 | mel_loss: 11.616 | 3.314 sec / 10 steps |\n",
      "Validation mel_loss: 14.152484893798828\n",
      "| step: 439210 | gen_loss: 47.763 | mel_loss: 11.934 | 3.437 sec / 10 steps |\n",
      "| step: 439220 | gen_loss: 45.614 | mel_loss: 12.012 | 3.127 sec / 10 steps |\n",
      "| step: 439230 | gen_loss: 50.375 | mel_loss: 12.463 | 3.248 sec / 10 steps |\n",
      "| step: 439240 | gen_loss: 52.898 | mel_loss: 13.589 | 2.928 sec / 10 steps |\n",
      "| step: 439250 | gen_loss: 46.351 | mel_loss: 11.216 | 3.435 sec / 10 steps |\n",
      "|| Epoch: 585 ||\n",
      "| step: 439260 | gen_loss: 41.969 | mel_loss: 8.948 | 3.357 sec / 10 steps |\n",
      "| step: 439270 | gen_loss: 42.645 | mel_loss: 9.958 | 3.059 sec / 10 steps |\n",
      "| step: 439280 | gen_loss: 46.969 | mel_loss: 11.325 | 3.336 sec / 10 steps |\n",
      "| step: 439290 | gen_loss: 48.992 | mel_loss: 11.873 | 3.100 sec / 10 steps |\n",
      "| step: 439300 | gen_loss: 45.001 | mel_loss: 9.971 | 3.227 sec / 10 steps |\n",
      "Validation mel_loss: 14.115795135498047\n",
      "| step: 439310 | gen_loss: 50.417 | mel_loss: 11.929 | 3.198 sec / 10 steps |\n",
      "| step: 439320 | gen_loss: 48.827 | mel_loss: 11.994 | 3.307 sec / 10 steps |\n",
      "| step: 439330 | gen_loss: 47.233 | mel_loss: 11.729 | 3.297 sec / 10 steps |\n",
      "| step: 439340 | gen_loss: 46.355 | mel_loss: 11.342 | 3.152 sec / 10 steps |\n",
      "| step: 439350 | gen_loss: 43.321 | mel_loss: 9.245 | 3.542 sec / 10 steps |\n",
      "| step: 439360 | gen_loss: 43.897 | mel_loss: 9.486 | 3.453 sec / 10 steps |\n",
      "| step: 439370 | gen_loss: 45.965 | mel_loss: 9.518 | 3.465 sec / 10 steps |\n",
      "| step: 439380 | gen_loss: 50.749 | mel_loss: 12.241 | 3.590 sec / 10 steps |\n",
      "| step: 439390 | gen_loss: 45.042 | mel_loss: 9.777 | 3.312 sec / 10 steps |\n",
      "| step: 439400 | gen_loss: 49.830 | mel_loss: 12.041 | 3.417 sec / 10 steps |\n",
      "Validation mel_loss: 14.108219146728516\n",
      "| step: 439410 | gen_loss: 51.037 | mel_loss: 12.727 | 3.470 sec / 10 steps |\n",
      "| step: 439420 | gen_loss: 45.159 | mel_loss: 10.687 | 3.381 sec / 10 steps |\n",
      "| step: 439430 | gen_loss: 48.806 | mel_loss: 11.636 | 3.744 sec / 10 steps |\n",
      "| step: 439440 | gen_loss: 48.733 | mel_loss: 11.996 | 3.621 sec / 10 steps |\n",
      "| step: 439450 | gen_loss: 46.022 | mel_loss: 10.302 | 3.437 sec / 10 steps |\n",
      "| step: 439460 | gen_loss: 45.457 | mel_loss: 11.131 | 3.155 sec / 10 steps |\n",
      "| step: 439470 | gen_loss: 45.731 | mel_loss: 10.907 | 3.227 sec / 10 steps |\n",
      "| step: 439480 | gen_loss: 46.718 | mel_loss: 10.536 | 3.137 sec / 10 steps |\n",
      "| step: 439490 | gen_loss: 43.466 | mel_loss: 9.227 | 3.144 sec / 10 steps |\n",
      "| step: 439500 | gen_loss: 48.723 | mel_loss: 11.259 | 2.949 sec / 10 steps |\n",
      "Validation mel_loss: 14.080994606018066\n",
      "| step: 439510 | gen_loss: 45.795 | mel_loss: 11.068 | 3.155 sec / 10 steps |\n",
      "| step: 439520 | gen_loss: 49.369 | mel_loss: 12.519 | 3.174 sec / 10 steps |\n",
      "| step: 439530 | gen_loss: 48.473 | mel_loss: 11.756 | 3.352 sec / 10 steps |\n",
      "| step: 439540 | gen_loss: 38.808 | mel_loss: 9.362 | 3.494 sec / 10 steps |\n",
      "| step: 439550 | gen_loss: 48.319 | mel_loss: 11.483 | 3.146 sec / 10 steps |\n",
      "| step: 439560 | gen_loss: 42.166 | mel_loss: 9.707 | 3.386 sec / 10 steps |\n",
      "| step: 439570 | gen_loss: 49.140 | mel_loss: 11.205 | 3.120 sec / 10 steps |\n",
      "| step: 439580 | gen_loss: 43.765 | mel_loss: 9.475 | 3.151 sec / 10 steps |\n",
      "| step: 439590 | gen_loss: 53.777 | mel_loss: 12.913 | 3.285 sec / 10 steps |\n",
      "| step: 439600 | gen_loss: 48.331 | mel_loss: 12.340 | 3.295 sec / 10 steps |\n",
      "Validation mel_loss: 14.158967018127441\n",
      "| step: 439610 | gen_loss: 46.925 | mel_loss: 11.629 | 3.496 sec / 10 steps |\n",
      "| step: 439620 | gen_loss: 44.519 | mel_loss: 9.354 | 3.347 sec / 10 steps |\n",
      "| step: 439630 | gen_loss: 51.602 | mel_loss: 13.189 | 2.954 sec / 10 steps |\n",
      "| step: 439640 | gen_loss: 53.386 | mel_loss: 13.283 | 3.255 sec / 10 steps |\n",
      "| step: 439650 | gen_loss: 49.534 | mel_loss: 11.782 | 3.381 sec / 10 steps |\n",
      "| step: 439660 | gen_loss: 48.062 | mel_loss: 11.238 | 3.213 sec / 10 steps |\n",
      "| step: 439670 | gen_loss: 44.631 | mel_loss: 9.628 | 3.211 sec / 10 steps |\n",
      "| step: 439680 | gen_loss: 45.356 | mel_loss: 9.694 | 3.457 sec / 10 steps |\n",
      "| step: 439690 | gen_loss: 46.008 | mel_loss: 10.770 | 3.478 sec / 10 steps |\n",
      "| step: 439700 | gen_loss: 47.328 | mel_loss: 11.091 | 3.352 sec / 10 steps |\n",
      "Validation mel_loss: 14.204998016357422\n",
      "| step: 439710 | gen_loss: 43.330 | mel_loss: 9.590 | 3.170 sec / 10 steps |\n",
      "| step: 439720 | gen_loss: 46.604 | mel_loss: 10.484 | 3.361 sec / 10 steps |\n",
      "| step: 439730 | gen_loss: 47.012 | mel_loss: 11.310 | 3.380 sec / 10 steps |\n",
      "| step: 439740 | gen_loss: 48.775 | mel_loss: 12.311 | 3.181 sec / 10 steps |\n",
      "| step: 439750 | gen_loss: 51.464 | mel_loss: 11.720 | 3.571 sec / 10 steps |\n",
      "| step: 439760 | gen_loss: 47.534 | mel_loss: 11.537 | 3.672 sec / 10 steps |\n",
      "| step: 439770 | gen_loss: 51.052 | mel_loss: 12.869 | 3.151 sec / 10 steps |\n",
      "| step: 439780 | gen_loss: 42.095 | mel_loss: 9.717 | 3.222 sec / 10 steps |\n",
      "| step: 439790 | gen_loss: 49.634 | mel_loss: 11.857 | 3.420 sec / 10 steps |\n",
      "| step: 439800 | gen_loss: 40.012 | mel_loss: 8.454 | 3.500 sec / 10 steps |\n",
      "Validation mel_loss: 14.20749568939209\n",
      "| step: 439810 | gen_loss: 48.764 | mel_loss: 12.585 | 3.134 sec / 10 steps |\n",
      "| step: 439820 | gen_loss: 43.552 | mel_loss: 9.978 | 3.072 sec / 10 steps |\n",
      "| step: 439830 | gen_loss: 43.347 | mel_loss: 10.935 | 3.097 sec / 10 steps |\n",
      "| step: 439840 | gen_loss: 49.421 | mel_loss: 12.518 | 3.425 sec / 10 steps |\n",
      "| step: 439850 | gen_loss: 48.367 | mel_loss: 12.092 | 3.301 sec / 10 steps |\n",
      "| step: 439860 | gen_loss: 42.241 | mel_loss: 10.297 | 3.056 sec / 10 steps |\n",
      "| step: 439870 | gen_loss: 47.444 | mel_loss: 11.160 | 3.215 sec / 10 steps |\n",
      "| step: 439880 | gen_loss: 32.223 | mel_loss: 8.339 | 2.963 sec / 10 steps |\n",
      "| step: 439890 | gen_loss: 50.578 | mel_loss: 12.917 | 3.210 sec / 10 steps |\n",
      "| step: 439900 | gen_loss: 44.231 | mel_loss: 10.003 | 3.516 sec / 10 steps |\n",
      "Validation mel_loss: 14.08890151977539\n",
      "| step: 439910 | gen_loss: 48.958 | mel_loss: 10.770 | 3.067 sec / 10 steps |\n",
      "| step: 439920 | gen_loss: 44.282 | mel_loss: 10.036 | 3.501 sec / 10 steps |\n",
      "| step: 439930 | gen_loss: 51.393 | mel_loss: 12.175 | 3.288 sec / 10 steps |\n",
      "| step: 439940 | gen_loss: 44.799 | mel_loss: 10.165 | 3.451 sec / 10 steps |\n",
      "| step: 439950 | gen_loss: 46.621 | mel_loss: 9.973 | 3.378 sec / 10 steps |\n",
      "| step: 439960 | gen_loss: 46.814 | mel_loss: 11.241 | 3.288 sec / 10 steps |\n",
      "| step: 439970 | gen_loss: 46.750 | mel_loss: 10.627 | 3.405 sec / 10 steps |\n",
      "| step: 439980 | gen_loss: 47.902 | mel_loss: 11.066 | 2.967 sec / 10 steps |\n",
      "| step: 439990 | gen_loss: 48.118 | mel_loss: 11.580 | 3.152 sec / 10 steps |\n",
      "| step: 440000 | gen_loss: 49.158 | mel_loss: 11.470 | 3.538 sec / 10 steps |\n",
      "Validation mel_loss: 13.929226875305176\n",
      "|| Epoch: 586 ||\n",
      "| step: 440010 | gen_loss: 46.959 | mel_loss: 11.080 | 3.190 sec / 10 steps |\n",
      "| step: 440020 | gen_loss: 42.369 | mel_loss: 9.415 | 3.019 sec / 10 steps |\n",
      "| step: 440030 | gen_loss: 54.143 | mel_loss: 13.638 | 2.834 sec / 10 steps |\n",
      "| step: 440040 | gen_loss: 46.407 | mel_loss: 10.626 | 3.377 sec / 10 steps |\n",
      "| step: 440050 | gen_loss: 46.458 | mel_loss: 10.452 | 3.525 sec / 10 steps |\n",
      "| step: 440060 | gen_loss: 52.588 | mel_loss: 12.166 | 2.934 sec / 10 steps |\n",
      "| step: 440070 | gen_loss: 40.752 | mel_loss: 8.883 | 3.166 sec / 10 steps |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 440080 | gen_loss: 41.430 | mel_loss: 10.442 | 3.335 sec / 10 steps |\n",
      "| step: 440090 | gen_loss: 49.844 | mel_loss: 11.505 | 3.454 sec / 10 steps |\n",
      "| step: 440100 | gen_loss: 44.304 | mel_loss: 8.717 | 3.310 sec / 10 steps |\n",
      "Validation mel_loss: 14.106343269348145\n",
      "| step: 440110 | gen_loss: 47.743 | mel_loss: 12.137 | 3.655 sec / 10 steps |\n",
      "| step: 440120 | gen_loss: 45.680 | mel_loss: 10.511 | 3.418 sec / 10 steps |\n",
      "| step: 440130 | gen_loss: 48.277 | mel_loss: 11.939 | 2.981 sec / 10 steps |\n",
      "| step: 440140 | gen_loss: 48.202 | mel_loss: 10.745 | 3.662 sec / 10 steps |\n",
      "| step: 440150 | gen_loss: 45.982 | mel_loss: 10.681 | 3.748 sec / 10 steps |\n",
      "| step: 440160 | gen_loss: 47.402 | mel_loss: 10.943 | 3.323 sec / 10 steps |\n",
      "| step: 440170 | gen_loss: 44.798 | mel_loss: 9.991 | 3.249 sec / 10 steps |\n",
      "| step: 440180 | gen_loss: 31.509 | mel_loss: 8.107 | 3.186 sec / 10 steps |\n",
      "| step: 440190 | gen_loss: 43.091 | mel_loss: 9.595 | 3.580 sec / 10 steps |\n",
      "| step: 440200 | gen_loss: 50.301 | mel_loss: 12.169 | 3.285 sec / 10 steps |\n",
      "Validation mel_loss: 13.94331169128418\n",
      "| step: 440210 | gen_loss: 45.312 | mel_loss: 11.060 | 3.329 sec / 10 steps |\n",
      "| step: 440220 | gen_loss: 49.959 | mel_loss: 11.751 | 2.887 sec / 10 steps |\n",
      "| step: 440230 | gen_loss: 43.981 | mel_loss: 9.935 | 3.289 sec / 10 steps |\n",
      "| step: 440240 | gen_loss: 49.877 | mel_loss: 11.967 | 3.225 sec / 10 steps |\n",
      "| step: 440250 | gen_loss: 49.595 | mel_loss: 11.627 | 3.298 sec / 10 steps |\n",
      "| step: 440260 | gen_loss: 49.265 | mel_loss: 11.361 | 3.292 sec / 10 steps |\n",
      "| step: 440270 | gen_loss: 49.784 | mel_loss: 11.325 | 3.282 sec / 10 steps |\n",
      "| step: 440280 | gen_loss: 45.266 | mel_loss: 10.928 | 3.239 sec / 10 steps |\n",
      "| step: 440290 | gen_loss: 45.730 | mel_loss: 10.537 | 3.308 sec / 10 steps |\n",
      "| step: 440300 | gen_loss: 48.341 | mel_loss: 12.347 | 3.469 sec / 10 steps |\n",
      "Validation mel_loss: 14.066965103149414\n",
      "| step: 440310 | gen_loss: 43.557 | mel_loss: 9.431 | 3.707 sec / 10 steps |\n",
      "| step: 440320 | gen_loss: 44.293 | mel_loss: 10.148 | 3.487 sec / 10 steps |\n",
      "| step: 440330 | gen_loss: 50.819 | mel_loss: 12.162 | 3.004 sec / 10 steps |\n",
      "| step: 440340 | gen_loss: 47.168 | mel_loss: 10.955 | 3.491 sec / 10 steps |\n",
      "| step: 440350 | gen_loss: 45.885 | mel_loss: 11.188 | 3.022 sec / 10 steps |\n",
      "| step: 440360 | gen_loss: 49.839 | mel_loss: 12.833 | 3.113 sec / 10 steps |\n",
      "| step: 440370 | gen_loss: 48.824 | mel_loss: 11.569 | 3.336 sec / 10 steps |\n",
      "| step: 440380 | gen_loss: 49.021 | mel_loss: 12.024 | 3.519 sec / 10 steps |\n",
      "| step: 440390 | gen_loss: 45.406 | mel_loss: 11.166 | 3.344 sec / 10 steps |\n",
      "| step: 440400 | gen_loss: 47.083 | mel_loss: 11.053 | 3.265 sec / 10 steps |\n",
      "Validation mel_loss: 14.060958862304688\n",
      "| step: 440410 | gen_loss: 43.967 | mel_loss: 10.277 | 3.593 sec / 10 steps |\n",
      "| step: 440420 | gen_loss: 44.961 | mel_loss: 9.922 | 3.509 sec / 10 steps |\n",
      "| step: 440430 | gen_loss: 44.040 | mel_loss: 9.539 | 3.413 sec / 10 steps |\n",
      "| step: 440440 | gen_loss: 46.612 | mel_loss: 11.198 | 3.459 sec / 10 steps |\n",
      "| step: 440450 | gen_loss: 41.796 | mel_loss: 10.357 | 3.083 sec / 10 steps |\n",
      "| step: 440460 | gen_loss: 42.394 | mel_loss: 8.560 | 3.032 sec / 10 steps |\n",
      "| step: 440470 | gen_loss: 45.724 | mel_loss: 10.809 | 3.230 sec / 10 steps |\n",
      "| step: 440480 | gen_loss: 49.523 | mel_loss: 11.621 | 3.044 sec / 10 steps |\n",
      "| step: 440490 | gen_loss: 48.594 | mel_loss: 11.316 | 3.387 sec / 10 steps |\n",
      "| step: 440500 | gen_loss: 46.401 | mel_loss: 9.666 | 3.368 sec / 10 steps |\n",
      "Validation mel_loss: 13.983274459838867\n",
      "| step: 440510 | gen_loss: 50.523 | mel_loss: 12.114 | 3.271 sec / 10 steps |\n",
      "| step: 440520 | gen_loss: 45.225 | mel_loss: 9.709 | 3.692 sec / 10 steps |\n",
      "| step: 440530 | gen_loss: 45.009 | mel_loss: 9.525 | 3.106 sec / 10 steps |\n",
      "| step: 440540 | gen_loss: 40.588 | mel_loss: 9.967 | 3.043 sec / 10 steps |\n",
      "| step: 440550 | gen_loss: 42.859 | mel_loss: 9.747 | 3.321 sec / 10 steps |\n",
      "| step: 440560 | gen_loss: 49.548 | mel_loss: 11.533 | 3.481 sec / 10 steps |\n",
      "| step: 440570 | gen_loss: 47.905 | mel_loss: 10.160 | 3.387 sec / 10 steps |\n",
      "| step: 440580 | gen_loss: 46.705 | mel_loss: 10.809 | 3.171 sec / 10 steps |\n",
      "| step: 440590 | gen_loss: 46.775 | mel_loss: 10.894 | 3.763 sec / 10 steps |\n",
      "| step: 440600 | gen_loss: 47.692 | mel_loss: 12.049 | 3.130 sec / 10 steps |\n",
      "Validation mel_loss: 14.15594482421875\n",
      "| step: 440610 | gen_loss: 46.941 | mel_loss: 10.658 | 3.289 sec / 10 steps |\n",
      "| step: 440620 | gen_loss: 43.967 | mel_loss: 9.177 | 3.802 sec / 10 steps |\n",
      "| step: 440630 | gen_loss: 46.610 | mel_loss: 10.084 | 3.414 sec / 10 steps |\n",
      "| step: 440640 | gen_loss: 50.003 | mel_loss: 12.573 | 3.052 sec / 10 steps |\n",
      "| step: 440650 | gen_loss: 44.193 | mel_loss: 9.276 | 3.127 sec / 10 steps |\n",
      "| step: 440660 | gen_loss: 44.648 | mel_loss: 10.295 | 2.856 sec / 10 steps |\n",
      "| step: 440670 | gen_loss: 46.602 | mel_loss: 9.784 | 3.577 sec / 10 steps |\n",
      "| step: 440680 | gen_loss: 50.031 | mel_loss: 11.733 | 3.091 sec / 10 steps |\n",
      "| step: 440690 | gen_loss: 46.520 | mel_loss: 10.765 | 3.380 sec / 10 steps |\n",
      "| step: 440700 | gen_loss: 49.714 | mel_loss: 11.832 | 3.197 sec / 10 steps |\n",
      "Validation mel_loss: 14.08867073059082\n",
      "| step: 440710 | gen_loss: 51.803 | mel_loss: 11.894 | 3.139 sec / 10 steps |\n",
      "| step: 440720 | gen_loss: 44.586 | mel_loss: 10.041 | 3.471 sec / 10 steps |\n",
      "| step: 440730 | gen_loss: 48.111 | mel_loss: 11.797 | 3.633 sec / 10 steps |\n",
      "| step: 440740 | gen_loss: 47.544 | mel_loss: 10.255 | 3.454 sec / 10 steps |\n",
      "| step: 440750 | gen_loss: 48.088 | mel_loss: 11.663 | 3.199 sec / 10 steps |\n",
      "|| Epoch: 587 ||\n",
      "| step: 440760 | gen_loss: 45.597 | mel_loss: 9.866 | 3.378 sec / 10 steps |\n",
      "| step: 440770 | gen_loss: 52.084 | mel_loss: 12.427 | 3.051 sec / 10 steps |\n",
      "| step: 440780 | gen_loss: 49.846 | mel_loss: 11.218 | 3.300 sec / 10 steps |\n",
      "| step: 440790 | gen_loss: 49.249 | mel_loss: 11.743 | 2.914 sec / 10 steps |\n",
      "| step: 440800 | gen_loss: 50.749 | mel_loss: 12.137 | 3.367 sec / 10 steps |\n",
      "Validation mel_loss: 13.906085968017578\n",
      "| step: 440810 | gen_loss: 51.987 | mel_loss: 12.511 | 3.226 sec / 10 steps |\n",
      "| step: 440820 | gen_loss: 48.485 | mel_loss: 11.916 | 3.464 sec / 10 steps |\n",
      "| step: 440830 | gen_loss: 42.286 | mel_loss: 10.468 | 3.699 sec / 10 steps |\n",
      "| step: 440840 | gen_loss: 46.546 | mel_loss: 10.809 | 3.043 sec / 10 steps |\n",
      "| step: 440850 | gen_loss: 52.855 | mel_loss: 12.889 | 2.950 sec / 10 steps |\n",
      "| step: 440860 | gen_loss: 42.654 | mel_loss: 9.305 | 3.130 sec / 10 steps |\n",
      "| step: 440870 | gen_loss: 43.927 | mel_loss: 9.970 | 3.032 sec / 10 steps |\n",
      "| step: 440880 | gen_loss: 44.877 | mel_loss: 10.054 | 3.523 sec / 10 steps |\n",
      "| step: 440890 | gen_loss: 51.042 | mel_loss: 11.515 | 3.126 sec / 10 steps |\n",
      "| step: 440900 | gen_loss: 42.886 | mel_loss: 9.112 | 3.410 sec / 10 steps |\n",
      "Validation mel_loss: 13.854371070861816\n",
      "| step: 440910 | gen_loss: 46.550 | mel_loss: 10.824 | 3.116 sec / 10 steps |\n",
      "| step: 440920 | gen_loss: 46.584 | mel_loss: 10.850 | 3.049 sec / 10 steps |\n",
      "| step: 440930 | gen_loss: 45.502 | mel_loss: 10.582 | 3.506 sec / 10 steps |\n",
      "| step: 440940 | gen_loss: 52.795 | mel_loss: 13.041 | 3.193 sec / 10 steps |\n",
      "| step: 440950 | gen_loss: 46.884 | mel_loss: 10.608 | 3.223 sec / 10 steps |\n",
      "| step: 440960 | gen_loss: 44.619 | mel_loss: 9.786 | 3.042 sec / 10 steps |\n",
      "| step: 440970 | gen_loss: 48.259 | mel_loss: 11.028 | 2.917 sec / 10 steps |\n",
      "| step: 440980 | gen_loss: 46.098 | mel_loss: 11.214 | 3.407 sec / 10 steps |\n",
      "| step: 440990 | gen_loss: 52.740 | mel_loss: 13.603 | 3.312 sec / 10 steps |\n",
      "| step: 441000 | gen_loss: 53.300 | mel_loss: 13.094 | 3.611 sec / 10 steps |\n",
      "Validation mel_loss: 14.071443557739258\n",
      "| step: 441010 | gen_loss: 47.003 | mel_loss: 10.924 | 3.208 sec / 10 steps |\n",
      "| step: 441020 | gen_loss: 44.064 | mel_loss: 10.261 | 3.307 sec / 10 steps |\n",
      "| step: 441030 | gen_loss: 49.107 | mel_loss: 12.232 | 3.278 sec / 10 steps |\n",
      "| step: 441040 | gen_loss: 49.923 | mel_loss: 11.581 | 3.641 sec / 10 steps |\n",
      "| step: 441050 | gen_loss: 46.807 | mel_loss: 10.743 | 3.538 sec / 10 steps |\n",
      "| step: 441060 | gen_loss: 45.258 | mel_loss: 10.111 | 3.330 sec / 10 steps |\n",
      "| step: 441070 | gen_loss: 42.587 | mel_loss: 8.991 | 3.235 sec / 10 steps |\n",
      "| step: 441080 | gen_loss: 46.933 | mel_loss: 10.624 | 3.485 sec / 10 steps |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 441090 | gen_loss: 42.879 | mel_loss: 9.329 | 3.575 sec / 10 steps |\n",
      "| step: 441100 | gen_loss: 44.380 | mel_loss: 9.541 | 3.061 sec / 10 steps |\n",
      "Validation mel_loss: 13.934513092041016\n",
      "| step: 441110 | gen_loss: 46.947 | mel_loss: 11.524 | 3.496 sec / 10 steps |\n",
      "| step: 441120 | gen_loss: 46.912 | mel_loss: 10.897 | 3.325 sec / 10 steps |\n",
      "| step: 441130 | gen_loss: 47.029 | mel_loss: 10.781 | 3.287 sec / 10 steps |\n",
      "| step: 441140 | gen_loss: 42.377 | mel_loss: 9.933 | 3.635 sec / 10 steps |\n",
      "| step: 441150 | gen_loss: 46.133 | mel_loss: 10.880 | 3.071 sec / 10 steps |\n",
      "| step: 441160 | gen_loss: 45.188 | mel_loss: 10.532 | 3.110 sec / 10 steps |\n",
      "| step: 441170 | gen_loss: 47.355 | mel_loss: 10.254 | 3.371 sec / 10 steps |\n",
      "| step: 441180 | gen_loss: 48.816 | mel_loss: 10.639 | 3.450 sec / 10 steps |\n",
      "| step: 441190 | gen_loss: 48.297 | mel_loss: 10.518 | 3.306 sec / 10 steps |\n",
      "| step: 441200 | gen_loss: 40.860 | mel_loss: 8.887 | 3.317 sec / 10 steps |\n",
      "Validation mel_loss: 13.83820915222168\n",
      "| step: 441210 | gen_loss: 44.603 | mel_loss: 9.628 | 3.373 sec / 10 steps |\n",
      "| step: 441220 | gen_loss: 51.417 | mel_loss: 12.302 | 2.979 sec / 10 steps |\n",
      "| step: 441230 | gen_loss: 46.680 | mel_loss: 10.733 | 3.251 sec / 10 steps |\n",
      "| step: 441240 | gen_loss: 47.142 | mel_loss: 11.150 | 3.440 sec / 10 steps |\n",
      "| step: 441250 | gen_loss: 49.040 | mel_loss: 12.368 | 3.121 sec / 10 steps |\n",
      "| step: 441260 | gen_loss: 44.792 | mel_loss: 10.363 | 3.209 sec / 10 steps |\n",
      "| step: 441270 | gen_loss: 48.126 | mel_loss: 11.561 | 3.272 sec / 10 steps |\n",
      "| step: 441280 | gen_loss: 50.826 | mel_loss: 10.715 | 3.419 sec / 10 steps |\n",
      "| step: 441290 | gen_loss: 48.704 | mel_loss: 11.915 | 3.195 sec / 10 steps |\n",
      "| step: 441300 | gen_loss: 40.715 | mel_loss: 9.034 | 3.227 sec / 10 steps |\n",
      "Validation mel_loss: 13.91624927520752\n",
      "| step: 441310 | gen_loss: 50.180 | mel_loss: 11.722 | 3.333 sec / 10 steps |\n",
      "| step: 441320 | gen_loss: 40.285 | mel_loss: 8.966 | 3.192 sec / 10 steps |\n",
      "| step: 441330 | gen_loss: 48.420 | mel_loss: 11.599 | 2.907 sec / 10 steps |\n",
      "| step: 441340 | gen_loss: 40.687 | mel_loss: 8.982 | 3.629 sec / 10 steps |\n",
      "| step: 441350 | gen_loss: 44.977 | mel_loss: 10.133 | 3.175 sec / 10 steps |\n",
      "| step: 441360 | gen_loss: 49.799 | mel_loss: 11.189 | 3.290 sec / 10 steps |\n",
      "| step: 441370 | gen_loss: 52.252 | mel_loss: 13.103 | 3.294 sec / 10 steps |\n",
      "| step: 441380 | gen_loss: 49.903 | mel_loss: 11.361 | 3.176 sec / 10 steps |\n",
      "| step: 441390 | gen_loss: 50.280 | mel_loss: 12.223 | 3.190 sec / 10 steps |\n",
      "| step: 441400 | gen_loss: 45.921 | mel_loss: 10.919 | 3.227 sec / 10 steps |\n",
      "Validation mel_loss: 13.929457664489746\n",
      "| step: 441410 | gen_loss: 52.732 | mel_loss: 12.340 | 3.433 sec / 10 steps |\n",
      "| step: 441420 | gen_loss: 47.075 | mel_loss: 9.676 | 3.035 sec / 10 steps |\n",
      "| step: 441430 | gen_loss: 48.546 | mel_loss: 11.726 | 3.168 sec / 10 steps |\n",
      "| step: 441440 | gen_loss: 51.839 | mel_loss: 12.571 | 3.519 sec / 10 steps |\n",
      "| step: 441450 | gen_loss: 48.481 | mel_loss: 11.713 | 3.149 sec / 10 steps |\n",
      "| step: 441460 | gen_loss: 40.057 | mel_loss: 8.788 | 3.519 sec / 10 steps |\n",
      "| step: 441470 | gen_loss: 47.414 | mel_loss: 11.117 | 3.109 sec / 10 steps |\n",
      "| step: 441480 | gen_loss: 51.884 | mel_loss: 12.604 | 3.286 sec / 10 steps |\n",
      "| step: 441490 | gen_loss: 46.574 | mel_loss: 10.200 | 3.598 sec / 10 steps |\n",
      "| step: 441500 | gen_loss: 45.108 | mel_loss: 10.465 | 3.294 sec / 10 steps |\n",
      "Validation mel_loss: 13.966034889221191\n",
      "|| Epoch: 588 ||\n",
      "| step: 441510 | gen_loss: 44.323 | mel_loss: 10.266 | 3.207 sec / 10 steps |\n",
      "| step: 441520 | gen_loss: 46.417 | mel_loss: 10.316 | 3.390 sec / 10 steps |\n",
      "| step: 441530 | gen_loss: 48.442 | mel_loss: 11.884 | 3.197 sec / 10 steps |\n",
      "| step: 441540 | gen_loss: 48.436 | mel_loss: 10.939 | 3.185 sec / 10 steps |\n",
      "| step: 441550 | gen_loss: 44.732 | mel_loss: 11.233 | 3.398 sec / 10 steps |\n",
      "| step: 441560 | gen_loss: 41.766 | mel_loss: 9.645 | 3.436 sec / 10 steps |\n",
      "| step: 441570 | gen_loss: 50.432 | mel_loss: 12.706 | 3.267 sec / 10 steps |\n",
      "| step: 441580 | gen_loss: 43.573 | mel_loss: 11.453 | 3.419 sec / 10 steps |\n",
      "| step: 441590 | gen_loss: 42.193 | mel_loss: 9.252 | 3.038 sec / 10 steps |\n",
      "| step: 441600 | gen_loss: 48.899 | mel_loss: 12.390 | 3.148 sec / 10 steps |\n",
      "Validation mel_loss: 13.982022285461426\n",
      "| step: 441610 | gen_loss: 41.727 | mel_loss: 10.228 | 3.219 sec / 10 steps |\n",
      "| step: 441620 | gen_loss: 52.732 | mel_loss: 12.583 | 3.306 sec / 10 steps |\n",
      "| step: 441630 | gen_loss: 50.214 | mel_loss: 12.171 | 2.897 sec / 10 steps |\n",
      "| step: 441640 | gen_loss: 49.241 | mel_loss: 11.531 | 3.159 sec / 10 steps |\n",
      "| step: 441650 | gen_loss: 45.747 | mel_loss: 10.771 | 3.541 sec / 10 steps |\n",
      "| step: 441660 | gen_loss: 45.213 | mel_loss: 9.351 | 3.315 sec / 10 steps |\n",
      "| step: 441670 | gen_loss: 51.926 | mel_loss: 12.837 | 3.355 sec / 10 steps |\n",
      "| step: 441680 | gen_loss: 46.471 | mel_loss: 10.128 | 3.330 sec / 10 steps |\n",
      "| step: 441690 | gen_loss: 47.375 | mel_loss: 12.224 | 3.538 sec / 10 steps |\n",
      "| step: 441700 | gen_loss: 48.703 | mel_loss: 11.844 | 3.060 sec / 10 steps |\n",
      "Validation mel_loss: 13.877264976501465\n",
      "| step: 441710 | gen_loss: 47.783 | mel_loss: 11.227 | 3.256 sec / 10 steps |\n",
      "| step: 441720 | gen_loss: 42.495 | mel_loss: 9.953 | 3.278 sec / 10 steps |\n",
      "| step: 441730 | gen_loss: 52.413 | mel_loss: 12.116 | 2.967 sec / 10 steps |\n",
      "| step: 441740 | gen_loss: 48.008 | mel_loss: 11.905 | 3.229 sec / 10 steps |\n",
      "| step: 441750 | gen_loss: 38.294 | mel_loss: 8.791 | 3.411 sec / 10 steps |\n",
      "| step: 441760 | gen_loss: 49.099 | mel_loss: 11.352 | 2.900 sec / 10 steps |\n",
      "| step: 441770 | gen_loss: 45.060 | mel_loss: 11.129 | 3.580 sec / 10 steps |\n",
      "| step: 441780 | gen_loss: 50.115 | mel_loss: 12.274 | 3.043 sec / 10 steps |\n",
      "| step: 441790 | gen_loss: 44.314 | mel_loss: 8.730 | 3.587 sec / 10 steps |\n",
      "| step: 441800 | gen_loss: 49.346 | mel_loss: 11.948 | 3.372 sec / 10 steps |\n",
      "Validation mel_loss: 13.956416130065918\n",
      "| step: 441810 | gen_loss: 43.142 | mel_loss: 9.802 | 3.790 sec / 10 steps |\n",
      "| step: 441820 | gen_loss: 45.311 | mel_loss: 10.880 | 2.883 sec / 10 steps |\n",
      "| step: 441830 | gen_loss: 44.694 | mel_loss: 10.325 | 3.022 sec / 10 steps |\n",
      "| step: 441840 | gen_loss: 47.549 | mel_loss: 11.361 | 3.233 sec / 10 steps |\n",
      "| step: 441850 | gen_loss: 47.195 | mel_loss: 11.150 | 3.508 sec / 10 steps |\n",
      "| step: 441860 | gen_loss: 54.414 | mel_loss: 13.032 | 3.411 sec / 10 steps |\n",
      "| step: 441870 | gen_loss: 46.402 | mel_loss: 10.854 | 2.949 sec / 10 steps |\n",
      "| step: 441880 | gen_loss: 45.068 | mel_loss: 10.007 | 3.420 sec / 10 steps |\n",
      "| step: 441890 | gen_loss: 44.502 | mel_loss: 9.607 | 3.419 sec / 10 steps |\n",
      "| step: 441900 | gen_loss: 47.528 | mel_loss: 11.871 | 3.184 sec / 10 steps |\n",
      "Validation mel_loss: 13.931452751159668\n",
      "| step: 441910 | gen_loss: 42.322 | mel_loss: 8.660 | 3.583 sec / 10 steps |\n",
      "| step: 441920 | gen_loss: 42.314 | mel_loss: 9.878 | 3.454 sec / 10 steps |\n",
      "| step: 441930 | gen_loss: 50.915 | mel_loss: 11.270 | 2.996 sec / 10 steps |\n",
      "| step: 441940 | gen_loss: 46.929 | mel_loss: 10.750 | 3.184 sec / 10 steps |\n",
      "| step: 441950 | gen_loss: 45.602 | mel_loss: 9.794 | 3.250 sec / 10 steps |\n",
      "| step: 441960 | gen_loss: 42.784 | mel_loss: 9.506 | 3.935 sec / 10 steps |\n",
      "| step: 441970 | gen_loss: 44.741 | mel_loss: 9.883 | 3.391 sec / 10 steps |\n",
      "| step: 441980 | gen_loss: 49.892 | mel_loss: 11.787 | 2.976 sec / 10 steps |\n",
      "| step: 441990 | gen_loss: 50.929 | mel_loss: 12.344 | 3.131 sec / 10 steps |\n",
      "| step: 442000 | gen_loss: 49.283 | mel_loss: 11.751 | 3.045 sec / 10 steps |\n",
      "Validation mel_loss: 13.931221961975098\n",
      "| step: 442010 | gen_loss: 46.278 | mel_loss: 10.148 | 3.287 sec / 10 steps |\n",
      "| step: 442020 | gen_loss: 44.112 | mel_loss: 9.859 | 2.988 sec / 10 steps |\n",
      "| step: 442030 | gen_loss: 48.198 | mel_loss: 11.610 | 3.153 sec / 10 steps |\n",
      "| step: 442040 | gen_loss: 47.824 | mel_loss: 10.604 | 3.365 sec / 10 steps |\n",
      "| step: 442050 | gen_loss: 47.486 | mel_loss: 9.982 | 3.417 sec / 10 steps |\n",
      "| step: 442060 | gen_loss: 49.895 | mel_loss: 11.256 | 3.759 sec / 10 steps |\n",
      "| step: 442070 | gen_loss: 48.087 | mel_loss: 11.781 | 3.303 sec / 10 steps |\n",
      "| step: 442080 | gen_loss: 45.615 | mel_loss: 10.375 | 3.860 sec / 10 steps |\n",
      "| step: 442090 | gen_loss: 53.800 | mel_loss: 13.482 | 3.266 sec / 10 steps |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 442100 | gen_loss: 43.230 | mel_loss: 9.631 | 2.987 sec / 10 steps |\n",
      "Validation mel_loss: 14.048620223999023\n",
      "| step: 442110 | gen_loss: 53.105 | mel_loss: 13.780 | 3.420 sec / 10 steps |\n",
      "| step: 442120 | gen_loss: 30.638 | mel_loss: 7.312 | 3.639 sec / 10 steps |\n",
      "| step: 442130 | gen_loss: 44.517 | mel_loss: 10.506 | 3.477 sec / 10 steps |\n",
      "| step: 442140 | gen_loss: 44.562 | mel_loss: 11.421 | 2.957 sec / 10 steps |\n",
      "| step: 442150 | gen_loss: 43.931 | mel_loss: 10.373 | 3.346 sec / 10 steps |\n",
      "| step: 442160 | gen_loss: 46.052 | mel_loss: 10.935 | 3.106 sec / 10 steps |\n",
      "| step: 442170 | gen_loss: 43.838 | mel_loss: 10.709 | 3.407 sec / 10 steps |\n",
      "| step: 442180 | gen_loss: 39.737 | mel_loss: 9.423 | 3.328 sec / 10 steps |\n",
      "| step: 442190 | gen_loss: 52.711 | mel_loss: 12.734 | 3.094 sec / 10 steps |\n",
      "| step: 442200 | gen_loss: 45.576 | mel_loss: 11.388 | 3.626 sec / 10 steps |\n",
      "Validation mel_loss: 13.968477249145508\n",
      "| step: 442210 | gen_loss: 48.379 | mel_loss: 12.139 | 3.102 sec / 10 steps |\n",
      "| step: 442220 | gen_loss: 49.354 | mel_loss: 11.996 | 3.263 sec / 10 steps |\n",
      "| step: 442230 | gen_loss: 43.764 | mel_loss: 10.333 | 3.453 sec / 10 steps |\n",
      "| step: 442240 | gen_loss: 44.366 | mel_loss: 10.666 | 3.649 sec / 10 steps |\n",
      "| step: 442250 | gen_loss: 49.460 | mel_loss: 12.155 | 3.379 sec / 10 steps |\n",
      "|| Epoch: 589 ||\n",
      "| step: 442260 | gen_loss: 48.085 | mel_loss: 11.306 | 3.405 sec / 10 steps |\n",
      "| step: 442270 | gen_loss: 42.289 | mel_loss: 9.959 | 3.254 sec / 10 steps |\n",
      "| step: 442280 | gen_loss: 43.309 | mel_loss: 9.773 | 3.658 sec / 10 steps |\n",
      "| step: 442290 | gen_loss: 42.939 | mel_loss: 9.524 | 3.346 sec / 10 steps |\n",
      "| step: 442300 | gen_loss: 45.281 | mel_loss: 10.035 | 3.237 sec / 10 steps |\n",
      "Validation mel_loss: 14.203736305236816\n",
      "| step: 442310 | gen_loss: 46.934 | mel_loss: 10.965 | 3.497 sec / 10 steps |\n",
      "| step: 442320 | gen_loss: 48.144 | mel_loss: 11.408 | 3.622 sec / 10 steps |\n",
      "| step: 442330 | gen_loss: 41.616 | mel_loss: 8.361 | 3.283 sec / 10 steps |\n",
      "| step: 442340 | gen_loss: 46.977 | mel_loss: 11.220 | 3.030 sec / 10 steps |\n",
      "| step: 442350 | gen_loss: 46.982 | mel_loss: 11.061 | 3.678 sec / 10 steps |\n",
      "| step: 442360 | gen_loss: 48.037 | mel_loss: 11.103 | 2.993 sec / 10 steps |\n",
      "| step: 442370 | gen_loss: 48.433 | mel_loss: 10.923 | 3.244 sec / 10 steps |\n",
      "| step: 442380 | gen_loss: 48.410 | mel_loss: 11.781 | 2.817 sec / 10 steps |\n",
      "| step: 442390 | gen_loss: 44.662 | mel_loss: 10.924 | 3.243 sec / 10 steps |\n",
      "| step: 442400 | gen_loss: 44.300 | mel_loss: 10.255 | 3.644 sec / 10 steps |\n",
      "Validation mel_loss: 13.976760864257812\n",
      "| step: 442410 | gen_loss: 45.300 | mel_loss: 10.364 | 3.277 sec / 10 steps |\n",
      "| step: 442420 | gen_loss: 50.008 | mel_loss: 12.098 | 2.736 sec / 10 steps |\n",
      "| step: 442430 | gen_loss: 44.451 | mel_loss: 9.352 | 3.141 sec / 10 steps |\n",
      "| step: 442440 | gen_loss: 43.028 | mel_loss: 9.366 | 3.149 sec / 10 steps |\n",
      "| step: 442450 | gen_loss: 49.585 | mel_loss: 11.685 | 3.553 sec / 10 steps |\n",
      "| step: 442460 | gen_loss: 51.410 | mel_loss: 12.298 | 3.370 sec / 10 steps |\n",
      "| step: 442470 | gen_loss: 49.428 | mel_loss: 12.325 | 3.350 sec / 10 steps |\n",
      "| step: 442480 | gen_loss: 44.203 | mel_loss: 10.373 | 3.382 sec / 10 steps |\n",
      "| step: 442490 | gen_loss: 45.293 | mel_loss: 11.031 | 3.043 sec / 10 steps |\n",
      "| step: 442500 | gen_loss: 39.051 | mel_loss: 9.469 | 3.092 sec / 10 steps |\n",
      "Validation mel_loss: 13.812935829162598\n",
      "| step: 442510 | gen_loss: 45.452 | mel_loss: 11.237 | 3.132 sec / 10 steps |\n",
      "| step: 442520 | gen_loss: 50.834 | mel_loss: 11.789 | 3.411 sec / 10 steps |\n",
      "| step: 442530 | gen_loss: 47.182 | mel_loss: 11.000 | 2.880 sec / 10 steps |\n",
      "| step: 442540 | gen_loss: 46.512 | mel_loss: 9.730 | 3.403 sec / 10 steps |\n",
      "| step: 442550 | gen_loss: 46.931 | mel_loss: 11.152 | 3.273 sec / 10 steps |\n",
      "| step: 442560 | gen_loss: 48.579 | mel_loss: 11.816 | 3.070 sec / 10 steps |\n",
      "| step: 442570 | gen_loss: 50.936 | mel_loss: 11.474 | 3.265 sec / 10 steps |\n",
      "| step: 442580 | gen_loss: 46.806 | mel_loss: 10.893 | 3.460 sec / 10 steps |\n",
      "| step: 442590 | gen_loss: 44.558 | mel_loss: 9.598 | 3.138 sec / 10 steps |\n",
      "| step: 442600 | gen_loss: 49.710 | mel_loss: 11.751 | 3.551 sec / 10 steps |\n",
      "Validation mel_loss: 13.873655319213867\n",
      "| step: 442610 | gen_loss: 46.006 | mel_loss: 11.893 | 3.342 sec / 10 steps |\n",
      "| step: 442620 | gen_loss: 48.763 | mel_loss: 11.176 | 3.436 sec / 10 steps |\n",
      "| step: 442630 | gen_loss: 42.078 | mel_loss: 10.703 | 3.413 sec / 10 steps |\n",
      "| step: 442640 | gen_loss: 43.842 | mel_loss: 10.036 | 3.577 sec / 10 steps |\n",
      "| step: 442650 | gen_loss: 45.634 | mel_loss: 9.721 | 3.503 sec / 10 steps |\n",
      "| step: 442660 | gen_loss: 44.885 | mel_loss: 10.290 | 3.233 sec / 10 steps |\n",
      "| step: 442670 | gen_loss: 46.671 | mel_loss: 11.295 | 3.126 sec / 10 steps |\n",
      "| step: 442680 | gen_loss: 52.047 | mel_loss: 13.231 | 2.935 sec / 10 steps |\n",
      "| step: 442690 | gen_loss: 42.506 | mel_loss: 9.121 | 3.432 sec / 10 steps |\n",
      "| step: 442700 | gen_loss: 43.018 | mel_loss: 10.074 | 3.496 sec / 10 steps |\n",
      "Validation mel_loss: 13.831443786621094\n",
      "| step: 442710 | gen_loss: 45.229 | mel_loss: 10.724 | 3.509 sec / 10 steps |\n",
      "| step: 442720 | gen_loss: 47.409 | mel_loss: 11.408 | 3.187 sec / 10 steps |\n",
      "| step: 442730 | gen_loss: 47.482 | mel_loss: 11.009 | 3.524 sec / 10 steps |\n",
      "| step: 442740 | gen_loss: 45.365 | mel_loss: 10.765 | 3.032 sec / 10 steps |\n",
      "| step: 442750 | gen_loss: 45.045 | mel_loss: 10.446 | 2.874 sec / 10 steps |\n",
      "| step: 442760 | gen_loss: 47.919 | mel_loss: 11.014 | 2.959 sec / 10 steps |\n",
      "| step: 442770 | gen_loss: 44.172 | mel_loss: 10.243 | 3.387 sec / 10 steps |\n",
      "| step: 442780 | gen_loss: 51.186 | mel_loss: 11.917 | 3.202 sec / 10 steps |\n",
      "| step: 442790 | gen_loss: 36.794 | mel_loss: 8.923 | 3.389 sec / 10 steps |\n",
      "| step: 442800 | gen_loss: 49.316 | mel_loss: 11.499 | 3.334 sec / 10 steps |\n",
      "Validation mel_loss: 13.866926193237305\n",
      "| step: 442810 | gen_loss: 47.784 | mel_loss: 11.160 | 3.540 sec / 10 steps |\n",
      "| step: 442820 | gen_loss: 50.394 | mel_loss: 12.490 | 3.211 sec / 10 steps |\n",
      "| step: 442830 | gen_loss: 47.769 | mel_loss: 11.357 | 3.103 sec / 10 steps |\n",
      "| step: 442840 | gen_loss: 48.198 | mel_loss: 10.799 | 3.525 sec / 10 steps |\n",
      "| step: 442850 | gen_loss: 48.438 | mel_loss: 11.442 | 3.382 sec / 10 steps |\n",
      "| step: 442860 | gen_loss: 50.231 | mel_loss: 11.468 | 3.420 sec / 10 steps |\n",
      "| step: 442870 | gen_loss: 45.736 | mel_loss: 10.033 | 3.384 sec / 10 steps |\n",
      "| step: 442880 | gen_loss: 43.307 | mel_loss: 8.623 | 3.548 sec / 10 steps |\n",
      "| step: 442890 | gen_loss: 43.262 | mel_loss: 9.204 | 3.351 sec / 10 steps |\n",
      "| step: 442900 | gen_loss: 43.689 | mel_loss: 9.815 | 3.099 sec / 10 steps |\n",
      "Validation mel_loss: 13.927454948425293\n",
      "| step: 442910 | gen_loss: 44.803 | mel_loss: 9.796 | 3.096 sec / 10 steps |\n",
      "| step: 442920 | gen_loss: 45.993 | mel_loss: 10.556 | 3.484 sec / 10 steps |\n",
      "| step: 442930 | gen_loss: 48.249 | mel_loss: 11.511 | 3.326 sec / 10 steps |\n",
      "| step: 442940 | gen_loss: 49.723 | mel_loss: 11.994 | 3.462 sec / 10 steps |\n",
      "| step: 442950 | gen_loss: 51.667 | mel_loss: 12.163 | 3.120 sec / 10 steps |\n",
      "| step: 442960 | gen_loss: 52.145 | mel_loss: 12.543 | 3.430 sec / 10 steps |\n",
      "| step: 442970 | gen_loss: 47.201 | mel_loss: 10.326 | 3.321 sec / 10 steps |\n",
      "| step: 442980 | gen_loss: 50.903 | mel_loss: 11.859 | 3.093 sec / 10 steps |\n",
      "| step: 442990 | gen_loss: 44.922 | mel_loss: 10.395 | 3.458 sec / 10 steps |\n",
      "| step: 443000 | gen_loss: 51.652 | mel_loss: 12.047 | 3.455 sec / 10 steps |\n",
      "Validation mel_loss: 13.770429611206055\n",
      "|| Epoch: 590 ||\n",
      "| step: 443010 | gen_loss: 46.477 | mel_loss: 10.653 | 2.949 sec / 10 steps |\n",
      "| step: 443020 | gen_loss: 45.200 | mel_loss: 10.065 | 3.470 sec / 10 steps |\n",
      "| step: 443030 | gen_loss: 49.024 | mel_loss: 11.336 | 3.493 sec / 10 steps |\n",
      "| step: 443040 | gen_loss: 42.505 | mel_loss: 9.296 | 3.491 sec / 10 steps |\n",
      "| step: 443050 | gen_loss: 46.412 | mel_loss: 10.947 | 3.359 sec / 10 steps |\n",
      "| step: 443060 | gen_loss: 46.250 | mel_loss: 10.366 | 3.025 sec / 10 steps |\n",
      "| step: 443070 | gen_loss: 45.749 | mel_loss: 9.763 | 3.047 sec / 10 steps |\n",
      "| step: 443080 | gen_loss: 47.305 | mel_loss: 10.315 | 3.281 sec / 10 steps |\n",
      "| step: 443090 | gen_loss: 50.881 | mel_loss: 12.805 | 3.275 sec / 10 steps |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 443100 | gen_loss: 46.882 | mel_loss: 10.659 | 3.167 sec / 10 steps |\n",
      "Validation mel_loss: 13.813389778137207\n",
      "| step: 443110 | gen_loss: 45.175 | mel_loss: 10.441 | 3.285 sec / 10 steps |\n",
      "| step: 443120 | gen_loss: 42.549 | mel_loss: 9.452 | 3.401 sec / 10 steps |\n",
      "| step: 443130 | gen_loss: 46.333 | mel_loss: 10.681 | 3.176 sec / 10 steps |\n",
      "| step: 443140 | gen_loss: 46.692 | mel_loss: 11.267 | 3.264 sec / 10 steps |\n",
      "| step: 443150 | gen_loss: 47.679 | mel_loss: 11.631 | 3.365 sec / 10 steps |\n",
      "| step: 443160 | gen_loss: 48.791 | mel_loss: 11.497 | 3.310 sec / 10 steps |\n",
      "| step: 443170 | gen_loss: 48.654 | mel_loss: 11.470 | 3.221 sec / 10 steps |\n",
      "| step: 443180 | gen_loss: 46.833 | mel_loss: 11.030 | 3.293 sec / 10 steps |\n",
      "| step: 443190 | gen_loss: 51.321 | mel_loss: 12.146 | 3.186 sec / 10 steps |\n",
      "| step: 443200 | gen_loss: 46.321 | mel_loss: 9.538 | 3.510 sec / 10 steps |\n",
      "Validation mel_loss: 13.992307662963867\n",
      "| step: 443210 | gen_loss: 50.282 | mel_loss: 10.832 | 3.424 sec / 10 steps |\n",
      "| step: 443220 | gen_loss: 49.586 | mel_loss: 11.549 | 3.292 sec / 10 steps |\n",
      "| step: 443230 | gen_loss: 46.133 | mel_loss: 11.354 | 3.281 sec / 10 steps |\n",
      "| step: 443240 | gen_loss: 43.113 | mel_loss: 9.661 | 3.272 sec / 10 steps |\n",
      "| step: 443250 | gen_loss: 49.369 | mel_loss: 11.704 | 3.076 sec / 10 steps |\n",
      "| step: 443260 | gen_loss: 46.148 | mel_loss: 10.862 | 3.411 sec / 10 steps |\n",
      "| step: 443270 | gen_loss: 48.496 | mel_loss: 11.740 | 2.816 sec / 10 steps |\n",
      "| step: 443280 | gen_loss: 50.164 | mel_loss: 12.280 | 3.140 sec / 10 steps |\n",
      "| step: 443290 | gen_loss: 48.329 | mel_loss: 10.989 | 3.089 sec / 10 steps |\n",
      "| step: 443300 | gen_loss: 50.561 | mel_loss: 12.202 | 3.173 sec / 10 steps |\n",
      "Validation mel_loss: 13.818065643310547\n",
      "| step: 443310 | gen_loss: 47.578 | mel_loss: 11.576 | 3.718 sec / 10 steps |\n",
      "| step: 443320 | gen_loss: 42.763 | mel_loss: 9.772 | 3.217 sec / 10 steps |\n",
      "| step: 443330 | gen_loss: 49.204 | mel_loss: 11.786 | 3.095 sec / 10 steps |\n",
      "| step: 443340 | gen_loss: 45.207 | mel_loss: 10.793 | 3.533 sec / 10 steps |\n",
      "| step: 443350 | gen_loss: 49.553 | mel_loss: 11.768 | 3.161 sec / 10 steps |\n",
      "| step: 443360 | gen_loss: 49.639 | mel_loss: 11.931 | 3.196 sec / 10 steps |\n",
      "| step: 443370 | gen_loss: 51.389 | mel_loss: 12.538 | 2.936 sec / 10 steps |\n",
      "| step: 443380 | gen_loss: 49.651 | mel_loss: 11.542 | 3.439 sec / 10 steps |\n",
      "| step: 443390 | gen_loss: 43.991 | mel_loss: 10.229 | 3.378 sec / 10 steps |\n",
      "| step: 443400 | gen_loss: 50.323 | mel_loss: 12.180 | 3.423 sec / 10 steps |\n",
      "Validation mel_loss: 13.810845375061035\n",
      "| step: 443410 | gen_loss: 50.929 | mel_loss: 12.111 | 3.444 sec / 10 steps |\n",
      "| step: 443420 | gen_loss: 41.900 | mel_loss: 9.258 | 3.474 sec / 10 steps |\n",
      "| step: 443430 | gen_loss: 41.380 | mel_loss: 8.929 | 3.321 sec / 10 steps |\n",
      "| step: 443440 | gen_loss: 50.782 | mel_loss: 12.502 | 3.353 sec / 10 steps |\n",
      "| step: 443450 | gen_loss: 56.418 | mel_loss: 13.034 | 3.310 sec / 10 steps |\n",
      "| step: 443460 | gen_loss: 48.916 | mel_loss: 11.066 | 2.859 sec / 10 steps |\n",
      "| step: 443470 | gen_loss: 51.842 | mel_loss: 12.095 | 3.285 sec / 10 steps |\n",
      "| step: 443480 | gen_loss: 50.114 | mel_loss: 11.808 | 3.337 sec / 10 steps |\n",
      "| step: 443490 | gen_loss: 49.516 | mel_loss: 11.831 | 3.169 sec / 10 steps |\n",
      "| step: 443500 | gen_loss: 40.344 | mel_loss: 8.270 | 3.500 sec / 10 steps |\n",
      "Validation mel_loss: 13.848652839660645\n",
      "| step: 443510 | gen_loss: 50.813 | mel_loss: 12.651 | 2.962 sec / 10 steps |\n",
      "| step: 443520 | gen_loss: 51.870 | mel_loss: 13.212 | 3.305 sec / 10 steps |\n",
      "| step: 443530 | gen_loss: 49.945 | mel_loss: 12.241 | 3.189 sec / 10 steps |\n",
      "| step: 443540 | gen_loss: 53.406 | mel_loss: 12.663 | 3.513 sec / 10 steps |\n",
      "| step: 443550 | gen_loss: 46.634 | mel_loss: 10.370 | 3.383 sec / 10 steps |\n",
      "| step: 443560 | gen_loss: 43.874 | mel_loss: 9.523 | 3.460 sec / 10 steps |\n",
      "| step: 443570 | gen_loss: 45.515 | mel_loss: 10.666 | 3.757 sec / 10 steps |\n",
      "| step: 443580 | gen_loss: 47.673 | mel_loss: 10.879 | 3.294 sec / 10 steps |\n",
      "| step: 443590 | gen_loss: 48.001 | mel_loss: 11.124 | 3.670 sec / 10 steps |\n",
      "| step: 443600 | gen_loss: 43.318 | mel_loss: 11.355 | 3.449 sec / 10 steps |\n",
      "Validation mel_loss: 13.805553436279297\n",
      "| step: 443610 | gen_loss: 45.859 | mel_loss: 11.020 | 3.215 sec / 10 steps |\n",
      "| step: 443620 | gen_loss: 46.780 | mel_loss: 10.851 | 3.347 sec / 10 steps |\n",
      "| step: 443630 | gen_loss: 41.620 | mel_loss: 9.197 | 3.252 sec / 10 steps |\n",
      "| step: 443640 | gen_loss: 42.328 | mel_loss: 9.051 | 3.503 sec / 10 steps |\n",
      "| step: 443650 | gen_loss: 47.191 | mel_loss: 11.096 | 3.521 sec / 10 steps |\n",
      "| step: 443660 | gen_loss: 47.156 | mel_loss: 10.499 | 3.129 sec / 10 steps |\n",
      "| step: 443670 | gen_loss: 43.209 | mel_loss: 9.037 | 3.372 sec / 10 steps |\n",
      "| step: 443680 | gen_loss: 49.430 | mel_loss: 10.998 | 3.237 sec / 10 steps |\n",
      "| step: 443690 | gen_loss: 48.329 | mel_loss: 11.135 | 3.212 sec / 10 steps |\n",
      "| step: 443700 | gen_loss: 49.163 | mel_loss: 11.554 | 3.277 sec / 10 steps |\n",
      "Validation mel_loss: 13.832253456115723\n",
      "| step: 443710 | gen_loss: 45.958 | mel_loss: 9.750 | 3.428 sec / 10 steps |\n",
      "| step: 443720 | gen_loss: 51.462 | mel_loss: 12.403 | 3.272 sec / 10 steps |\n",
      "| step: 443730 | gen_loss: 44.028 | mel_loss: 10.153 | 3.630 sec / 10 steps |\n",
      "| step: 443740 | gen_loss: 48.744 | mel_loss: 11.170 | 3.268 sec / 10 steps |\n",
      "| step: 443750 | gen_loss: 51.018 | mel_loss: 12.663 | 2.980 sec / 10 steps |\n",
      "|| Epoch: 591 ||\n",
      "| step: 443760 | gen_loss: 44.208 | mel_loss: 9.760 | 3.157 sec / 10 steps |\n",
      "| step: 443770 | gen_loss: 47.591 | mel_loss: 10.121 | 3.489 sec / 10 steps |\n",
      "| step: 443780 | gen_loss: 44.187 | mel_loss: 9.536 | 3.475 sec / 10 steps |\n",
      "| step: 443790 | gen_loss: 48.932 | mel_loss: 11.485 | 3.272 sec / 10 steps |\n",
      "| step: 443800 | gen_loss: 47.740 | mel_loss: 11.051 | 3.324 sec / 10 steps |\n",
      "Validation mel_loss: 13.77395248413086\n",
      "| step: 443810 | gen_loss: 46.859 | mel_loss: 11.521 | 3.677 sec / 10 steps |\n",
      "| step: 443820 | gen_loss: 43.965 | mel_loss: 10.478 | 2.973 sec / 10 steps |\n",
      "| step: 443830 | gen_loss: 51.364 | mel_loss: 12.168 | 3.009 sec / 10 steps |\n",
      "| step: 443840 | gen_loss: 45.307 | mel_loss: 9.557 | 3.312 sec / 10 steps |\n",
      "| step: 443850 | gen_loss: 54.044 | mel_loss: 13.297 | 3.385 sec / 10 steps |\n",
      "| step: 443860 | gen_loss: 49.630 | mel_loss: 11.464 | 3.111 sec / 10 steps |\n",
      "| step: 443870 | gen_loss: 45.906 | mel_loss: 9.919 | 3.664 sec / 10 steps |\n",
      "| step: 443880 | gen_loss: 45.186 | mel_loss: 10.432 | 3.111 sec / 10 steps |\n",
      "| step: 443890 | gen_loss: 45.481 | mel_loss: 9.894 | 3.391 sec / 10 steps |\n",
      "| step: 443900 | gen_loss: 52.328 | mel_loss: 12.442 | 3.177 sec / 10 steps |\n",
      "Validation mel_loss: 13.817706108093262\n",
      "| step: 443910 | gen_loss: 52.266 | mel_loss: 12.619 | 3.252 sec / 10 steps |\n",
      "| step: 443920 | gen_loss: 47.069 | mel_loss: 10.979 | 3.420 sec / 10 steps |\n",
      "| step: 443930 | gen_loss: 38.377 | mel_loss: 8.667 | 3.596 sec / 10 steps |\n",
      "| step: 443940 | gen_loss: 41.305 | mel_loss: 8.945 | 3.716 sec / 10 steps |\n",
      "| step: 443950 | gen_loss: 45.432 | mel_loss: 9.950 | 3.476 sec / 10 steps |\n",
      "| step: 443960 | gen_loss: 46.068 | mel_loss: 10.170 | 3.446 sec / 10 steps |\n",
      "| step: 443970 | gen_loss: 53.457 | mel_loss: 13.211 | 3.933 sec / 10 steps |\n",
      "| step: 443980 | gen_loss: 46.128 | mel_loss: 11.057 | 3.083 sec / 10 steps |\n",
      "| step: 443990 | gen_loss: 53.768 | mel_loss: 13.069 | 3.242 sec / 10 steps |\n",
      "| step: 444000 | gen_loss: 51.421 | mel_loss: 12.239 | 3.124 sec / 10 steps |\n",
      "Validation mel_loss: 13.865589141845703\n",
      "| step: 444010 | gen_loss: 50.662 | mel_loss: 11.647 | 3.777 sec / 10 steps |\n",
      "| step: 444020 | gen_loss: 43.078 | mel_loss: 9.179 | 3.204 sec / 10 steps |\n",
      "| step: 444030 | gen_loss: 45.323 | mel_loss: 10.933 | 3.308 sec / 10 steps |\n",
      "| step: 444040 | gen_loss: 47.633 | mel_loss: 11.565 | 3.011 sec / 10 steps |\n",
      "| step: 444050 | gen_loss: 47.566 | mel_loss: 11.555 | 3.112 sec / 10 steps |\n",
      "| step: 444060 | gen_loss: 51.493 | mel_loss: 12.315 | 3.241 sec / 10 steps |\n",
      "| step: 444070 | gen_loss: 48.063 | mel_loss: 11.386 | 3.168 sec / 10 steps |\n",
      "| step: 444080 | gen_loss: 49.954 | mel_loss: 12.176 | 3.057 sec / 10 steps |\n",
      "| step: 444090 | gen_loss: 45.223 | mel_loss: 9.889 | 3.310 sec / 10 steps |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| step: 444100 | gen_loss: 51.744 | mel_loss: 12.049 | 2.953 sec / 10 steps |\n",
      "Validation mel_loss: 13.81508731842041\n",
      "| step: 444110 | gen_loss: 49.654 | mel_loss: 11.346 | 2.882 sec / 10 steps |\n",
      "| step: 444120 | gen_loss: 48.036 | mel_loss: 11.312 | 3.390 sec / 10 steps |\n",
      "| step: 444130 | gen_loss: 47.360 | mel_loss: 10.941 | 3.129 sec / 10 steps |\n",
      "| step: 444140 | gen_loss: 45.385 | mel_loss: 10.142 | 3.387 sec / 10 steps |\n",
      "| step: 444150 | gen_loss: 48.113 | mel_loss: 11.670 | 3.287 sec / 10 steps |\n",
      "| step: 444160 | gen_loss: 43.403 | mel_loss: 9.613 | 3.236 sec / 10 steps |\n",
      "| step: 444170 | gen_loss: 53.199 | mel_loss: 12.750 | 3.035 sec / 10 steps |\n",
      "| step: 444180 | gen_loss: 50.091 | mel_loss: 11.514 | 3.265 sec / 10 steps |\n",
      "| step: 444190 | gen_loss: 49.207 | mel_loss: 11.529 | 3.120 sec / 10 steps |\n",
      "| step: 444200 | gen_loss: 42.150 | mel_loss: 9.197 | 3.221 sec / 10 steps |\n",
      "Validation mel_loss: 13.91747760772705\n",
      "| step: 444210 | gen_loss: 47.455 | mel_loss: 10.992 | 3.884 sec / 10 steps |\n",
      "| step: 444220 | gen_loss: 48.199 | mel_loss: 11.461 | 3.004 sec / 10 steps |\n",
      "| step: 444230 | gen_loss: 50.729 | mel_loss: 12.561 | 3.465 sec / 10 steps |\n",
      "| step: 444240 | gen_loss: 48.191 | mel_loss: 10.701 | 3.415 sec / 10 steps |\n",
      "| step: 444250 | gen_loss: 42.146 | mel_loss: 8.607 | 3.195 sec / 10 steps |\n",
      "| step: 444260 | gen_loss: 47.772 | mel_loss: 11.578 | 3.037 sec / 10 steps |\n",
      "| step: 444270 | gen_loss: 44.430 | mel_loss: 10.669 | 3.042 sec / 10 steps |\n",
      "| step: 444280 | gen_loss: 40.509 | mel_loss: 9.849 | 3.454 sec / 10 steps |\n",
      "| step: 444290 | gen_loss: 45.256 | mel_loss: 10.104 | 3.123 sec / 10 steps |\n",
      "| step: 444300 | gen_loss: 52.893 | mel_loss: 12.508 | 3.033 sec / 10 steps |\n",
      "Validation mel_loss: 13.970870018005371\n",
      "| step: 444310 | gen_loss: 49.187 | mel_loss: 11.833 | 3.454 sec / 10 steps |\n",
      "| step: 444320 | gen_loss: 46.814 | mel_loss: 10.492 | 3.168 sec / 10 steps |\n",
      "| step: 444330 | gen_loss: 46.313 | mel_loss: 10.310 | 3.780 sec / 10 steps |\n",
      "| step: 444340 | gen_loss: 49.814 | mel_loss: 11.554 | 3.256 sec / 10 steps |\n",
      "| step: 444350 | gen_loss: 47.982 | mel_loss: 11.292 | 3.286 sec / 10 steps |\n",
      "| step: 444360 | gen_loss: 50.062 | mel_loss: 11.592 | 3.181 sec / 10 steps |\n",
      "| step: 444370 | gen_loss: 46.172 | mel_loss: 9.938 | 3.452 sec / 10 steps |\n",
      "| step: 444380 | gen_loss: 44.185 | mel_loss: 9.872 | 3.624 sec / 10 steps |\n",
      "| step: 444390 | gen_loss: 46.773 | mel_loss: 11.112 | 3.285 sec / 10 steps |\n",
      "| step: 444400 | gen_loss: 40.740 | mel_loss: 9.478 | 3.550 sec / 10 steps |\n",
      "Validation mel_loss: 13.954410552978516\n",
      "| step: 444410 | gen_loss: 44.084 | mel_loss: 10.892 | 3.434 sec / 10 steps |\n",
      "| step: 444420 | gen_loss: 44.963 | mel_loss: 10.013 | 3.426 sec / 10 steps |\n",
      "| step: 444430 | gen_loss: 52.940 | mel_loss: 12.663 | 3.451 sec / 10 steps |\n",
      "| step: 444440 | gen_loss: 44.124 | mel_loss: 10.858 | 3.099 sec / 10 steps |\n",
      "| step: 444450 | gen_loss: 46.722 | mel_loss: 11.383 | 3.636 sec / 10 steps |\n",
      "| step: 444460 | gen_loss: 42.923 | mel_loss: 9.733 | 3.234 sec / 10 steps |\n",
      "| step: 444470 | gen_loss: 48.392 | mel_loss: 11.425 | 3.499 sec / 10 steps |\n",
      "| step: 444480 | gen_loss: 49.616 | mel_loss: 11.868 | 3.439 sec / 10 steps |\n",
      "| step: 444490 | gen_loss: 42.905 | mel_loss: 9.274 | 3.268 sec / 10 steps |\n",
      "| step: 444500 | gen_loss: 49.510 | mel_loss: 11.906 | 3.042 sec / 10 steps |\n",
      "Validation mel_loss: 13.779772758483887\n",
      "|| Epoch: 592 ||\n",
      "| step: 444510 | gen_loss: 46.129 | mel_loss: 10.321 | 3.469 sec / 10 steps |\n",
      "| step: 444520 | gen_loss: 51.145 | mel_loss: 12.476 | 3.445 sec / 10 steps |\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5700\\3877341421.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mHiFi_GAN_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5700\\1620609681.py\u001b[0m in \u001b[0;36mHiFi_GAN_train\u001b[1;34m(model_name, check_step)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[1;31m# MPD\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m             \u001b[0mmpd_real_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmpd_gen_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmpd_real_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmpd_gen_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmpd_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal_wav\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgen_wav\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m             \u001b[1;31m# MSD\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[0mmsd_real_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsd_gen_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsd_real_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsd_gen_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmsd_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal_wav\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgen_wav\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5700\\3669501684.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, real_waveform, gen_waveform)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0msub_pd\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub_pds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[0mreal_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal_feature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msub_pd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal_waveform\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mgen_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgen_feature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msub_pd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen_waveform\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[0mreal_outputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mgen_outputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5700\\3669501684.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, waveform)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mconv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1210\u001b[0m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1212\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1213\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 459\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    HiFi_GAN_train(model_name, check_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e746d756",
   "metadata": {},
   "source": [
    "## 5.2. Train Glow-TTS using pretrained HiFi-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cee75270",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### INPUT #####\n",
    "model_name = \"model_sce+01\"\n",
    "check_step = 337000\n",
    "hifi_model_name = \"model_01\"\n",
    "hifi_check_step = 400000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bfc87408",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Glow-TTS model: model_sce+01 | Step 337000\n",
      "Load HiFi-GAN model: model_01 | Step 400000\n",
      "|| Epoch: 3585 ||\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19080\\2826939880.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cuda\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mGlow_TTS_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhifi_model_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhifi_check_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19080\\2212085307.py\u001b[0m in \u001b[0;36mGlow_TTS_train\u001b[1;34m(model_name, check_step, hifi_model_name, hifi_check_step)\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0mgrad_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m             \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[0mstep\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19080\\3730983013.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_optim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_learning_rate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m                     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m                     \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'differentiable'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[0;32m    232\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'step'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m             adam(params_with_grad,\n\u001b[0m\u001b[0;32m    235\u001b[0m                  \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m                  \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m     func(params,\n\u001b[0m\u001b[0;32m    301\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    408\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 410\u001b[1;33m                 \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m             \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if __name__ == \"__main__\":\n",
    "    Glow_TTS_train(model_name, check_step, hifi_model_name, hifi_check_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3a8470",
   "metadata": {},
   "source": [
    "## 5.3. Generate TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf6941f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### INPUT #####\n",
    "model_name = \"model_sce+\"\n",
    "check_step = 337000\n",
    "hifi_model_name = \"model_01\"\n",
    "hifi_check_step = 400000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25cb40bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Glow-TTS model: model_sce+ | Step 337000\n",
      "Load HiFi-GAN model: model_01 | Step 400000\n",
      "Save: 0.wav\n",
      "Save: 1.wav\n",
      "Save: 2.wav\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if __name__ == \"__main__\":\n",
    "    text = [\n",
    "        \"안녕하세요? 딥러닝 티티에스를 테스트하는 중입니다.\",\n",
    "        \"근데 너희 때도 운영체제 과제가 있었어?\",\n",
    "        \"더 최신의 모델을 찾거나 많은 실험을 할 수도 있고, 통계적 분석을 통해 최적의 데이터셋을 만들어서 좋은 티티에스 모델을 만들 수도 있습니다.\"\n",
    "    ]\n",
    "    Inference(text, model_name, check_step, hifi_model_name, hifi_check_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd5bf32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
